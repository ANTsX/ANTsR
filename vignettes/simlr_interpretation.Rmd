---
title: "simlr_interpretation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simlr_interpretation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Similarity-driven multi-view linear reconstruction

## Introduction

Modern science generates data from multiple sources (e.g., genetics, imaging, behavior), yet integrating these 'views' to find a single, coherent underlying signal is a major statistical challenge. Similarity-driven multi-view linear reconstruction (SiMLR) addresses this by exploiting inter-modality relationships to transform large scientific datasets into a smaller joint space. 
The link between the original data $X_i$ ($n \times p$) and the reduced embedding space is a sparse set of features $v_i$ ( $p \times k$ ).  Standard statistical tools may then be applied on the embeddings $s_i=X_i v_i$.

SiMLR may be used in a variety of other ways. We will cover a basic example and follow with different use cases.  These examples are perhaps less involved than those given in the [original publication's](https://www.nature.com/articles/s43588-021-00029-8) cloud computing [examples](https://doi.org/10.24433/CO.3087836.v2) but illustrate new functionality.


## Example: Applying SiMLR to Simulated Multi-View Data

In this section, we explore how SiMLR can be applied to multi-view datasets. We'll generate synthetic data to simulate the application of SiMLR, compare the results to traditional methods like Singular Value Decomposition (SVD), and visualize the relationships between the views.  This example derives from the simulation-based evaluation in the original paper.

### Step 1: Simulate Multi-View Data

We begin by simulating three different views of data, each representing different modalities or datasets that share some underlying structure.

```{r step1}
library(ANTsR)
library(reshape2)
set.seed(1500)
nsub <- 100          # Number of subjects/samples
npix <- c(100, 200, 133)  # Number of features in each view
nk <- 5             # Number of latent factors

# Generating outcome matrices for each view
outcome <- matrix(rnorm(nsub * nk), ncol = nk)
outcome1 <- matrix(rnorm(nsub * nk), ncol = nk)
outcome2 <- matrix(rnorm(nsub * nk), ncol = nk)
outcome3 <- matrix(rnorm(nsub * nk), ncol = nk)

# Generating transformation matrices for each view
view1tx <- matrix(rnorm(npix[1] * nk), nrow = nk)
view2tx <- matrix(rnorm(npix[2] * nk), nrow = nk)
view3tx <- matrix(rnorm(npix[3] * nk), nrow = nk)

# Creating the multi-view data matrices
mat1 <- (outcome %*% t(outcome1) %*% (outcome1)) %*% view1tx
mat2 <- (outcome %*% t(outcome2) %*% (outcome2)) %*% view2tx
mat3 <- (outcome %*% t(outcome3) %*% (outcome3)) %*% view3tx
colnames(mat1)=paste0("m1.",1:ncol(mat1))
colnames(mat2)=paste0("m2.",1:ncol(mat2))
colnames(mat3)=paste0("m3.",1:ncol(mat3))

# Combine the matrices into a list
matlist <- list(m1 = mat1, m2 = mat2, m3 = mat3)
```

In this code, `mat1`, `mat2`, and `mat3` represent three views of the data. Each view is constructed to share a common underlying structure but with different transformations applied.

### Step 2: Apply SiMLR

Now we apply the SiMLR algorithm to the simulated multi-view data to find a common representation across the views.

```{r step2}
# Applying SiMLR
prepro=c( "centerAndScale","np")
constr='Stiefelx100x1'
constr='orthox100x0.1'
constr='Grassmannx100x1'
result <- simlr(matlist,constraint=constr,scale=prepro, 
  mixAlg='pca', energyType='normalized_correlation', # recommended
#  mixAlg='pca', energyType='cca', # recommended
#  mixAlg='ica', energyType='regression', # recommended
  sparsenessAlg = "spmp",
  iterations=100, verbose=TRUE)

# Projecting data into the reduced space
p1 <- mat1 %*% (result$v[[1]])
p2 <- mat2 %*% (result$v[[2]])
p3 <- mat3 %*% (result$v[[3]])
```

SiMLR identifies a sparse set of features that best reconstruct each view, resulting in projections `p1`, `p2`, and `p3` for each view.

### Step 3: Visualization and Comparison

To understand the effectiveness of the SiMLR projections, we can compare the correlations between the projections of different views. Additionally, we’ll compare the results to those obtained via Singular Value Decomposition (SVD).

```{r step3}
# Calculate SVD for comparison
svd1 <- svd(mat1, nu = nk, nv = 0)$u
svd2 <- svd(mat2, nu = nk, nv = 0)$u
svd3 <- svd(mat3, nu = nk, nv = 0)$u

# Calculate correlations between the SiMLR projections
cor_p1_p2 <- range(cor(p1, p2))
cor_p1_p3 <- range(cor(p1, p3))
cor_p2_p3 <- range(cor(p2, p3))

# Compare with correlations from SVD
cor_svd1_svd2 <- range(cor(svd1, svd2))

# Print the results
cat("Correlation between p1 and p2:", cor_p1_p2, "\n")
cat("Correlation between p1 and p3:", cor_p1_p3, "\n")
cat("Correlation between p2 and p3:", cor_p2_p3, "\n")
cat("Correlation between SVD1 and SVD2:", cor_svd1_svd2, "\n")
```

#### Visualizing Correlations

Visualizing these correlations helps us understand the similarity between the views after the SiMLR transformation.

```{r stepviz}
library(ggplot2)

# Function to plot correlation heatmaps
plot_cor_heatmap <- function(mat1, mat2, title) {
  cor_mat <- abs(cor(mat1,mat2))
  ggplot(melt(cor_mat), aes(Var1, Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(midpoint = 0.5, low = "blue", high = "red", mid = "white") +
    theme_minimal() +
    labs(title = title, x = "Variables", y = "Variables") +
    coord_fixed()
}

# Plotting the correlations
plot_cor_heatmap(p1, p2, "Correlation Heatmap of p1-p2")
plot_cor_heatmap(p1, p3,"Correlation Heatmap of p1-p3")
plot_cor_heatmap(p2, p3, "Correlation Heatmap of p2-p3")
```

These heatmaps show how the reduced representations from SiMLR correlate within each view. Comparing these with similar plots for SVD will reveal whether SiMLR captures more meaningful relationships between the views.

### Step 4: Permutation Test

A permutation test can assess whether the observed correlations are significant.

```{r step4}
s1 <- sample(1:nsub)
s2 <- sample(1:nsub)
permMats=list(vox = mat1, vox2 = mat2[s1, ], vox3 = mat3[s2, ])
resultp <- simlr(permMats,
  constraint=constr,scale=prepro,  sparsenessAlg = "spmp" )
p1p <- mat1 %*% (resultp$v[[1]])
p2p <- mat2[s1, ] %*% (resultp$v[[2]])
p3p <- mat3[s2, ] %*% (resultp$v[[3]])

# Compare the permuted correlations
cor_p1p_p2p <- range(cor(p1p, p2p))
cor_p1p_p3p <- range(cor(p1p, p3p))
cor_p2p_p3p <- range(cor(p2p, p3p))

# Print permuted results
cat("Permuted Correlation between p1p and p2p:", cor_p1p_p2p, "\n")
cat("Permuted Correlation between p1p and p3p:", cor_p1p_p3p, "\n")
cat("Permuted Correlation between p2p and p3p:", cor_p2p_p3p, "\n")
```

The permutation test will show if the observed correlations are stronger than those expected by chance.


This introductory example demonstrates how SiMLR can uncover the shared structure across multiple views of data. The correlations between the projections indicate the degree to which the algorithm has captured this shared structure, and the permutation test serves as a validation step.

## Integration of Multiple Data Types via Dimensionality Reduction 

SiMLR is particularly useful when working with datasets from different modalities (e.g., genomics, proteomics, and imaging data). By finding a joint embedding space, SiMLR can integrate these diverse data types into a unified representation, facilitating downstream analyses such as clustering, classification, or regression.

When dealing with high-dimensional data, dimensionality reduction techniques are often required to make the data more manageable and to avoid issues like the curse of dimensionality. SiMLR provides an approach that not only reduces dimensionality but also maintains the relationships between different views of the data, making it a powerful tool for exploratory data analysis and visualization.  Below, we illustrate reading, writing and exploratory integrated visualization.

```{r dataintegration,fig.width=12,fig.height=6.5}
library(fpc)
library(cluster)
library(gridExtra)
library(ggpubr)
sim2nm=tempfile()
write_simlr_data_frames( result$v, sim2nm  )
simres2=read_simlr_data_frames( sim2nm, names(matlist) )
popdf = data.frame( age = outcome, cog=outcome1, mat1, mat2, mat3 )
temp2=apply_simlr_matrices( popdf, simres2, center=TRUE, scale=TRUE )
simnames=temp2[[2]]
zz=exploratory_visualization( temp2[[1]][, temp2[[2]]], dotsne=FALSE )
print( zz$plot )
```

Now we can icorporate the "modalities" in order to predict the simulated outcome matrix.
Note that contributions exist from each modality in the regression (in some cases).  This 
is one of the advantages of SiMLR: it provides systematic guidance for building these types of integrative models.

```{r dataintegration2,fig.width=12,fig.height=6.5}
# multi-view regression
summary( lm( age.1 ~ m1PC1 + m2PC1 + m3PC1, data=temp2[[1]] ))
summary( lm( age.2 ~ m1PC1 + m2PC1 + m3PC1, data=temp2[[1]] ))
summary( lm( age.3 ~ m1PC1 + m2PC1 + m3PC1, data=temp2[[1]] ))
summary( lm( age.4 ~ m1PC1 + m2PC1 + m3PC1, data=temp2[[1]] ))
summary( lm( age.5 ~ m1PC1 + m2PC1 + m3PC1, data=temp2[[1]] ))
```


## Sparse Feature Selection

SiMLR incorporates sparse feature selection, allowing users to identify a minimal subset of features that contribute most to the joint embedding space. This can be particularly advantageous in settings where interpretability is crucial, such as biomarker discovery in biological datasets.

```{r featureselection,fig.width=12,fig.height=8}
pp=plot_features( simres2 )
grid.arrange( grobs=pp, nrow=3, top='Joint features' )
```

### Improved Model Interpretability

By projecting the data into a joint space, SiMLR helps improve the interpretability of machine learning models. The sparse feature selection ensures that only the most informative features are retained, making it easier to understand the relationships between the input data and the model's predictions.



## Cross-Modal Prediction or Imputation

In scenarios where you have multiple data modalities but missing information scattered across some of the views, SiMLR can be used to impute the missing data by leveraging the relationships between the available modalities. This cross-modal prediction capability is particularly useful in multi-omics studies and other fields where complete data is often unavailable.


```{r imp}
popdfi=popdf
proportion <- 0.1
num_NAs <- ceiling(proportion * nrow(popdfi) * ncol(popdfi))
indices <- arrayInd(sample(1:(nrow(popdfi) * ncol(popdfi)), num_NAs), .dim = dim(popdfi))
popdfi[indices] <- NA
nms=names(simres2)
nsimx=3
sep='.'
imputedcols=c()
for ( n in nms ) {
  for ( v in 1:nsimx ) {
    thiscol=paste0(n,sep,v)
    if ( any( is.na( popdfi[,thiscol] ) )) {
      imputedcols=c(imputedcols,thiscol)
      popdfi = simlr_impute( popdfi, nms, v, n, separator=sep )
    }
  }
}
impresult=data.frame( og=popdf[,imputedcols[1]], imp=popdfi[,imputedcols[1]])
ggscatter( impresult, 'og', 'imp' ) + ggtitle(paste("Imputed vs original data",imputedcols[1]))
```


## Testing Significance of Joint Relationships

We use a metric (`rvcoef`) that is not directly optimized to assess the significance of the simlr result versus permuted data.  The RV coefficient is analogous to the Pearson correlation coefficient but is used for comparing two matrices (or datasets) rather than two variables. It measures the similarity of the column spaces (subspaces) spanned by the columns of two matrices.  We compute the `rvcoef` across all pairs of data at each permutation and compare to the original values.  Another alternative (`diagcorr`) is also possible.

```{r permutation,fig.width=8,fig.height=8}
diagcorr = function( X,Y ) mean(abs(diag(cor(X,Y))))
initu = initializeSimlr( matlist, 3, jointReduction = TRUE  )
np=40
myperm = simlr.perm(  matlist,
  energyType='normalized_correlation',
  constraint='none',scale=prepro, 
  initialUMatrix=initu,
  iterations=100,
  nperms=np, FUN=adjusted_rvcoef, verbose=TRUE )
print( tail( myperm$significance, 2 )  )
gglist = list()
simlrmaps=colnames(myperm$significance)[-c(1:2)]
for ( xxx in simlrmaps ) {
    pvec=myperm$significance[ myperm$significance$perm %in% as.character(1:np),xxx]
    original_tstat <- myperm$significance[1,xxx] # replace with actual value
    gglist[[length(gglist)+1]]=( visualize_permutation_test( pvec, original_tstat, xxx ) )
}
grid.arrange( grobs=gglist, top='Cross-modality relationships: permutation tested')
```


```{r permutation2,fig.width=12,fig.height=6}
# display simlr results
gglist2=list()
for ( i in 1:(length(matlist)-1)) {
    for ( j in ((i+1):length(matlist))) {
        toviz1=data.matrix(simres2[[i]])
        toviz2=data.matrix(simres2[[j]])
        temp=visualize_lowrank_relationships( matlist[[i]], matlist[[j]], 
            toviz1, toviz2,
            nm1=names(matlist)[i], nm2=names(matlist)[j] )$plot
        gglist2[[length(gglist2)+1]]=temp
    }
}
grid.arrange(grobs=(gglist2),nrow=1, top='Low-rank correlations: SiMLR')

```


## Evaluating Machine Learning Objective Functions

This section of the tutorial vignette provides an example of how to set up and run a parameter search using the simlr.search function.  Here, `csearch` contains a list of constraint options that include "none" and different options for enforcing orthogonality _in the feature space_. This list will be used to explore different constraint options during the parameter search.

In this step, the simlr.parameters function is called to create a parameter list (`simparms`) that will control the grid search. Each parameter category (e.g., `nsimlr_options`, `prescaling_options`, etc.) is provided with options as a list. These parameters specify the settings and constraints that will be explored during the optimization process.

* `nsimlr_options` : The number of latent factors to consider.

* `prescaling_options` : Methods for data scaling/preprocessing (robust, whiten, np).

* `objectiver_options` : The objective function used (eg regression or cca).

* `mixer_options` : The mixing technique (ica or pca).

* `sparval_options` : Sparseness settings.

* `expBeta_options` : Exponential smoothing parameter beta values. e.g. 0.0, 0.9, 0.99, 0.999.

* `positivities_options` : Positivity constraints.

* `optimus_options` : The optimization strategy (lineSearch, mixed, greedy).

* `constraint_options` : Feature space orthogonality constraints to apply.

* `search_type` : The type of search to perform (full indicates a full grid search).

* `num_samples` : The number of samples to evaluate during the search.

```{r grassmann0}
library(ggiraph) # For interactive plots

paster <- function(vec1, vec2) {
  paste0(rep(vec1, each = length(vec2)), vec2)
}
csearch = list( "none", "Grassmannx0","Stiefelx0","orthox0.1x5")
# the 10x10 are weights on the optimization term of the energy
simparms = simlr.parameters( 
    nsimlr_options = list( 2 ),
    prescaling_options = list( 
            c( "robust", "centerAndScale",  "np") ),
    objectiver_options = list("regression",'acc','normalized_correlation','reconorm','lrr'),
    mixer_options=list("ica",'pca'), 
    sparval_options=list(  c(0.5,0.5,0.5) ),
    expBeta_options = list( c(0.0) ),
    positivities_options = list(
            c("positive","positive","positive")),
    optimus_options=list( "adam","nadam","rmsprop", 'ls_adam', 'adagrad' ),
    constraint_options=csearch,
    search_type="full",  # full or random
    num_samples=5
    )
``` 

The `regularizeSimlr` function is used to apply regularization to the input matrices matlist. Regularization can help prevent overfitting by penalizing complexity. The fraction and sigma parameters control the extent and type of regularization.

```{r grassmann1}
regs = regularizeSimlr(matlist, fraction=0.05, 
  sigma=rep(1.5,length(matlist)))
plot( image( regs[[1]] ) )
plot( image( regs[[2]] ) )
```

The `simlr.search` function performs the grid search over the parameter space defined in `simparms`. It takes the matrices matlist and regs as input. The `nperms` specifies the number of permutations to evaluate for robustness. The `verbose` parameter controls the level of detail in the output during the search process.


```{r grassmann2, eval=!exists("simlrXs")}
simlrXs=simlr.search(
        matlist,
        regs=regs,
        simparms,
        nperms=5, # just fast for vignette
        verbose=4
        )
# simlrXs$paramsearch
```

Finally, `simlrXs$paramsearch` retrieves the results of the parameter search. This will show the performance of different parameter combinations, helping to identify the optimal settings for the SiMLR algorithm.  The "best" result here will have the highest value for `final_energy` and its parameters are stored in the `parameters` slot.

```{r visparam,echo=FALSE,fig.width=12,fig.height=8}
library(tidyverse)
# --- Data Preparation ---
# 1. Rename columns for clarity
# 2. Calculate a composite performance score
# 3. Pivot the data into a "long" format for easy plotting
simlrXs_paramsearch = simlrXs$paramsearch
performance_data <- simlrXs_paramsearch %>%
  # Select and rename columns
  select(
    Objective = objectiver,
    Mixer = mixer,
    Constraint = constraint,
    Optimizer = optimus,
    RV_12 = rvcoef_m1_m2,
    RV_13 = rvcoef_m1_m3,
    RV_23 = rvcoef_m2_m3
  ) %>%
  # Calculate summary metrics
  mutate(
    # Mean RV is a good measure of overall shared signal
    Mean_RV = (RV_12 + RV_13 + RV_23) / 3,
    # Std Dev of RV measures how balanced the solution is
    # A low SD means all pairs are equally correlated (good)
    SD_RV = apply(select(., RV_12, RV_13, RV_23), 1, sd),
    
    # Create a unique ID for each experimental run
    Configuration = paste(Objective, Mixer, Constraint, Optimizer, sep = " | ")
  ) %>%
  # Arrange by the best overall performance
  arrange(desc(Mean_RV))

# --- Plot 2: Ranked Heatmap of Mean RV ---

# We first need to summarize the data
heatmap_data <- performance_data %>%
  group_by(Objective, Constraint, Mixer) %>%
  # Find the best score achieved for each combination
  summarise(Best_Mean_RV = max(Mean_RV, na.rm = TRUE), .groups = "drop") %>%
  # Order the factors for a clean plot
  mutate(
    Objective = fct_reorder(Objective, Best_Mean_RV, .fun = median, .desc = TRUE),
    Constraint = fct_reorder(Constraint, Best_Mean_RV, .fun = median, .desc = TRUE)
  )

plot2 <- ggplot(heatmap_data, aes(x = Constraint, y = Objective, fill = Best_Mean_RV)) +
  geom_tile(color = "white", linewidth = 1.5) +
  geom_text(aes(label = round(Best_Mean_RV, 2)), color = "white", size = 4, fontface = "bold") +
  facet_wrap(~Mixer) +
  scale_fill_viridis_c(option = "inferno", name = "Best Mean\nRV Score", limits = c(0,1)) +
  theme_minimal(base_size = 14) +
  labs(
    title = "SIMLR Performance Heatmap",
    subtitle = "Comparing the best performance for each configuration",
    x = "Constraint Type",
    y = "Objective Function"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    panel.grid = element_blank(),
    strip.text = element_text(face = "bold", size = 12)
  )

print(plot2)


plot_ranking <- ggplot(
    performance_data %>% slice_head(n = 20), # Show top 20
    aes(x = Mean_RV, y = fct_reorder(Configuration, Mean_RV), color = Objective)
  ) +
  geom_segment(aes(x = 0, xend = Mean_RV, yend = fct_reorder(Configuration, Mean_RV)), linewidth = 0.8, alpha = 0.5) +
  geom_point(aes(shape = Mixer), size = 4) +
  scale_color_viridis_d(option = "cividis", name = "Objective") +
  scale_shape_manual(name = "Mixer", values = c("ica" = 17, "pca" = 16)) +
  theme_minimal(base_size = 12) +
  labs(
    title = "Top 20 SIMLR Configurations by Mean RV",
    x = "Mean RV-Coefficient (Higher is Better)",
    y = "Configuration (Objective | Mixer | Constraint | Optimizer)"
  ) +
  theme(panel.grid.major.y = element_line(linetype = "dashed", color = "gray85"))

print(plot_ranking)



plot_data_interactive <- performance_data %>%
  slice_head(n = 30)

# Create the ggplot object with interactive aesthetics
p_interactive <- ggplot(
    plot_data_interactive,
    aes(
      x = RV_12, y = RV_13, color = Mean_RV, size = -SD_RV, shape = Mixer,
      tooltip = paste(
        "<b>", Configuration, "</b><br>",
        "------------------<br>",
        "Mean RV: ", round(Mean_RV, 3), "<br>",
        "SD of RVs: ", round(SD_RV, 3), "<br>",
        "RV(1,2): ", round(RV_12, 3), "<br>",
        "RV(1,3): ", round(RV_13, 3), "<br>",
        "RV(2,3): ", round(RV_23, 3)
      ),
      data_id = Configuration
    )
  ) +
  geom_point(alpha = 0.8) + # Standard geom_point
  scale_color_viridis_c(option = "plasma", name = "Mean RV\n(Higher is Better)") +
  scale_size_continuous(range = c(2.5, 8), name = "Balance (1/SD)\n(Larger is Better)") +
  scale_shape_manual(name = "Mixer Algorithm", values = c("ica" = 17, "pca" = 16)) +
  facet_wrap(~Constraint, labeller = label_wrap_gen(width=15)) +
  theme_bw(base_size = 14) +
  labs(
    title = "SIMLR Performance: Exploring Inter-Modality Correlations",
    subtitle = "Best solutions are top-right and large. Hover for details.",
    x = "RV-Coefficient between View 1 & 2",
    y = "RV-Coefficient between View 1 & 3"
  ) +
  theme(legend.position = "bottom", plot.title.position = "plot")

# Convert the ggplot object to an interactive girafe object
interactive_plot <- girafe(
  ggobj = p_interactive,
  options = list(
    opts_tooltip(css = "background-color:black;color:white;padding:5px;border-radius:5px;font-family:sans-serif;"),
    opts_hover(css = "fill:cyan;stroke:black;stroke-width:2px;"),
    opts_zoom(min = 1, max = 4)
  )
)

# View the interactive plot (will show up in RStudio's Viewer pane)
print(interactive_plot)

```


## Path modeling

SIMLR has a path modeling variant that lets you build the graph of connections however you like. Call the matrices A, B and C. If you build: A=&gt;A, B=&gt;B, C=&gt;C then you will learn within-modality feature sets. If you build A=&gt;(B,C), B=&gt;A, C=&gt;A then you will “focus” on A. The default graph is A=&gt;(B,C),
B=&gt;(A,C), C=&gt;(A,B) ]

## Summary 

We provided a quick overview of potential uses of the SiMLR framework.  
In addition, we provided visualization hints and standards that may be 
of use in scientific applications of these methods.
