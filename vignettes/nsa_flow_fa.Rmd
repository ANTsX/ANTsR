---
title: "Comparison of NSA-Flow Regularized Factor Analysis and Standard Principal Axis Factoring"
author: "Brian B. Avants"
date: "2025-10-25"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: paged
header-includes:
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(psych)
library(tidyverse)
library(gt)
library(corrplot)
library(ggplot2)
library(patchwork)
library(reshape2)
set.seed(1234)
```

# Introduction

Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. This document compares two factor analysis approaches: the standard Principal Axis Factoring (PAF) method implemented in the `psych` package, and a novel method that integrates Non-negative Stiefel Approximating Flow (NSA-Flow) regularization within the iterative factoring process.

The standard PAF method iteratively estimates communalities to extract factors, aiming to explain the common variance in the data. This document uses a standalone implementation wrapping `psych::fa()` with factoring method "pa" (principal axis).  This method has been widely used due to its simplicity and effectiveness in various applications and is closely related to principal component analysis (PCA).  

The new methodology, referred to as NSA-Flow Factor Analysis (NSA.FA), enhances this by incorporating NSA flow regularization during the iteration loop. This regularization helps in achieving more separable and (potentially) interpretable factor loadings relative to the standard method. Regularized exploratory factor analysis has been shown to improve interpretability by producing sparser loading matrices as an alternative to traditional rotation methods. Additionally, regularization can lead to more stable estimates in small samples and better overall model fit.

The comparison is performed using the Big Five Inventory (BFI) dataset from the `psych` package, which consists of 25 personality items from over 1000 participants. We evaluate both methods on factor loadings, explained variance, model fit indices via Confirmatory Factor Analysis (CFA), communalities, interpretability metrics, orthogonality defect, predictive correlations with Big Five total scores, and reproduced correlations. Visualizations and tables are used to highlight differences, with interpretations provided to guide understanding of the results.


```{r epxectedcorrs,echo=FALSE}
# Create expected correlation matrix

big5_corr <- tribble(
~Trait1, ~Trait2, ~Correlation,
"Openness (O)", "Conscientiousness (C)", -0.10,
"Openness (O)", "Extraversion (E)", 0.15,
"Openness (O)", "Agreeableness (A)", 0.10,
"Openness (O)", "Neuroticism (N)", 0.05,
"Conscientiousness (C)", "Extraversion (E)", 0.15,
"Conscientiousness (C)", "Agreeableness (A)", 0.25,
"Conscientiousness (C)", "Neuroticism (N)", -0.30,
"Extraversion (E)", "Agreeableness (A)", 0.25,
"Extraversion (E)", "Neuroticism (N)", -0.30,
"Agreeableness (A)", "Neuroticism (N)", -0.30
)

big5_corr %>%
gt() %>%
tab_header(
title = "Expected Correlations Between Big Five Traits",
subtitle = "Based on empirical meta-analyses of personality structure"
) %>%
fmt_number(columns = Correlation, decimals = 2) %>%
cols_label(
Trait1 = "Trait 1",
Trait2 = "Trait 2",
Correlation = "r"
)
```

# Methods

## Overview

The **NSA-Flow Factor Analysis (NSA-Flow-FA)** framework extends classical factor analysis by embedding a **geometric regularization flow**—the *Non-negative Stiefel Approximating Flow (NSA-Flow)*—into the estimation of factor loadings.  
The method combines three ideas:

1. **Low-rank covariance reconstruction** via factor analysis,  
2. **Orthogonality-preserving optimization** on the Stiefel manifold, and  
3. **Sparse or structured regularization** via a proximal flow with soft-thresholding dynamics.

This integration allows the model to discover interpretable and stable latent factors while maintaining fidelity to the observed covariance structure.

---

## Factor Model and Objective

We begin with the standard factor model for a \( p \times n \) data matrix \( X \):

$$
X = \Lambda F + E,
$$

where  

- \( \Lambda \in \mathbb{R}^{p \times k} \) is the **loading matrix**,  
- \( F \in \mathbb{R}^{k \times n} \) are the latent factors, and  
- \( E \) is a noise matrix with \( \mathbb{E}[E] = 0 \) and covariance \( \Psi = \mathrm{diag}(\psi_1, \dots, \psi_p) \).

The implied covariance model is

$$
\Sigma \approx \Lambda \Lambda^\top + \Psi,
$$

where \( \Sigma \) is the sample covariance or correlation matrix.  
Estimation typically proceeds by minimizing a loss function based on the reconstruction error between \( \Sigma \) and its low-rank approximation.

---

## NSA-Flow Regularization

NSA-Flow introduces a **smooth regularization flow** for \( \Lambda \) that continuously balances reconstruction fidelity and sparsity while constraining columns to remain approximately orthogonal.

The flow is defined by the gradient dynamics:

$$
\frac{d\Lambda_t}{dt} = -\nabla_\Lambda \left(
\frac{1}{2} \|\Sigma - \Lambda \Lambda^\top\|_F^2
+ \lambda \|\Lambda\|_1
- \frac{w}{2} \|\Lambda^\top \Lambda - I_k\|_F^2
\right),
$$

where  

- the first term encourages **accurate covariance reconstruction**,
- the last term penalizes **departure from column orthogonality**, and  
- \( w \ge 0 \) controls the strength of this decorrelation constraint.

Optimization proceeds by discretizing this flow and retracting updates back toward the **Stiefel manifold** (the set of orthonormal column matrices).

This retraction is implemented via a **soft-polar retraction**, an adaptive interpolation between the current estimate and its orthogonalized form:

$$
\Lambda^{(t+1)} = \Lambda^{(t)} \Big[(1 - \alpha)I_k + \alpha \, (\Lambda^{(t)\top}\Lambda^{(t)} + \epsilon I_k)^{-1/2}\Big],
$$

where \( \alpha \in [0,1] \) governs how strongly orthogonality is enforced.  
This allows the algorithm to maintain stability and interpretability without forcing strict orthogonality early in optimization.

---

## Algorithmic Structure

NSA-Flow-FA integrates this flow-based regularization into the **iterative factor analysis cycle**, replacing the classical loading update step with a proximal manifold update:

1. **Initialization via power iteration:**  
   A low-rank orthonormal basis for \( \Sigma \) is estimated using repeated multiplication and QR normalization:
   $$
   L^{(t+1)} = \mathrm{QR}(\Sigma L^{(t)}).
   $$

2. **Scaling by Rayleigh quotients:**  
   Columns of \( L \) are scaled by \( \sqrt{\lambda_i} \), approximating the leading eigenstructure of \( \Sigma \).

3. **Regularization via NSA-Flow:**  
   The current loadings \( L \) are used as a warm start \( Y_0 \) for NSA-Flow optimization:
   $$
   \Lambda = \mathrm{NSA\text{-}Flow}(Y_0; \, \lambda, w, \text{retraction} = \text{"soft\_polar"}).
   $$

4. **Communality update:**  
   Communalities \( h_i^2 = \|\Lambda_{i\cdot}\|^2 \) are damped and updated iteratively:
   $$
   h_i^{2,(t+1)} = (1-\eta_h)h_i^{2,(t)} + \eta_h \, \min(\|\Lambda_{i\cdot}\|^2, 1),
   $$
   ensuring stability and positive definiteness.

5. **Optional rotation:**  
   After convergence, the loadings may be rotated using **varimax**, **promax**, or **oblimin** to enhance interpretability.

---

## Annealed Regularization and Convergence

To promote smooth convergence and avoid premature sparsification, the regularization weight \( w \) can be **annealed** linearly over outer iterations:

$$
w_t = w_{\text{final}} \cdot \frac{t}{T},
$$

where \( T \) is the total number of iterations.  
This gradual increase in orthogonality pressure allows early exploration of the parameter space before enforcing tighter manifold constraints.

Convergence is monitored through both the change in communalities \( \max_i |\Delta h_i^2| \) and the relative reconstruction error:

$$
E^{(t)} = \frac{\|\Sigma - \Lambda^{(t)} \Lambda^{(t)\top}\|_F}{\|\Sigma\|_F}.
$$

The algorithm stops when both fall below user-defined tolerances.

---

## Mathematical and Statistical Assumptions

NSA-Flow-FA makes the following implicit assumptions:

1. **Approximate factor model:**  
   The covariance structure can be represented as a low-rank plus diagonal decomposition.

2. **Manifold regularization:**  
   Loadings evolve near the Stiefel manifold (orthonormal columns), ensuring well-conditioned factor representations and reducing multicollinearity among latent dimensions.  The NSA-Flow regularization encourages parsimony by pushing solutions toward orthogonality and (optionally) non-negativity.
 
3. **Locally convex energy landscape:**  
   For sufficiently small learning rates, the discretized flow behaves like a descent method for a smooth objective, with guaranteed monotonic energy decrease.

These assumptions ensure interpretability, robustness to noise, and numerical stability across iterations.

---

## Relationship to Classical Methods

Classical factor analysis seeks \( \Lambda \) by solving an eigenvalue problem and optionally applying rotations to achieve simple structure.  NSA-Flow-FA generalizes this by introducing a **continuous, geometry-aware path** from dense to sparse solutions, effectively integrating the roles of both **rotation** and **regularization** within a single dynamical system.

Whereas LASSO-type sparse factor models impose sparsity directly through penalized likelihoods, NSA-Flow enforces it **implicitly** through proximal evolution constrained by manifold geometry.  This yields factors that are simultaneously **orthogonalized, interpretable, and data-adaptive**, bridging the gap between statistical estimation and geometric  learning.


## Implementation

The algorithm is implemented in R with computational backends in **PyTorch** for efficient tensor operations and automatic differentiation. Optimization steps are performed in Python through the `nsa_flow_py` module, accessed via the `reticulate` interface. The R function `nsa_flow_fa()` orchestrates data preparation, flow control, and convergence monitoring, while `nsa_flow_torch()` executes the core NSA-Flow updates on the GPU.

---


NSA-Flow-FA combines traditional statistical factor modeling with modern geometric regularization to yield interpretable, sparse, and stable representations. The method can be viewed as a **proximal Riemannian generalization of sparse factor analysis**, maintaining theoretical transparency while extending applicability to high-dimensional, non-convex problems.


## Evaluation Metrics

- **Factor Loadings**: Compared via heatmaps of absolute differences.
- **Explained Variance**: Sum of squared loadings per factor.
- **Model Fit**: Derived CFA models fitted using `lavaan`, with indices like RMSEA, CFI, TLI, SRMR, BIC.
- **Communalities**: Proportion of variance explained per variable.
- **Interpretability**: Measured using mean Hofmann's complexity (lower is better) and number of cross-loadings (|loading| > 0.3 on >1 factor, lower is better).
- **Orthogonality Defect**: Measure of how close the rotated loadings are to orthogonality (lower is better).
- **Predictive Validity**: Correlations of factor scores with Big Five total scores in a hold-out test set.
- **Reproduced Correlations**: Comparison of observed vs. factor-implied correlation matrices.

## Data Splitting

For predictive comparisons, the dataset is split into 80% training and 20% testing sets. Models are fitted on the training set, and evaluations are performed on the test set.

```{r methods,echo=FALSE}

library(ANTsR)
paf_standalone <- function(data = NULL, R = NULL, nfactors, rotate = "none", max_iter = 100, tol = 1e-5) {
  if (!is.null(data)) R <- cor(data)
  fit <- psych::fa(R, nfactors = nfactors, rotate = rotate, fm = "pa", max.iter = max_iter)
  list(
    loadings = fit$loadings,
    communalities = fit$communality,
    uniqueness = 1 - fit$communality,
    iterations = fit$iter,
    converged = TRUE
  )
}



my_standalone <- function(data = NULL, R = NULL, nfactors, rotate = "none", max_iter = 100, tol = 1e-5) {
  if (is.null(R)) {
    if (is.null(data)) stop("Provide either raw data or correlation matrix R.")
    R <- cor(data, use = "pairwise.complete.obs")
  }
  p <- nrow(R)
  
  smc <- 1 - 1 / diag(solve(R))
  h2 <- pmin(smc, 1)
  
  converged <- FALSE
  iter <- 0
  while (iter < max_iter) {
    iter <- iter + 1
    R_mod <- R
    diag(R_mod) <- h2
    
    e <- eigen(R_mod)
    vals <- Re(e$values[1:nfactors])
    vecs <- Re(e$vectors[, 1:nfactors])
    
    vals[vals < 0] <- 0
    loadings <- vecs %*% diag(sqrt(vals))
    
    new_h2 <- rowSums(loadings^2)
    new_h2 <- pmin(new_h2, 1)
    
    change <- max(abs(new_h2 - h2))
    h2 <- new_h2
    if (change < tol) {
      converged <- TRUE
      break
    }
  }
  if (!converged) warning("Did not converge within max_iter.")
  
  if (rotate == "varimax") {
    rotated <- stats::varimax(loadings, normalize=FALSE)
    loadings <- rotated$loadings
  }
  
  rownames(loadings) <- rownames(R)
  colnames(loadings) <- paste0("PA", 1:nfactors)
  
  list(loadings = loadings, 
  communalities = h2, 
  uniqueness = 1 - h2, 
  iterations = iter, 
  converged = converged)


}

same_size_noise_matrix <- function(X) {
    set.seed(42)
    # Create a matrix of Gaussian noise with the same dimensions as X
    noise_matrix <- matrix(rnorm(nrow(X) * ncol(X)), nrow = nrow(X), ncol = ncol(X))
    colnames(noise_matrix) <- colnames(X)
    rownames(noise_matrix) <- rownames(X)
    return(noise_matrix)
    }

# NSA-Flow Regularized Factor Analysis
# This is a from-scratch implementation of Factor Analysis (FA) with NSA-Flow regularization.
# It uses power iteration to estimate initial loadings, then applies NSA-Flow for regularization (e.g., sparsity via soft-thresholding).
# Assumes nsa_flow is an external function that performs non-smooth optimization flow with soft_polar retraction for L1-like regularization.
# Includes incremental updating of loadings via warm-start Y0 and optional annealing of w (regularization strength).

nsa_fa_flow <- function(
  data = NULL,
  R = NULL,
  nfactors,
  rotate = c("varimax", "none", "promax", "oblimin"),  # New: rotation option
  nsa_w = 0.5,  # Target regularization strength (w); annealed if anneal_w=TRUE
  anneal_w = FALSE,  # Whether to anneal w from 0 to nsa_w over iterations
  nsa_max_iter = 1000,
  max_iter = 100,
  power_iter = 100,
  tol = 1e-5,
  eta_h = 0.1,  # Damping for communality updates
  verbose = TRUE,
  energy_tol = 1e-4,
  seed = 123,
  scores = c("regression", "none", "Bartlett"),
  ...
) {
  if (!requireNamespace("psych", quietly = TRUE))
    stop("Package 'psych' is required for scoring if used.")

  set.seed(seed)
  scores <- match.arg(scores)
  rotate <- match.arg(rotate)  # New: match rotate

  # Prepare correlation matrix R
  if (is.null(R)) {
    if (is.null(data)) stop("Either 'R' or 'data' must be provided.")
    R <- stats::cor(data, use = "pairwise.complete.obs")
  }
  if (!is.matrix(R)) R <- as.matrix(R)
  p <- nrow(R)
  if (p != ncol(R)) stop("R must be square.")
  if (nfactors >= p) {
    warning("nfactors >= p; setting nfactors = p-1")
    nfactors <- p - 1
  }

  var_names <- rownames(R)
  if (is.null(var_names)) var_names <- colnames(R)

  # Regularize R for positive definiteness
  eig <- eigen(R, symmetric = TRUE)
  eps_eig <- 1e-6
  eig$values[eig$values < eps_eig] <- eps_eig
  R_reg <- eig$vectors %*% diag(eig$values) %*% t(eig$vectors)
  dimnames(R_reg) <- list(var_names, var_names)

  # Initial communalities (SMC approximation)
  small_diag <- 1e-3
  invRR <- tryCatch(solve(R_reg + diag(small_diag, p)), error = function(e) NULL)
  if (is.null(invRR)) {
    h2 <- rep(0.5, p)
  } else {
    h2 <- pmin(pmax(1 - 1 / diag(invRR), 0.1), 0.9)
  }

  # Bookkeeping
  converged <- FALSE
  energy_trace <- numeric(0)
  last_loadings <- NULL
  best_loadings <- NULL
  best_energy <- Inf
  final_nsa_result <- NULL
  Phi <- NULL  # New: for oblique factor correlations if applicable

  if (verbose) message("Starting NSA-Flow Regularized FA ...")

  for (iter in seq_len(max_iter)) {
    # Build modified R with communalities on diagonal
    R_mod <- R_reg
    diag(R_mod) <- h2

    # Power iteration for initial orthonormal basis and loadings
    if (!is.null(last_loadings) && all(dim(last_loadings) == c(p, nfactors))) {
      L <- last_loadings / sqrt(rowSums(last_loadings^2) + 1e-10)  # Normalize previous as warm start
    } else {
      L <- matrix(rnorm(p * nfactors, sd = 0.1), nrow = p, ncol = nfactors)
    }

    for (k in seq_len(power_iter)) {
      L <- R_mod %*% L
      # Orthonormalize via QR
      qrL <- qr(L)
      L <- qr.Q(qrL)
      if (ncol(L) < nfactors) {
        # Pad if rank deficient
        more <- matrix(rnorm(p * (nfactors - ncol(L)), sd = 1e-3), nrow = p)
        L <- cbind(L, qr.Q(qr(more - L %*% (t(L) %*% more)))[, seq_len(nfactors - ncol(L))])
      }
      L[!is.finite(L)] <- 0
    }

    # Compute Rayleigh quotients for scaling
    vals_vec <- diag(t(L) %*% R_mod %*% L)
    vals_vec[vals_vec < 0] <- 0
    loadings_power <- L %*% diag(sqrt(vals_vec))
    loadings_power[!is.finite(loadings_power)] <- 0
    if (!is.null(var_names)) rownames(loadings_power) <- var_names
    colnames(loadings_power) <- paste0("PA", seq_len(nfactors))

    # Anneal w if enabled (start from 0, linear to nsa_w)
    w_iter <- if (anneal_w) nsa_w * (iter / max_iter) else nsa_w

    # Apply NSA-Flow regularization with incremental update (Y0 from power loadings)
    nsa_result <- tryCatch({
      nsa_flow(
        Y0 = as.matrix(loadings_power),
        w = w_iter,
        max_iter = nsa_max_iter,
        retraction = "soft_polar",
        plot = TRUE,
        ...
      )
    }, error = function(e) {
      warning("nsa_flow failed at iteration ", iter, ": ", conditionMessage(e))
      NULL
    })

    if (!is.null(nsa_result) && is.matrix(nsa_result$Y)) {
      loadings_post <- nsa_result$Y
      final_nsa_result <- nsa_result
    } else {
      # Fallback to power loadings if NSA fails
      loadings_post <- loadings_power
    }
    loadings_post[!is.finite(loadings_post)] <- 0

    if (!is.null(Phi)) {
      recon_R <- loadings_post %*% Phi %*% t(loadings_post)
    } else {
      recon_R <- loadings_post %*% t(loadings_post)
    }
    energy_pre <- norm(R_reg - recon_R, "F") / norm(R_reg, "F")

    # --- Rotation (new: applied after NSA-flow, before energy) ---
    if (rotate != "none") {
      rot_fun <- switch(rotate,
                        varimax = stats::varimax,
                        promax  = psych::promax,
                        oblimin = psych::oblimin)
      rot_res <- tryCatch({
        rot_fun(loadings_post, normalize = FALSE)
      }, error = function(e) {
        warning("Rotation failed; using unrotated loadings.")
        list(loadings = loadings_post)
      })
      loadings_post <- as.matrix(rot_res$loadings)
      if ("Phi" %in% names(rot_res)) Phi <- rot_res$Phi  # Capture Phi for oblique
      loadings_post[!is.finite(loadings_post)] <- 0
      if (!is.null(var_names)) rownames(loadings_post) <- var_names
    }
    colnames(loadings_post) <- paste0("PA", seq_len(nfactors))

    # Energy: relative Frobenius reconstruction error (handle oblique with Phi if available)
    if (!is.null(Phi)) {
      recon_R <- loadings_post %*% Phi %*% t(loadings_post)
    } else {
      recon_R <- loadings_post %*% t(loadings_post)
    }
    energy <- norm(R_reg - recon_R, "F") / norm(R_reg, "F")

    # Update best if improved
    if (energy < best_energy) {
      best_energy <- energy
      best_loadings <- loadings_post
    }

    # Communality update with damping
    new_h2 <- pmin(rowSums(loadings_post^2), 1)
    delta_h2 <- new_h2 - h2
    h2 <- (1 - eta_h) * h2 + eta_h * new_h2
    max_delta_h2 <- max(abs(delta_h2))

    # Record
    energy_trace <- c(energy_trace, energy)
    last_loadings <- loadings_post

    if (verbose) {
      message(sprintf("Iter %03d: max|Δh2|=%.6g | Energy (pre)=%.6g | Energy (post)=%.6g | bestEnergy=%.6g | w=%.3g",
                      iter, iter, max_delta_h2, energy_pre, energy, best_energy, w_iter))
    }

    # Convergence check (energy change and delta h2)
    if (iter > 5) {
      echange <- abs(diff(tail(energy_trace, 2)))
      if (echange < tol && max_delta_h2 < tol) {
        converged <- TRUE
        if (verbose) message("Converged.")
        break
      }
      if (energy < energy_tol) {
        converged <- TRUE
        if (verbose) message("Converged (energy < tol).")
        break
      }
    }
  }

  if (!converged && verbose) warning("Did not converge within max_iter.")

  final_loadings <- best_loadings %||% loadings_post

  # Optional factor scores
  factor_scores <- NULL
  if (!is.null(data) && scores != "none") {
    data_mat <- as.matrix(data)
    S <- stats::cov(data_mat, use = "pairwise.complete.obs")
    invS <- tryCatch(solve(S), error = function(e) ginverse(S))  # Pseudoinverse fallback
    Lf <- final_loadings
    if (scores == "regression") {
      B <- invS %*% Lf %*% solve(t(Lf) %*% invS %*% Lf)
      factor_scores <- scale(data_mat) %*% B
    } else if (scores == "Bartlett") {
      Psi <- diag(1 - h2 + 1e-12)
      inner <- solve(t(Lf) %*% solve(Psi) %*% Lf)
      B <- inner %*% t(Lf) %*% solve(Psi)
      factor_scores <- scale(data_mat) %*% t(B)
    }
    colnames(factor_scores) <- paste0("PA", seq_len(nfactors))
  }

  out <- list(
    loadings = final_loadings,
    communalities = h2,
    uniqueness = 1 - h2,
    converged = converged,
    iterations = iter,
    energy_trace = energy_trace,
    factor_scores = factor_scores,
    nsa_result = final_nsa_result,
    best_energy = best_energy,
    rotation = rotate,  # New: add rotation to output
    Phi = Phi  # New: add Phi if applicable
  )
  class(out) <- "nsa_fa_flow"

  if (verbose && length(energy_trace) > 1) {
    plot(energy_trace, type = "b", main = "Energy Trace", ylab = "Energy", xlab = "Iteration")
  }

  invisible(out)
}

```


```{r alg,echo=FALSE}

library(DiagrammeR)

a1=grViz("
digraph NSA_FA {
  graph [layout = dot, rankdir=TB]

  # Nodes
  node [shape=ellipse, style=filled, color=lightblue, fontname=Helvetica]
  input_data [label='Input: Data Matrix X or Correlation R']
  init_comm [label='Initialize Communalities h2']
  modify_corr [label='Modify Correlation: R_mod = diag(h2) + off-diagonal']

  node [shape=box, style=filled, color=lightgreen]
  power_iter [label='Power Iteration for Principal Axes']
  rotate [label='Optional Rotation (varimax, promax, oblimin)']
  nsa_flow [label='NSA-Flow Regularization\n(Y on manifold, optional non-negativity)']
  update_h2 [label='Update Communalities: h2']
  check_conv [label='Check Convergence']

  node [shape=ellipse, style=filled, color=lightcoral]
  output [label='Output: Loadings, Communalities, Factor Scores']

  # Edges
  input_data -> init_comm -> modify_corr -> power_iter -> rotate -> nsa_flow -> update_h2 -> check_conv
  check_conv -> power_iter [label='Not converged', style=dashed]
  check_conv -> output [label='Converged', style=bold]
}
")


library(DiagrammeR)

a2=grViz("
digraph NSA_FA {
  graph [rankdir=TB, fontsize=12, nodesep=0.3, ranksep=0.4]

  # Nodes
  Data [label='Input Data (X)', shape=box]
  CorMat [label='Compute Correlation Matrix (R)', shape=box]
  InitComm [label='Initialize Communalities (h²)', shape=box]
  PowerIter [label='Power Iteration: Principal Axes', shape=box]
  NSAFlow [label='NSA-Flow Fit: Optimize Loadings (Y)', shape=box]
  UpdateH2 [label='Update Communalities (h²)', shape=box]
  ConvergeCheck [label='Convergence Check', shape=diamond]
  FactorScores [label='Compute Factor Scores (optional)', shape=box]
  Output [label='Output Loadings, h², Energy Trace', shape=box]

  # Edges
  Data -> CorMat -> InitComm -> PowerIter -> NSAFlow -> UpdateH2 -> ConvergeCheck
  ConvergeCheck -> PowerIter [label='No']
  ConvergeCheck -> FactorScores [label='Yes']
  FactorScores -> Output
}
")


library(DiagrammeR)

a3=grViz("
digraph NSA_FA_horizontal {
  graph [rankdir=LR, fontsize=12, nodesep=0.4, ranksep=0.4]

  # Nodes
  Data [label='Input Data (X)', shape=box]
  CorMat [label='Compute\nCorrelation Matrix (R)', shape=box]
  InitComm [label='Initialize\nCommunalities (h²)', shape=box]
  PowerIter [label='Power Iteration:\nPrincipal Axes', shape=box]
  NSAFlow [label='NSA-Flow Fit:\nOptimize Loadings (Y)', shape=box]
  UpdateH2 [label='Update\nCommunalities (h²)', shape=box]
  ConvergeCheck [label='Convergence\nCheck', shape=diamond]
  FactorScores [label='Compute Factor\nScores (optional)', shape=box]
  Output [label='Output:\nLoadings, h², Energy Trace', shape=box]

  # Edges
  Data -> CorMat -> InitComm -> PowerIter -> NSAFlow -> UpdateH2 -> ConvergeCheck
  ConvergeCheck -> PowerIter [label='No', fontsize=10]
  ConvergeCheck -> FactorScores [label='Yes', fontsize=10]
  FactorScores -> Output
}
")


a2


```


# Comparison: Psychometric Dataset

We use **real psychometric data** (`psych::bfi`) with 25 personality items and > 1000 participants.
We compare how each method recovers interpretable factors.

```{r real-data}
data(bfi)
bfi_data <- bfi %>% select(A1:A5, C1:C5, E1:E5, N1:N5, O1:O5) %>% na.omit()
nrow(bfi_data)
```

## NSA.FA vs PAF Loadings

```{r real-data-fit}
# dook
nsaval=0.50
# c("brent", "grid", "armijo", "golden", "adaptive", "default")
lrval='adaptive'
myo='lars'
nsa_max_iter=500
if ( ! exists("nn") ) nn=TRUE
rot='varimax'
# bfi_data_tx=transform_matrix(data.matrix(bfi_data),'frob')$Xs*sqrt(prod(dim(bfi_data)))
bfi_data_tx=transform_matrix(data.matrix(bfi_data),'minmax')$Xs
#############################################################################    
if ( ! exists("real_A") | ! exists("real_B") | TRUE ) {
    real_A <- paf_standalone(bfi_data_tx, nfactors = 5, rotate = rot)
    real_B <- nsa_fa_flow( data = bfi_data_tx, nfactors = 5, max_iter = 50,
        rotate = rot, nsa_w = nsaval, initial_learning_rate = lrval,
        optimizer = myo, nsa_max_iter = nsa_max_iter, apply_nonneg = nn )
    }
############################################################################# 
```

## Factor Loadings Comparison

```{r real-loadings-compare,echo=FALSE}
# Convert loadings to data frames
loadA <- as.data.frame(real_A$loadings[, ])
loadB <- as.data.frame(real_B$loadings[, ])

# Compute absolute difference in loadings
load_diff <- abs(loadA) - abs(loadB)

# Add row names as a column for melting
load_diff$Item <- rownames(load_diff)

# Melt the data frame
melted_diff <- melt(load_diff, id.vars = "Item", variable.name = "Factor", value.name = "Diff")

# Create the heatmap with improved aesthetics
ggplot(melted_diff, aes(x = Factor, y = Item, fill = Diff)) +
  geom_tile(color = "white", linewidth = 0.1) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, name = "Difference") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(size = 10),
        panel.grid = element_blank()) +
  labs(title = "Difference in Absolute Loadings: PAF vs NSA.FA", x = "Factor", y = "Item")
```

**Interpretation**: This heatmap shows the differences in absolute factor loadings between PAF and NSA.FA. Red tiles indicate items where PAF has higher loadings, blue where NSA.FA has higher, and white where they are similar. Larger differences suggest where the regularization in NSA.FA alters factor-item associations, potentially leading to more sparse or interpretable structures.

## Explained Variance Comparison

```{r variance-comparison,echo=FALSE}
var_explained_A <- colSums(loadA^2)
var_explained_B <- colSums(loadB^2)

df_var <- data.frame(
  Factor = paste0("F", 1:5),
  PAF = var_explained_A,
  `NSA.FA` = var_explained_B
) %>%
  pivot_longer(cols = c("PAF", "NSA.FA"), names_to = "Method", values_to = "Variance")

ggplot(df_var, aes(x = Factor, y = Variance, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  scale_fill_manual(values = c("PAF" = "#0072B2", "NSA.FA" = "#D55E00")) +
  theme_minimal(base_size = 14) +
  labs(title = "Variance Explained by Factor", y = "Sum of Squared Loadings", fill = "Method")
```

**Interpretation**: The bar chart compares the variance explained by each factor for both methods. Higher bars indicate factors that account for more variance in the data. Differences between methods highlight how NSA regularization may redistribute variance across factors, potentially emphasizing more meaningful latent structures.

# Visualizing Factor Structure (Real Data)

## Scree Plot of Eigenvalues

```{r scree-plot}
fa_parallel <- fa.parallel(bfi_data_tx, fa = "fa", n.iter = 20, show.legend = TRUE, main = "Scree Plot with Parallel Analysis")
```

**Interpretation**: The scree plot displays eigenvalues from the actual data (blue) against those from simulated random data (red). The point where the actual eigenvalues drop below the random ones suggests the number of retainable factors (here, around 5). This helps validate the choice of 5 factors for both methods.

## Factor Correlation Circle (Varimax Rotated)

```{r factor-circle}

fa_result <- real_B

fa_df <- as.data.frame(fa_result$loadings[,1:2])
fa_df$Item <- colnames(bfi_data_tx)


ggplot(fa_df, aes(x = PA1, y = PA2)) +
  geom_hline(yintercept = 0, color = "gray60", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "gray60", linetype = "dashed") +
  geom_point(color = "#0072B2", size = 3, alpha = 0.8) +
  geom_text(aes(label = Item), hjust = 0.5, vjust = -1, size = 3.5, check_overlap = TRUE) +
  coord_equal() +
  theme_minimal(base_size = 14) +
  labs(title = "Factor Correlation Circle (First Two Factors - NSA.FA)",
       x = "Factor 1", y = "Factor 2")
```

**Interpretation**: This plot projects items onto the plane of the first two factors. Points near the circle's edge have high communalities explained by these factors. Clustering of items indicates groups associated with the same latent trait (e.g., personality dimensions). Distances from the origin reflect the strength of association.

```{r varx0,echo=FALSE}

# ---- Model Fit Indices: compare simple CFA models derived from PAF vs Alt FA ----

# Ensure lavaan and plotting libs are available
if (!requireNamespace("lavaan", quietly = TRUE)) install.packages("lavaan")
library(lavaan)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)

# Compute the two factor models directly for clarity
set.seed(123)
L_A <- as.matrix(real_A$loadings[])
L_B <- as.matrix(real_B$loadings[])

# Use BFI covariance structure
bfi_clean <- na.omit(bfi_data_tx)
S <- cov(bfi_clean)
n <- nrow(bfi_clean)

# Build simple CFA syntax based on max loadings
build_simple_model <- function(L_matrix, prefix = "F") {
  if (is.null(rownames(L_matrix))) rownames(L_matrix) <- paste0("V", seq_len(nrow(L_matrix)))
  assignments <- apply(abs(L_matrix), 1, which.max)
  lines <- sapply(seq_len(ncol(L_matrix)), function(f) {
    items <- rownames(L_matrix)[assignments == f]
    if (length(items) == 0) return(paste0(prefix, f, " =~ 0*dummy"))
    paste0(prefix, f, " =~ ", paste(items, collapse = " + "))
  })
  paste(lines, collapse = "\n")
}

model_A_str <- build_simple_model(L_A)
model_B_str <- build_simple_model(L_B)

# Fit both models via lavaan CFA
fit_A <- try(cfa(model_A_str, sample.cov = S, sample.nobs = n, fixed.x = FALSE, estimator = "ML"), silent = TRUE)
fit_B <- try(cfa(model_B_str, sample.cov = S, sample.nobs = n, fixed.x = FALSE, estimator = "ML"), silent = TRUE)

# Helper to safely extract indices
extract_indices <- function(fit) {
  if (inherits(fit, "try-error")) {
    return(tibble(RMSEA = NA, CFI = NA, TLI = NA, SRMR = NA, BIC = NA, converged = FALSE))
  }
  fm <- fitMeasures(fit, c("rmsea","cfi","tli","srmr","bic"))
  tibble(
    RMSEA = round(fm["rmsea"], 3),
    CFI   = round(fm["cfi"], 3),
    TLI   = round(fm["tli"], 3),
    SRMR  = round(fm["srmr"], 3),
    BIC   = round(fm["bic"], 1),
    converged = inspect(fit, "converged")
  )
}

fit_A_idx <- extract_indices(fit_A)
fit_B_idx <- extract_indices(fit_B)

fit_comp <- bind_rows(
  tibble(Method = "PAF Standalone") %>% bind_cols(fit_A_idx),
  tibble(Method = "NSA.FA") %>% bind_cols(fit_B_idx)
)

# ----- Diagnostics Table -----
fit_comp %>%
  gt() %>%
  tab_header(
    title = "Model Fit Indices — PAF vs NSA.FA (CFA Validation on BFI Data)"
  ) %>%
  tab_spanner(
    label = "Fit Statistics",
    columns = c(RMSEA, CFI, TLI, SRMR, BIC)
  ) %>%
  cols_align(
    align = "center",
    columns = everything()
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(60)
  )

```
**Interpretation**: The table presents CFA fit indices for models derived from each FA method. Lower RMSEA, SRMR, and BIC indicate better fit; higher CFI and TLI (closer to 1) are desirable. Convergence status shows if the model estimation succeeded. NSA.FA may show improved fit due to regularization.

# ---- Visualization 1: Rescaled Fit Indices ----

```{r varx1,echo=FALSE}
viz_df <- fit_comp %>%
  pivot_longer(cols = c(RMSEA, CFI, TLI, SRMR, BIC),
               names_to = "Metric", values_to = "Value") %>%
  mutate(Metric = factor(Metric, levels = c("RMSEA", "SRMR", "CFI", "TLI", "BIC")))

viz_df <- viz_df %>%
  group_by(Metric) %>%
  mutate(
    Value_scaled = case_when(
      Metric %in% c("CFI","TLI") ~ rescale(Value, to = c(0,1), from = c(0.7,1)),
      Metric %in% c("RMSEA","SRMR") ~ rescale(-Value, to = c(0,1), from = c(-0.2,-0.0)),
      Metric == "BIC" ~ rescale(-Value, to = c(0,1)),
      TRUE ~ rescale(Value)
    )
  )

ggplot(viz_df, aes(x = Metric, y = Value_scaled, fill = Method)) +
  geom_col(position = position_dodge(0.7), width = 0.6, color = "black") +
  geom_text(aes(label = round(Value,3)), position = position_dodge(0.7),
            vjust = -0.3, size = 3.5) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_fill_manual(values = c("PAF Standalone" = "#00BFC4", "NSA.FA" = "#F8766D")) +
  labs(
    title = "Rescaled CFA Fit Indices — PAF vs NSA.FA",
    subtitle = "Higher values indicate better fit (rescaled for comparability)",
    y = "Relative Fit (0–100%)", x = ""
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 25, hjust = 1))
```

**Interpretation**: This bar chart rescales fit indices to a 0-100% scale for easy comparison (higher is better). It visually emphasizes which method performs better on each metric, aiding in overall assessment of model quality.

# ---- Visualization 2: Variable-level Communality Comparison ----
```{r varx2,echo=FALSE}
h2_A <- real_A$communalities
h2_B <- real_B$communalities
comm_df <- tibble(
  Variable = names(h2_A),
  PAF = h2_A,
  `NSA.FA` = h2_B
) %>%
  pivot_longer(cols = c(PAF, `NSA.FA`), names_to = "Method", values_to = "Communality")

ggplot(comm_df, aes(x = reorder(Variable, Communality), y = Communality, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  coord_flip() +
  scale_fill_manual(values = c("PAF" = "#00BFC4", "NSA.FA" = "#F8766D")) +
  theme_minimal(base_size = 14) +
  labs(title = "Communalities by Variable — PAF vs NSA.FA",
       subtitle = "Shows how much variance each factor method explains per variable",
       x = "Variable", y = "Proportion of Variance Explained")
```

**Interpretation**: Communalities represent the proportion of each variable's variance explained by the factors. Higher values indicate better fit; differences between methods show where NSA.FA may provide more or less explanation, potentially due to regularization effects.

# Overall Rating and Comparison Panel


```{r compare_loadings_at_last, echo=FALSE,fig.width=10, fig.height=4}
# ---- Train-Test Split ----
n <- nrow(bfi_data_tx)
train_idx <- sample(seq_len(n), size = floor(0.8 * n))
train_data <- bfi_data_tx[train_idx, ]
test_data  <- bfi_data_tx[-train_idx, ]

# ---- Compute Correlation Matrix on Training Set ----
R_train <- cor(train_data, use = "pairwise.complete.obs")

# ---- Method A: PAF ----
real_A_train <- paf_standalone(train_data, nfactors = 5, rotate = rot)
L_A <- real_A_train$loadings

# ---- Method B: NSA.FA ----
real_B_train <-nsa_fa_flow( data = train_data, nfactors = 5, max_iter = 50,
    rotate = rot, nsa_w = nsaval, initial_learning_rate = lrval, 
    optimizer = myo, nsa_max_iter = nsa_max_iter, apply_nonneg = nn )

L_B <- real_B_train$loadings


# ---- Compute Communalities & Orthogonality Defect ----
h2_A <- rowSums(L_A^2)
h2_B <- rowSums(L_B^2)

iod_A <- ANTsR::invariant_orthogonality_defect(L_A)
iod_B <- ANTsR::invariant_orthogonality_defect(L_B)

comm_df <- tibble(
  Variable = rownames(L_A),
  PAF = h2_A,
  `NSA.FA` = h2_B
) %>%
  pivot_longer(cols = c(PAF, `NSA.FA`), names_to = "Method", values_to = "Communality")

iod_tbl <- tibble(
  Method = c("PAF", "NSA.FA"),
  OrthogonalityDefect = c(iod_A, iod_B)
)

# ---- Plot Communalities ----
ggplot(comm_df, aes(x = reorder(Variable, Communality), y = Communality, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  coord_flip() +
  scale_fill_manual(values = c("PAF" = "#0073C2", "NSA.FA" = "#EFC000")) +
  labs(title = "Variable-level Communalities: PAF vs NSA.FA",
       y = "Proportion of Variance Explained", x = "Variable") +
  theme_minimal(base_size = 14)

```

**Interpretation**: Similar to the earlier communality plot, this reinforces the variance explanation per variable, ordered by value for clarity.


## Predictive Validity on Train Set

```{r predictive_validity_train, echo=FALSE,fig.width=10, fig.height=5}
# ---- Train Factor Scores ----
scores_train_A <- as.matrix(train_data) %*% as.matrix(L_A)
scores_train_B <- as.matrix(train_data) %*% as.matrix(L_B)

# ---- Big 5 Traits ----
traits <- list(
  Agreeableness = "^A",
  Conscientiousness = "^C",
  Extraversion = "^E",
  Neuroticism = "^N",
  Openness = "^O"
)

outcomes <- lapply(traits, function(pat) rowSums(train_data[, grep(pat, colnames(train_data))]))

# ---- Compute Correlations ----
cor_matrix_A <- sapply(1:5, function(f) sapply(outcomes, function(out) cor(scores_train_A[,f], out, use = "pairwise.complete.obs")))
cor_matrix_B <- sapply(1:5, function(f) sapply(outcomes, function(out) cor(scores_train_B[,f], out, use = "pairwise.complete.obs")))

cor_df_all <- data.frame(
  Factor = rep(paste0("F", 1:5), length(traits)),
  Trait = rep(names(traits), each = 5),
  PAF = abs(as.vector(cor_matrix_A)),
  `NSA.FA` = abs(as.vector(cor_matrix_B))
) %>%
  pivot_longer(cols = c(PAF, `NSA.FA`), names_to = "Method", values_to = "Correlation")

# ---- Heatmap Plot ----
ggplot((cor_df_all), aes(x = Factor, y = Trait, fill = Correlation)) +
  geom_tile(color = "white", linewidth = 0.1) +
  geom_text(aes(label = round(Correlation, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, name = "Correlation") +
  facet_wrap(~Method) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 0),
        panel.grid = element_blank()) +
  labs(title = "Factor Scores Correlations with Big Five Traits in Train Set",
       x = "Factor", y = "Trait")

```

## Predictive Validity on Test Set

```{r predictive_validity, echo=FALSE,fig.width=10, fig.height=5}
# ---- Train Factor Scores ----
scores_train_A <- as.matrix(train_data) %*% as.matrix(L_A)
scores_train_B <- as.matrix(train_data) %*% as.matrix(L_B)

# ---- Apply to Test Data ----
scores_test_A <- as.matrix(test_data) %*% as.matrix(L_A)
scores_test_B <- as.matrix(test_data) %*% as.matrix(L_B)

# ---- Big 5 Traits ----
traits <- list(
  Agreeableness = "^A",
  Conscientiousness = "^C",
  Extraversion = "^E",
  Neuroticism = "^N",
  Openness = "^O"
)

outcomes <- lapply(traits, function(pat) rowSums(test_data[, grep(pat, colnames(test_data))]))

# ---- Compute Correlations ----
cor_matrix_A <- sapply(1:5, function(f) sapply(outcomes, function(out) cor(scores_test_A[,f], out, use = "pairwise.complete.obs")))
cor_matrix_B <- sapply(1:5, function(f) sapply(outcomes, function(out) cor(scores_test_B[,f], out, use = "pairwise.complete.obs")))

cor_df_all <- data.frame(
  Factor = rep(paste0("F", 1:5), length(traits)),
  Trait = rep(names(traits), each = 5),
  PAF = abs(as.vector(cor_matrix_A)),
  `NSA.FA` = abs(as.vector(cor_matrix_B))
) %>%
  pivot_longer(cols = c(PAF, `NSA.FA`), names_to = "Method", values_to = "Correlation")

# ---- Heatmap Plot ----
ggplot((cor_df_all), aes(x = Factor, y = Trait, fill = Correlation)) +
  geom_tile(color = "white", linewidth = 0.1) +
  geom_text(aes(label = round(Correlation, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, name = "Correlation") +
  facet_wrap(~Method) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 0),
        panel.grid = element_blank()) +
  labs(title = "Factor Scores Correlations with Big Five Traits in Test Set",
       x = "Factor", y = "Trait")

```

**Interpretation**: This heatmap displays the correlations between extracted factor scores and the total scores of each Big Five personality trait in the test set. Higher absolute correlations along the diagonal (if factors align with traits) indicate better recovery of the known structure. The NSA.FA method may show stronger or more specific correlations due to regularization, providing evidence of improved predictive validity and structure recovery.

```{r compare_loadings_at_last_cont2, echo=FALSE,fig.width=10, fig.height=4}
# ---- Reconstruct Test Correlations ----

# Factor-implied covariance matrices
# Sigma_hat = L %*% t(L) + diag(uniqueness)
uniqueness_A <- 1 - rowSums(L_A^2)
uniqueness_B <- 1 - rowSums(L_B^2)

R_hat_A <- L_A %*% t(L_A) + diag(uniqueness_A)
R_hat_B <- L_B %*% t(L_B) + diag(uniqueness_B)

# Observed correlation on test set
R_test <- cor(test_data, use = "pairwise.complete.obs")

# Melt for ggplot
melt_R_test <- melt(R_test); melt_R_test$Method <- "Observed"
melt_R_A    <- melt(R_hat_A);  melt_R_A$Method <- "PAF"
melt_R_B    <- melt(R_hat_B);  melt_R_B$Method <- "NSA.FA"

all_corr <- bind_rows(melt_R_test, melt_R_A, melt_R_B)

# Plot with improved aesthetics
ggplot(all_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white", linewidth = 0.1) +
  facet_wrap(~Method) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, name = "Correlation") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        axis.text.y = element_text(size = 8),
        panel.grid = element_blank()) +
  labs(title = "Observed vs Factor-implied Correlations", fill = "Correlation")

```

**Interpretation**: These heatmaps compare the observed test set correlations with those reproduced by each method. Closer alignment (similar patterns and values) indicates better model reproduction of the data structure. Discrepancies highlight areas where one method fits better.

## Interpretability Metrics

```{r interpretability_metrics, echo=FALSE}
# ---- Hofmann's Complexity ----
hofmann_complexity <- function(loadings) {
  lambda <- abs(as.matrix(loadings))
  sum_l2 <- rowSums(lambda^2)
  sum_l4 <- rowSums(lambda^4)
  complexity <- sum_l2^2 / sum_l4
  complexity
}

mean_comp_A <- mean(hofmann_complexity(L_A))
mean_comp_B <- mean(hofmann_complexity(L_B))

# ---- Number of Cross-Loadings (|loading| > 0.3 on >1 factor) ----
cross_load_A <- sum(rowSums(abs(as.matrix(L_A)) > 0.3) > 1)
cross_load_B <- sum(rowSums(abs(as.matrix(L_B)) > 0.3) > 1)

interp_tbl <- tibble(
  Method = c("PAF", "NSA.FA"),
  `Mean Complexity` = c(mean_comp_A, mean_comp_B),
  `Cross-Loadings` = c(cross_load_A, cross_load_B)
)

interp_tbl %>%
  gt() %>%
  tab_header(
    title = "Interpretability Metrics"
  ) %>%
  fmt_number(
    columns = `Mean Complexity`,
    decimals = 3
  ) %>%
  cols_align(
    align = "center",
    columns = everything()
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(50)
  )
```

**Interpretation**: Hofmann's complexity measures how concentrated the loadings are for each variable (lower values closer to 1 indicate simpler structure). The number of cross-loadings counts variables loading significantly on multiple factors (lower is better for interpretability). Lower values for NSA.FA demonstrate its superiority in producing more interpretable factor structures.

```{r compare_loadings_at_last_tables, echo=FALSE}
# ---- Summary Tables ----
comm_df_wide <- comm_df %>%
  pivot_wider(names_from = Method, values_from = Communality) %>%
  mutate(
    Group = case_when(
      substr(Variable, 1, 1) == "A" ~ "Agreeableness",
      substr(Variable, 1, 1) == "C" ~ "Conscientiousness",
      substr(Variable, 1, 1) == "E" ~ "Extraversion",
      substr(Variable, 1, 1) == "N" ~ "Neuroticism",
      substr(Variable, 1, 1) == "O" ~ "Openness"
    ),
    Difference = `NSA.FA` - PAF
  ) %>%
  arrange(Group, Variable) %>%
  select(Group, Variable, PAF, `NSA.FA`, Difference)

comm_df_wide %>%
  group_by(Group) %>%
  gt(rowname_col = "Variable") %>%
  tab_header(
    title = "Communalities by Method"
  ) %>%
  tab_spanner(
    label = "Communalities",
    columns = c(PAF, `NSA.FA`)
  ) %>%
  fmt_number(
    columns = c(PAF, `NSA.FA`, Difference),
    decimals = 3
  ) %>%
  data_color(
    columns = Difference,
    fn = scales::col_numeric(
      palette = "RdBu",
      domain = c(-1, 1)
    )
  ) %>%
  cols_align(
    align = "center",
    columns = c(PAF, `NSA.FA`, Difference)
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(60),
    row_group.background.color = "lightgray",
    row_group.font.weight = "bold"
  )

iod_tbl %>%
  gt() %>%
  tab_header(
    title = "Orthogonality Defects"
  ) %>%
  cols_align(
    align = "center",
    columns = OrthogonalityDefect
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(40)
  )
```

**Interpretation of Tables**:

- **Communalities**: As above, higher values are better. The table is grouped by personality trait for better organization. The Difference column highlights where NSA.FA explains more (red) or less (blue) variance than PAF.

- **Orthogonality Defects**: Lower values indicate loadings closer to perfect orthogonality post-rotation, desirable for interpretability.




# Appendix: Algorithms


# NSA-Flow Regularized Factor Analysis Algorithm

Factor analysis (FA) is a statistical method used to uncover latent structures in multivariate data by modeling observed variables as linear combinations of unobserved factors plus unique errors. Traditional FA estimates factor loadings via methods like maximum likelihood or principal axis factoring but often produces dense, hard-to-interpret solutions. To address this, we propose NSA-Flow Regularized FA, an iterative algorithm that integrates power iteration for initial loading estimation with non-smooth analysis (NSA) flow—a manifold optimization technique—for sparsity-promoting regularization.

The algorithm operates on the sample correlation matrix \( R \) (computed from data or provided directly), regularized for positive definiteness via eigenvalue shifting to ensure numerical stability. It iteratively refines a modified correlation matrix \( R_{\text{mod}} \) by placing estimated communalities (shared variances) on its diagonal. Initial loadings are derived using power iteration, which approximates the leading eigenspace of \( R_{\text{mod}} \), yielding an orthonormal basis scaled by Rayleigh quotients.

Regularization is applied via NSA-flow, a gradient-flow method toward Stiefel matrix manifolds. The flow starts from the power-iteration loadings as a warm start, with regularization strength \( w \) optionally annealed quadratically from 0 to a target value over iterations to gradually introduce sparsity while avoiding early over-penalization.

An optional rotation (varimax for orthogonality, promax or oblimin for oblique) is applied post-regularization to enhance interpretability. Reconstruction error (energy) is computed as the relative Frobenius norm \( \| R - \hat{R} \|_F / \| R \|_F \), where \( \hat{R} \) is the reproduced correlation from loadings (adjusted for factor correlations in oblique cases). Energy is evaluated both pre- and post-rotation for diagnostic purposes, with the post-rotation value guiding optimization.

Communalities are updated with damping to balance stability and adaptation, and convergence is assessed via small changes in energy and communality deltas. The final output includes the best (lowest-energy) loadings, communalities, optional factor scores (regression or Bartlett), and diagnostics like the energy trace.

This approach yields sparse, interpretable factors while maintaining good data fit, suitable for high-dimensional datasets where traditional FA may overfit. Implementation in R leverages psych for rotations and assumes an external nsa_flow function.

::: {.callout-note appearance="minimal"}
## **Algorithm 1 — NSA-Flow Optimization**

**Inputs:**  
Initial matrix \( Y_0 \in \mathbb{R}^{p \times k} \), target matrix \( X_c \);  
weights \( w_{\mathrm{PCA}}, \lambda \); step size \( \eta \); maximum iterations \( T \).

**Output:**  
Optimized matrix \( Y \), final energy \( \mathcal{E}(Y) \), iteration count \( t \).

**Procedure:**

1. Initialize \( Y \leftarrow Y_0 \), \( E_{\text{prev}} \leftarrow \mathcal{E}(Y) \)
2. **for** \( t = 1, \dots, T \):  
   a. Compute energy  
   \[
   \mathcal{E}(Y) = -\frac{w_{\mathrm{PCA}}}{2n}\|X_c Y\|_F^2 + \lambda\|Y\|_1
   \]  
   b. Compute gradient \( \nabla_Y \mathcal{E}(Y) \); set descent direction \( D \leftarrow -\nabla_Y \mathcal{E}(Y) \)  
   c. Initialize step size \( \alpha \leftarrow \eta \)  
   d. **While** Armijo condition not satisfied:  
      • \( Y_{\text{cand}} \leftarrow Y + \alpha D \)  
      • \( Y_{\text{cand}} \leftarrow \text{Retract}(Y_{\text{cand}}, \text{type}) \)  
      • **If** \( \mathcal{E}(Y_{\text{cand}}) \le \mathcal{E}(Y) + c \alpha \langle \nabla_Y, D \rangle \), **then** break  
      • **Else** \( \alpha \leftarrow \beta \alpha \)  
   e. Update \( Y \leftarrow Y_{\text{cand}} \)  
   f. **If** \( |\mathcal{E}(Y) - E_{\text{prev}}| < \varepsilon \), **then** break  
   g. Set \( E_{\text{prev}} \leftarrow \mathcal{E}(Y) \)
3. **Return** \( (Y, \mathcal{E}(Y), t) \)
:::




::: {.callout-note appearance="minimal"}
## **Algorithm 2 — NSA-Flow Regularized Factor Analysis (NSA-Flow-FA)**

**Inputs:**  
Data matrix \( X \in \mathbb{R}^{n \times p} \) or correlation matrix \( R \in \mathbb{R}^{p \times p} \);  
number of factors \( k \); regularization weight \( w \);  
maximum iterations \( T_{\text{outer}} \), NSA iterations \( T_{\text{NSA}} \).

**Outputs:**  
Factor loadings \( L \), communalities \( h^2 \), uniqueness \( 1 - h^2 \),  
final reconstruction energy \( \mathcal{E} \), and rotation matrix \( \Phi \) (if oblique).

**Procedure:**

1. **Initialize:**
   - If data \( X \) is given, compute correlation \( R \).
   - Regularize \( R \) to ensure positive definiteness.
   - Initialize communalities \( h^2 \) using squared multiple correlations (SMC).
2. **Repeat for** \( t = 1, \dots, T_{\text{outer}} \):  
   a. Construct \( R^{(t)} = R \) with diagonal replaced by \( h^2 \).  
   b. Estimate initial orthonormal basis \( L^{(0)} \) using **power iteration**:  
      \[
      L^{(k+1)} \leftarrow \text{QR}(R^{(t)} L^{(k)})
      \]  
   c. Scale by eigenvalues to obtain preliminary loadings \( L_{\text{power}} \).  
   d. **Regularize via NSA-Flow:**  
      \[
      L_{\text{NSA}} \leftarrow \text{NSA-Flow}(L_{\text{power}}, w, T_{\text{NSA}})
      \]  
      (enforces non-negativity, sparsity, and near-orthogonality).  
   e. Apply optional **rotation** (varimax, promax, or oblimin) → \( L_{\text{rot}} \).  
   f. Update communalities with damping:  
      \[
      h^2 \leftarrow (1 - \eta_h) h^2 + \eta_h \,\text{rowSums}(L_{\text{rot}}^2)
      \]  
   g. Compute reconstruction energy  
      \[
      \mathcal{E}^{(t)} = \|R - L_{\text{rot}} L_{\text{rot}}^\top\|_F / \|R\|_F
      \]  
      and check convergence:  
      stop if \( |\Delta \mathcal{E}| < \varepsilon \) and \( \max|\Delta h^2| < \varepsilon \).
3. **Return:**  
   Final \( L = L_{\text{rot}} \), \( h^2 \), uniqueness \( 1 - h^2 \),  
   factor scores (if computed), and energy trace.
:::
