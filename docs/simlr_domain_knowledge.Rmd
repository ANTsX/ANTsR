---
title: "SiMLR with Domain Knowledge Guidance"
author: "B. Avants"
date: "September 12, 2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ANTsR)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(plotly)
library(subtyper)
library(gt)
library(DT)
library(heatmaply)
library(proxy)
library(clue)
set.seed(123)  # For reproducibility
```

## Introduction

We demonstrate a method for incorporating soft prior knowledge into SiMLR (`simlr` in `ANTsR`). SiMLR integrates high-dimensional multi-modal data into low-dimensional embeddings, leveraging inter-modality similarities and domain priors for sparsity and interpretability.

We simulate three "modalities" of information:

- **Cortex**: Structural features (e.g., cortical thickness, n=300 subjects, p=500 features).

- **White Matter**: Diffusion metrics (e.g., fractional anisotropy, p=400 features).

- **Functional Connectivity**: Connectivity strengths (p=300 features).

The simulation embeds 6 cognitive domains as latent signals, with modality-specific corruptions and noise. Domain matrices map domains to features, serving as "ground truth" priors. We evaluate SiMLR with and without domain guidance, varying `energyType`, `mixAlg`, and `domainLambdas`, and evaluate with several relevant metrics.

### Metrics

- **Cross-view Embedding Correlation**: Mean RV coefficient between test embeddings (higher better).

- **Domain Latent Recovery R²**: Mean squared canonical correlations between concatenated test embeddings and true latents (higher better).

- **Domain Alignment Score**: Mean cosine similarity between feature weights (\(V_i\)) and domain matrices (higher better).

- **Reconstruction Error**: Mean normalized Frobenius norm of SiMLR reconstruction on test data (lower better).

- **Cross-Modal Reconstruction Error**: Mean normalized Frobenius error predicting each view’s test data from other views’ concatenated embeddings via linear regression (lower better).

## Technical Background

Similarity-driven Multi-view Linear Reconstruction (SiMLR), implemented in the ANTsR package, is a dimensionality reduction framework designed to integrate high-dimensional, multimodal data by exploiting inter-modality relationships and incorporating prior domain knowledge to enhance interpretability [Avants et al., 2021]. SiMLR extends canonical correlation analysis (CCA) and related multi-view methods by incorporating regularization terms that promote sparsity and alignment with prior knowledge, making it particularly suited for neuroimaging applications where modalities like structural MRI, diffusion tensor imaging (DTI), and functional MRI (fMRI) must be fused to uncover latent biological signals [Stone et al., 2020].

### SiMLR Objective Function

SiMLR operates on a set of \( K \) data matrices \( \{X_k \in \mathbb{R}^{n \times p_k}\}_{k=1}^K \), where \( n \) is the number of subjects and \( p_k \) is the number of features for modality \( k \). The goal is to find low-dimensional embeddings \( \{U_k \in \mathbb{R}^{n \times d}\}_{k=1}^K \) and corresponding feature weight matrices \( \{V_k \in \mathbb{R}^{p_k \times d}\}_{k=1}^K \), where \( d \) is the number of latent components (\( d \ll p_k \)). These are derived by optimizing an objective function that balances cross-modal similarity, reconstruction fidelity, and alignment with domain knowledge priors.

The SiMLR objective function can be expressed as:

\[
J = \sum_{k=1}^K \sum_{j \neq k} \text{Energy}(U_k, U_j) + \sum_{k=1}^K \lambda_k \|\hat{X}_k - X_k\|_F^2 + \sum_{k=1}^K \gamma_k \text{Prior}(V_k, Z_k),
\]

where:
- **Energy Term**: The first term, \( \text{Energy}(U_k, U_j) \), measures the similarity between embeddings of different modalities. For the "acc" energy type, this is typically the RV coefficient, a multivariate generalization of the squared correlation coefficient, defined as:

\[
\text{RV}(U_k, U_j) = \frac{\text{tr}(U_k^T U_j U_j^T U_k)}{\sqrt{\text{tr}(U_k^T U_k)^2 \text{tr}(U_j^T U_j)^2}}.
\]

For the "regression" energy type, it involves a linear regression loss between embeddings. The choice of energy type influences how SiMLR captures inter-modality relationships, with "acc" emphasizing correlation and "regression" focusing on predictive accuracy.

- **Reconstruction Term**: The second term, \( \|\hat{X}_k - X_k\|_F^2 \), minimizes the Frobenius norm of the reconstruction error, where \( \hat{X}_k = U_k V_k^T \) is the reconstructed data matrix for modality \( k \), and \( \lambda_k \) is a weighting parameter. This ensures that the low-dimensional embeddings preserve the original data’s structure.

- **Domain Alignment Term**: The third term, \( \text{Prior}(V_k, Z_k) \), incorporates domain knowledge through a quadratic alignment energy, defined as:

\[
\text{Prior}(V_k, Z_k) = -\gamma_k \|Z_k V_k\|_F^2,
\]

where \( Z_k \in \mathbb{R}^{m_k \times p_k} \) is a prior matrix encoding domain knowledge for modality \( k \), with \( m_k \) representing the number of prior domains (e.g., cognitive domains like processing speed or memory), and \( \gamma_k \) (controlled by `domainLambda`) is a non-negative scalar controlling the strength of the prior. The term \( \|Z_k V_k\|_F^2 \) is the squared Frobenius norm of the projection of the feature weight matrix \( V_k \) onto the prior matrix \( Z_k \), normalized by the product of the dimensions of \( Z_k \) and \( V_k \):

\[
\|Z_k V_k\|_F^2 = \sum_{i,j} (Z_k V_k)_{i,j}^2.
\]

This negative quadratic form encourages the feature weights \( V_k \) to align with the structure of \( Z_k \), maximizing the projection’s magnitude and thus promoting components that capture features relevant to the specified domains. Unlike L1-norm regularization, which promotes sparsity, the quadratic Frobenius norm encourages smooth alignment with the prior, balancing interpretability with flexibility to capture data-driven patterns.

### Domain Knowledge Integration

The domain alignment term is critical for enhancing the biological interpretability of SiMLR embeddings in neuroimaging. The prior matrix \( Z_k \) encodes domain knowledge, such as mappings of cortical regions to cognitive functions or white matter tracts to motor skills, derived from anatomical atlases, functional parcellations, or prior studies [Avants et al., 2021]. For example, in the cortical thickness modality, \( Z_k \) might assign higher weights to temporal lobe features for the memory domain based on neuroscientific evidence. The negative sign in \( -\gamma_k \|Z_k V_k\|_F^2 \) ensures that the objective function minimizes the negative of the alignment energy, effectively maximizing \( \|Z_k V_k\|_F^2 \), which aligns the learned feature weights with the prior domains.

The `domainLambda` parameter (\( \gamma_k \)) controls the trade-off between data-driven discovery and prior-guided alignment. A low \( \gamma_k \) allows SiMLR to prioritize the energy and reconstruction terms, capturing novel signals not represented in \( Z_k \), while a high \( \gamma_k \) enforces stronger adherence to the prior, potentially at the cost of overfitting to the provided knowledge. The normalization by \( \text{nrow}(Z_k) \times \text{ncol}(V_k) \) in the energy calculation ensures scale invariance, making the term robust to differences in the dimensionality of \( Z_k \) and \( V_k \). This formulation is particularly valuable in neuroimaging, where high-dimensional data (e.g., thousands of voxels or connectivity measures) must be reduced to interpretable components that align with biological constructs [Stone et al., 2020].

### Optimization and Mixing Algorithms

SiMLR optimizes the objective function iteratively using algorithms like Adam (`optimizationStyle = "adam"`) with constraints such as orthogonality (`constraint = "orthox0.02x1"`) and sparsity (`sparsenessQuantiles`). The mixing algorithm (`mixAlg`), such as independent component analysis (ICA) or principal component analysis (PCA), determines how latent components are separated. ICA, which assumes non-Gaussian sources, is often more effective for capturing complex signals in neuroimaging data, while PCA assumes Gaussianity and maximizes variance [Avants et al., 2021]. The interplay between the energy type, mixing algorithm, and domain alignment term shapes the balance between data-driven and prior-informed embeddings, making SiMLR a flexible tool for multimodal integration.

This formulation enables SiMLR to address the challenges of multimodal neuroimaging, where modalities have distinct dimensionalities, noise profiles, and biological underpinnings. By incorporating domain knowledge via the quadratic alignment energy, SiMLR enhances the biological plausibility of the resulting embeddings, making it a powerful tool for applications like biomarker discovery or clinical stratification [Stone et al., 2020].

## Data Simulation

Simulate n=300 subjects (200 train, 100 test), d=6 domains.

```{r simulate-data}
n <- 300
n_train <- 200
domains <- c("processing_speed", "language", "memory", "executive_functioning", "motor", "visuospatial_skills")
d <- length(domains)
p <- c(500, 400, 300)
snr <- 0.5

# Domain latents
domain_latents <- matrix(rnorm(n * d), nrow = n)
domain_latents <- scale(domain_latents, center = TRUE, scale = TRUE)
domain_latents_train <- domain_latents[1:n_train, ]
domain_latents_test <- domain_latents[(n_train+1):n, ]

# Domain knowledge matrices
dlist <- lapply(1:3, function(i) {
  dm <- matrix(0, d, p[i])
  features_per_domain <- p[i] / d
  for (j in 1:d) {
    start <- round((j-1) * features_per_domain + 1)
    end <- round(j * features_per_domain)
    dm[j, start:end] <- runif(end - start + 1, 0.5, 1)
    other_domain <- sample((1:d)[-j], 1)
    overlap_idx <- sample(start:end, size = round(0.1 * (end - start)))
    dm[other_domain, overlap_idx] <- runif(length(overlap_idx), 0.1, 0.3)
  }
  dm <- dm / rowSums(dm, na.rm = TRUE)
  rownames(dm) <- domains
  dm/max(dm)
})
names(dlist) <- c("cortex", "white_matter", "functional_connectivity")

# Modality-specific corruptions
corrupt_cortex <- svd(matrix(rnorm(d * d), d, d))$u
corrupt_wm <- svd(matrix(rnorm(d * d), d, d))$u
corrupt_fc <- svd(matrix(rnorm(d * d), d, d))$u

# Generate data
signal_sd <- 1
noise_sd <- signal_sd / snr
cortex <- domain_latents %*% corrupt_cortex %*% dlist[[1]] + 
          matrix(rnorm(n * p[1], sd = noise_sd), n, p[1]) + 
          matrix(rnorm(n * p[1], sd = noise_sd * 0.2), n, p[1])
white_matter <- domain_latents %*% corrupt_wm %*% dlist[[2]] + 
                matrix(rnorm(n * p[2], sd = noise_sd), n, p[2]) + 
                matrix(rnorm(n * p[2], sd = noise_sd * 0.2), n, p[2])
func_conn <- domain_latents %*% corrupt_fc %*% dlist[[3]] + 
             matrix(rnorm(n * p[3], sd = noise_sd), n, p[3]) + 
             matrix(rnorm(n * p[3], sd = noise_sd * 0.2), n, p[3])

# Split train/test
matlist_train <- list(cortex = cortex[1:n_train, ], white_matter = white_matter[1:n_train, ], functional_connectivity = func_conn[1:n_train, ])
matlist_test <- list(cortex = cortex[(n_train+1):n, ], white_matter = white_matter[(n_train+1):n, ], functional_connectivity = func_conn[(n_train+1):n, ])
```

## Helper Functions for Metrics

```{r metrics-funcs}
# Cross-view RV coefficient
cross_view_rv <- function(emb_list) {
  tot <- 0
  count <- 0
  for (k in 1:(length(emb_list)-1)) {
    for (j in (k+1):length(emb_list)) {
      myrv <- rvcoef(emb_list[[k]], emb_list[[j]])
      tot <- tot + myrv
      count <- count + 1
    }
  }
  tot / count
}

# Domain recovery R²
domain_recovery_r2 <- function(emb_list, true_latents) {
  # Input validation
  if (!is.list(emb_list) || length(emb_list) == 0) {
    stop("emb_list must be a non-empty list of matrices or vectors.")
  }
  if (!is.matrix(true_latents)) {
    stop("true_latents must be a matrix.")
  }
  
  # Ensure all elements in emb_list are matrices
  emb_list <- lapply(emb_list, as.matrix)
  
  # Check number of rows consistency
  n_rows <- unique(sapply(emb_list, nrow))
  if (length(n_rows) != 1 || n_rows != nrow(true_latents)) {
    stop("All elements in emb_list and true_latents must have the same number of rows.")
  }
  
  # Check for valid dimensions
  if (ncol(true_latents) == 0) {
    stop("true_latents has zero columns.")
  }
  
  # Initialize vector to store mean squared canonical correlations for each embedding
  mean_cc2 <- numeric(length(emb_list))
  
  # Compute canonical correlations for each embedding
  for (i in seq_along(emb_list)) {
    emb <- emb_list[[i]]
    
    # Check for non-zero columns
    if (ncol(emb) == 0) {
      warning(paste("Embedding", i, "has zero columns. Skipping."))
      mean_cc2[i] <- 0
      next
    }
    
    # Scale data, handling zero-variance columns
    emb_scaled <- scale(emb, center = TRUE, scale = TRUE)
    true_latents_scaled <- scale(true_latents, center = TRUE, scale = TRUE)
    
    # Replace NA values (from zero-variance columns) with 0
    emb_scaled[is.na(emb_scaled)] <- 0
    true_latents_scaled[is.na(true_latents_scaled)] <- 0
    
    # Compute cross-correlation matrix
    R <- t(emb_scaled) %*% true_latents_scaled / (nrow(emb_scaled) - 1)
    
    # Compute canonical correlations via SVD
    cc <- svd(R, nu = 0, nv = 0)$d
    
    # Ensure canonical correlations are between 0 and 1 (due to numerical precision)
    cc <- pmin(pmax(cc, 0), 1)
    
    # Store mean squared canonical correlations for this embedding
    mean_cc2[i] <- mean(cc^2, na.rm = TRUE)
  }
  
  # Return average of mean squared canonical correlations across embeddings
  mean(mean_cc2, na.rm = TRUE)
}

# Domain alignment (optimized)
domain_alignment <- function(basis1, basis2) {
  similarity_matrix <- abs(proxy::simil(basis1, basis2, method = "correlation"))
  match_result <- clue::solve_LSAP(similarity_matrix, maximum = TRUE)
  mean(diag(similarity_matrix[, match_result]))
}

# SiMLR reconstruction error
recon_error <- function(simlr_result, mat_test) {
  pp <- predictSimlr(mat_test, simlr_result)
  errs <- mean(pp$finalErrors)
  mean(errs, na.rm = TRUE)
}

# New: Cross-modal reconstruction error
cross_modal_recon <- function(emb_train_list, emb_test_list, mat_train_list, mat_test_list) {
  errs <- numeric(length(mat_train_list))
  for (i in 1:length(mat_train_list)) {
    X_train <- mat_train_list[[i]]
    X_test <- mat_test_list[[i]]
    other_inds <- setdiff(1:length(mat_train_list), i)
    concat_train <- do.call(cbind, emb_train_list[other_inds])
    concat_test <- do.call(cbind, emb_test_list[other_inds])
    trdf=data.frame(concat_train)
    tedf=data.frame(concat_test)
    mylm=lm(X_train ~ . , data=trdf )
    pp=data.matrix(predict( mylm, newdata=tedf))
    errs[i] <- norm(X_test - data.matrix(pp), "F") / norm(X_test, "F")
  }
  mean(errs, na.rm = TRUE)
}

# Permutation test
perm_test <- function(result, matlist_train, dlist, n_perm = 10, domlam) {
  orig_r2 <- domain_recovery_r2(lapply(1:3, function(i) matlist_train[[i]] %*% result$v[[i]]), domain_latents_train)
  perm_r2 <- numeric(n_perm)
  for (p in 1:n_perm) {
    set.seed(p)
    perm_dlist <- lapply(dlist, function(dm) {
      dm_perm <- dm
      for (j in 1:nrow(dm)) dm_perm[j, ] <- dm[j, sample(ncol(dm))]
      dm_perm
    })
    result_perm <- simlr(
      data_matrices = matlist_train,
      smoothingMatrices = smoothingMats,
      iterations = myits,
      sparsenessQuantiles = myspar,
      positivities = mypos,
      initialUMatrix = initu,
      mixAlg = as.character(result$mixAlg),
      energyType = as.character(result$energyType),
      constraint = myorth,
      domainLambdas = rep(domlam, length(matlist_train)),
      domainMatrices = perm_dlist,
      optimizationStyle = myopt,
      randomSeed = p,
      verbose = FALSE
    )
    emb_perm <- lapply(1:3, function(i) matlist_train[[i]] %*% result_perm$v[[i]])
    perm_r2[p] <- domain_recovery_r2(emb_perm, domain_latents_train)
  }
  t_stat <- t.test(orig_r2 - perm_r2, alternative = "greater")$statistic
  list(r2 = orig_r2, t_stat = t_stat)
}
```

## Parameter Grid and Runs

```{r run-grid, eval=!exists("results")}
smoothingMats <- lapply(matlist_train, function(m) diag(ncol(m)))
myspar <- rep(0.8, length(matlist_train))
mypos <- rep("positive", length(matlist_train))
myorth <- "orthox0.02x1"
initu <- initializeSimlr(matlist_train, round(d*1.2) )
myits <- 200
myopt <- "adam"
#############################################
params_grid <- expand.grid(
  energyType = c("acc", "regression"),# , "kurtosis", "logcosh" ),
#  mixAlg = c("pca", "ica", "svd"),
  mixAlg = c("pca",'ica'),
  domainLambda = c( 0,  0.2, 0.5, 1, 2, 5 )/10.0
)
results <- list()
metrics <- data.frame(params_grid, cross_rv = NA, recovery_r2 = NA, alignment = NA, recon_err = NA, cross_modal = NA, perm_t = NA)

for (row in 1:nrow(params_grid)) {
  par <- params_grid[row, ]
  key <- paste(par$energyType, par$mixAlg, par$domainLambda, sep="_")
  result <- simlr(
    data_matrices = matlist_train,
    smoothingMatrices = smoothingMats,
    iterations = myits,
    sparsenessQuantiles = myspar,
    positivities = mypos,
    initialUMatrix = initu,
    energyType = as.character(par$energyType),
    mixAlg = as.character(par$mixAlg),
    constraint = myorth,
    optimizationStyle = myopt,
    verbose = FALSE,
    domainMatrices = dlist,
    domainLambdas = rep(par$domainLambda, length(matlist_train)),
    randomSeed = 123
  )
  result$mixAlg <- as.character(par$mixAlg)
  results[[key]] <- result
  emb_train <- lapply(1:3, function(i) matlist_train[[i]] %*% result$v[[i]])
  emb_test <- lapply(1:3, function(i) matlist_test[[i]] %*% result$v[[i]])
  metrics$cross_rv[row] <- cross_view_rv(emb_test)
  metrics$recovery_r2[row] <- domain_recovery_r2(emb_test, domain_latents_test)
  metrics$alignment[row] <- mean(sapply(1:length(dlist), function(i) domain_alignment(dlist[[i]], t(result$v[[i]]))))
  metrics$recon_err[row] <- recon_error(result, matlist_test)
  metrics$cross_modal[row] <- cross_modal_recon(emb_train, emb_test, matlist_train, matlist_test)
  if (par$domainLambda > 0  ) {
    perm_res <- perm_test(result, matlist_train, dlist, n_perm = 5, domlam = par$domainLambda)
    metrics$perm_t[row] <- perm_res$t_stat
  }
}
```

## Visualizations and Comparisons

### Result Metrics Table

```{r metrics-table}
library(gt)
library(gtExtras)
# datatable(
#  metrics %>% mutate(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal, perm_t), ~round(., 4))),
##  options = list(pageLength = 10, order = list(list(3, "desc"))),
#  caption = "Metrics for All Runs (Sortable)"
# )
#
gt_tbl <- metrics %>% 
  mutate(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal, perm_t), round, 4)) %>%
  arrange(desc(recovery_r2)) 
library(reactable)

reactable(
  gt_tbl,
  # Table-level options
  bordered = TRUE,
  striped = TRUE,
  highlight = TRUE,
  resizable = TRUE,
  pagination = FALSE,
  defaultSorted = list(recovery_r2 = "desc"),
  
  # Global styling
  defaultColDef = colDef(
    align = "center",
    headerStyle = list(
      background = "#f8f9fa",
      borderBottom = "2px solid #dee2e6",
      fontWeight = "600",
      fontSize = "14px"
    ),
    style = list(
      padding = "6px 8px",
      borderBottom = "1px solid #dee2e6",
      fontSize = "13px"
    )
  ),
  
  # Column-specific tweaks
  columns = list(
    mpg = colDef(name = "Miles/Gallon", format = colFormat(digits = 1)),
    cyl = colDef(name = "Cylinders"),
    hp  = colDef(name = "Horsepower"),
    wt  = colDef(name = "Weight (1000 lbs)", format = colFormat(digits = 2))
  ),
  
  # Theme (with custom style overrides)
  theme = reactableTheme(
    borderColor = "#dee2e6",
    stripedColor = "#f8f9fa",
    highlightColor = "#f1f3f5",
    cellPadding = "8px 12px",
    color = "#212529",
    backgroundColor = "white",
    style = list(
      borderRadius = "8px",       # rounded corners
      borderWidth = "1px",        # table border thickness
      borderStyle = "solid",
      borderColor = "#dee2e6"
    )
  )
)
#########
```

### Heatmap of Metrics

```{r heatmap, fig.width=10, fig.height=6}
metrics_long <- metrics %>%
  pivot_longer(cols = c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), names_to = "Metric", values_to = "Value") %>%
  mutate(Param = paste(energyType, mixAlg, sep="+"))
heatmaply(
  metrics_long %>% dplyr::select(Param, domainLambda, Metric, Value) %>%
    pivot_wider(names_from = Metric, values_from = Value),
  Rowv = FALSE, Colv = FALSE, scale = "column", colors = viridis::viridis(100),
  main = "Metrics Heatmap by Parameters"
)
```

### Radar Chart of Metrics

```{r radar-chart, fig.width=8, fig.height=6}
metrics_norm <- metrics %>%
  mutate(
    cross_rv = (cross_rv - min(cross_rv)) / (max(cross_rv) - min(cross_rv)),
    recovery_r2 = (recovery_r2 - min(recovery_r2)) / (max(recovery_r2) - min(recovery_r2)),
    alignment = (alignment - min(alignment)) / (max(alignment) - min(alignment)),
    recon_err = 1 - (recon_err - min(recon_err)) / (max(recon_err) - min(recon_err)),
    cross_modal = 1 - (cross_modal - min(cross_modal)) / (max(cross_modal) - min(cross_modal))
  )

# Top guided, best unguided, and group averages
top_guided <- metrics_norm %>% filter(domainLambda > 0) %>% slice(which.max(alignment))
best_unguided <- metrics_norm %>% filter(domainLambda == 0) %>% slice(which.max(recovery_r2))
guided_avg <- metrics_norm %>% filter(domainLambda > 0) %>% summarise(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), mean))
unguided_avg <- metrics_norm %>% filter(domainLambda == 0) %>% summarise(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), mean))
best_unguided$perm_t=0
plot_ly(type = "scatterpolar") %>%
  add_trace(
    r = as.numeric(top_guided[1, c("cross_rv", "recovery_r2", "alignment", "recon_err", "cross_modal")]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = paste(top_guided$energyType, top_guided$mixAlg, top_guided$domainLambda, sep="_"),
    fill = "toself"
  ) %>%
  add_trace(
    r = as.numeric(best_unguided[1, c("cross_rv", "recovery_r2", "alignment", "recon_err", "cross_modal")]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = paste(best_unguided$energyType, best_unguided$mixAlg, "0", sep="_"),
    fill = "toself"
  ) %>%
  add_trace(
    r = as.numeric(guided_avg[1, ]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = "Guided Avg",
    fill = "toself"
  ) %>%
  add_trace(
    r = as.numeric(unguided_avg[1, ]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = "Unguided Avg",
    fill = "toself"
  ) %>%
  layout(
    polar = list(radialaxis = list(visible = TRUE, range = c(0, 1))),
    showlegend = TRUE,
    title = "Normalized Metrics Comparison"
  )
```

### Grouped Bar Plot with Significance

```{r bar-plot, fig.width=10, fig.height=6}
metrics_long <- metrics %>%
  mutate(Group = ifelse(domainLambda > 0, "Guided", "Unguided")) %>%
  pivot_longer(cols = c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), names_to = "Metric", values_to = "Value")
if ( FALSE ) {
# T-tests for significance
t_tests <- metrics_long %>%
  group_by(Metric, energyType) %>%
  summarise(
    p_value = t.test(Value[Group == "Guided"], Value[Group == "Unguided"], alternative = "two.sided")$p.value,
    .groups = "drop"
  ) %>%
  mutate(Signif = case_when(p_value < 0.01 ~ "***", p_value < 0.05 ~ "**", p_value < 0.1 ~ "*", TRUE ~ ""))
}
ggplot(metrics_long, aes(x = Group, y = Value, fill = Group)) +
  geom_bar(stat = "summary", fun = "mean", position = position_dodge()) +
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.4, position = position_dodge(0.45)) +
  facet_grid(Metric ~ energyType, scales = "free_y") +
#  geom_text(data = t_tests, aes(x = 1.5, y = Inf, label = Signif), vjust = 1.5, size = 5) +
  labs(title = "Guided vs. Unguided Metrics by Energy Type", y = "Value") +
  theme_minimal() +
  scale_fill_manual(values = c("Guided" = "#1b9e77", "Unguided" = "#d95f02"))
```

### Multi-Metric Line Plot

```{r line-plot, fig.width=12, fig.height=8}
ggplot(metrics_long, aes(x = domainLambda, y = Value, color = paste(energyType, mixAlg, sep="+"))) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  facet_wrap(~Metric, scales = "free_y", ncol = 3) +
  labs(title = "Metrics vs. Domain Lambda", x = "Domain Lambda", y = "Value") +
  theme_minimal() +
  scale_color_discrete(name = "Energy + MixAlg")
```

### Scaled Line Plot

```{r scaled-line-plot, fig.width=12, fig.height=8, echo=FALSE}
library(dplyr)
library(ggplot2)

# Create a combined factor for energyType and mixAlg
metrics_long <- metrics_long %>%
  mutate(energy_mix = paste(energyType, mixAlg, sep = "+")) %>%
  group_by(Metric, energy_mix) %>%
  mutate(Value_normalized = (Value - min(Value, na.rm = TRUE)) / 
         (max(Value, na.rm = TRUE) - min(Value, na.rm = TRUE))) %>%
  ungroup()

# Create the ggplot with normalized values
ggplot(metrics_long, aes(x = domainLambda, y = Value_normalized, 
                        color = energy_mix)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  facet_wrap(~Metric, scales = "free_y", ncol = 3) +
  labs(title = "Normalized Metrics vs. Domain Lambda", 
       x = "Domain Lambda", 
       y = "Normalized Value (0 to 1)") +
  theme_minimal() +
  scale_color_discrete(name = "Energy + MixAlg")
  
```

### Energy Decomposition for Top Configurations

```{r energy-decomp, fig.width=12, fig.height=8}
top_configs <- metrics %>% filter(domainLambda > 0) %>% arrange(desc(alignment)) %>% slice(1:3)
plots <- lapply(1:nrow(top_configs), function(i) {
  key <- paste(top_configs$energyType[i], top_configs$mixAlg[i], top_configs$domainLambda[i], sep="_")
  p <- plot_energy_decomposition(results[[key]], "cortex")
  p + ggtitle(paste("Energy Decomp:", key))
})
grid.arrange(grobs = plots, ncol = 2)
```

## Example Runs

### Basic (No Domain, acc + pca)

```{r basic-example, fig.width=8, fig.height=6}
result_basic <- results[["acc_pca_0"]]
emb_basic <- lapply(1:3, function(i) matlist_train[[i]] %*% result_basic$v[[i]])
plot_energy_decomposition(result_basic, "cortex")
cat("Recovery R²:", domain_recovery_r2(emb_basic, domain_latents_train), "\n")
cat("Cross-Modal Recon:", cross_modal_recon(emb_basic, lapply(1:3, function(i) matlist_test[[i]] %*% result_basic$v[[i]]), matlist_train, matlist_test), "\n")
```

### With Domain Guidance (Top Config)

```{r domain-example, fig.width=8, fig.height=6}
result_domain <- results[[paste(top_configs$energyType[1], top_configs$mixAlg[1], top_configs$domainLambda[1], sep="_")]]
emb_domain <- lapply(1:3, function(i) matlist_train[[i]] %*% result_domain$v[[i]])
plot_energy_decomposition(result_domain, "cortex")
cat("Recovery R²:", domain_recovery_r2(emb_domain, domain_latents_train), "\n")
cat("Cross-Modal Recon:", cross_modal_recon(emb_domain, lapply(1:3, function(i) matlist_test[[i]] %*% result_domain$v[[i]]), matlist_train, matlist_test), "\n")
```

### Feature Weights Inspection

To gain deeper insight into how domain priors influence the learned representations, we examine the feature weights for the cortical modality in the top guided configuration. We compute the domain contributions by projecting the absolute values of the loading matrix V onto the prior domain matrix. This reveals the extent to which each learned component captures specific cognitive domains, promoting interpretability in neuroimaging contexts where linking features to biological constructs is essential.

```{r inspect-weights}
v_cortex <- result_domain$v[[1]]
domain_contrib <- dlist[[1]] %*% abs(v_cortex)
colnames(domain_contrib) <- paste0("Comp", 1:ncol(domain_contrib))
rownames(domain_contrib) <- domains

# Print the matrix
print(domain_contrib)

# Visualize as heatmap for better interpretation
heatmaply(domain_contrib, 
          colors = viridis::viridis(100), 
          main = "Domain Contributions to Learned Components (Cortex)",
          xlab = "Learned Components", ylab = "Cognitive Domains")
          
```

## Summary Tables

### Top 3 Guided Runs

```{r top3-guided}
top3 <- metrics %>%
  filter(domainLambda > 0) %>%
  arrange(desc(alignment)) %>%
  slice(1:3) %>%
  mutate(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal, perm_t), round, 3)) 

top3 %>%  gt() %>%
  tab_header(title = "Top 3 Guided Configurations")
```

### Best Unguided Baseline

```{r best-unguided}
best_unguided <- metrics %>%
  filter(domainLambda == 0) %>%
  arrange(desc(recovery_r2)) %>%
  slice(1) %>%
  mutate(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), round, 3)) 
  
best_unguided%>%
  gt() %>%
  tab_header(title = "Best Unguided Configuration")
```

### Guided vs. Unguided Summary

```{r summary-table}
summary_tbl <- tibble(
  Run = c("Most Significant Guided", "Best Unguided"),
  Perm_t = c(round(top3$perm_t[1], 2), NA),
  Recovery_R2 = c(round(top3$recovery_r2[1], 3), round(best_unguided$recovery_r2[1], 3)),
  Alignment = c(round(top3$alignment[1], 3), round(best_unguided$alignment[1], 3)),
  Recon_Error = c(round(top3$recon_err[1], 3), round(best_unguided$recon_err[1], 3)),
  Cross_Modal = c(round(top3$cross_modal[1], 3), round(best_unguided$cross_modal[1], 3))
) %>%
  mutate(
    Delta_vs_Unguided = c(
      paste0("+", round(top3$recovery_r2[1] - best_unguided$recovery_r2[1], 3), " R²; ",
             "+", round(top3$alignment[1] - best_unguided$alignment[1], 3), " Align; ",
             "-", round(best_unguided$recon_err[1] - top3$recon_err[1], 3), " Recon Err; ",
             "-", round(best_unguided$cross_modal[1] - top3$cross_modal[1], 3), " Cross-Modal"),
      "—"
    )
  ) %>%
  gt() %>%
  tab_header(title = "Guided vs. Unguided Summary")
summary_tbl
```

## Discussion

This study leverages the similarity-driven multi-view linear reconstruction (SiMLR) framework within ANTsR to integrate high-dimensional, multimodal neuroimaging data, incorporating domain knowledge priors to enhance interpretability and statistical power, as inspired by prior work in neuroimaging and multi-omics [Avants et al., 2021; Stone et al., 2020]. Using simulated data mimicking cortical thickness, white matter diffusion metrics, and functional connectivity, we demonstrate that guided SiMLR configurations—particularly those employing the "acc" energy type with independent component analysis (ICA) mixing and moderate domain lambdas—substantially outperform unguided baselines across key metrics, aligning with the need for biologically grounded, reproducible analyses in quantitative neuroscience.

The top-performing guided configuration (acc, ICA, domainLambda = 0.20) achieves a cross-view RV coefficient of 0.766, a domain latent recovery R² of 0.836, and a domain alignment score of 0.915, with a permutation test t-statistic of 37.99, indicating robust improvements over random prior permutations. These results highlight SiMLR’s ability to capture inter-modality similarities and align learned embeddings with predefined cognitive domains (e.g., processing speed, memory), as evidenced by the high alignment score. Compared to the best unguided baseline (regression, ICA, domainLambda = 0, recovery R² = 0.712, alignment = 0.582), the guided configuration yields a 0.124 increase in recovery R² and a 0.333 increase in alignment, underscoring the value of domain priors in enhancing the recovery of latent signals and their biological interpretability. The reconstruction error (385.963) and cross-modal reconstruction error (0.983) in this configuration are also notably lower than the unguided baseline (reconstruction error = 389.756, cross-modal error = 0.982), suggesting improved generalizability across modalities.

From a neuroimaging perspective, these findings are particularly relevant for integrating multimodal data, such as structural MRI, diffusion tensor imaging (DTI), and resting-state fMRI, where each modality captures complementary aspects of brain structure and function [Stone et al., 2020]. The high alignment scores in guided configurations (e.g., 0.915 for acc, ICA, domainLambda = 0.20) indicate that SiMLR effectively maps learned components to biologically meaningful domains, facilitating hypothesis-driven research, such as linking cortical thinning to cognitive deficits or white matter alterations to executive dysfunction. This mirrors applications in studies of traumatic brain injury or Alzheimer’s disease, where priors from anatomical atlases or functional parcellations enhance interpretability [Avants et al., 2021]. The low cross-modal reconstruction errors (e.g., 0.981 for acc, ICA, domainLambda = 0.05) suggest that guided embeddings can predict one modality from others, a critical capability for clinical settings where missing data or modality-specific artifacts are common, such as in longitudinal studies of neurodegenerative disorders.

The permutation tests provide strong statistical evidence of the non-random contribution of domain priors, with t-statistics ranging from 7.958 to 83.734 across guided configurations, peaking at 83.734 for regression, ICA, domainLambda = 0.02. This indicates that the priors significantly enhance performance in a manner robust to random perturbations, reinforcing confidence in SiMLR’s reliability for neuroimaging applications. However, the performance trade-offs are notable: configurations with higher domain lambdas (e.g., 0.50, acc, ICA) show a decline in recovery R² (0.781) and alignment (0.829) compared to moderate lambdas (e.g., 0.20), suggesting that excessive prior weighting may constrain the model’s flexibility to capture latent signals not fully represented in the domain matrices. This trade-off is evident in the acc, ICA configurations, where cross-view RV (0.766 at domainLambda = 0.20) and recovery R² (0.836) peak at moderate lambdas before declining at higher values.

Interestingly, the regression energy type with ICA mixing shows consistent performance across domain lambdas (e.g., recovery R² ≈ 0.694, alignment ≈ 0.591), with high permutation t-statistics (e.g., 83.689 for domainLambda = 0.10), suggesting robustness but limited sensitivity to prior strength. In contrast, the acc energy type with ICA mixing is highly sensitive to domainLambda, with cross-view RV increasing from 0.465 (unguided) to 0.766 (domainLambda = 0.20) and alignment from 0.574 to 0.915, indicating that the acc energy type benefits more from prior guidance. The PCA mixing algorithm, however, underperforms with the acc energy type (e.g., recovery R² = 0.645 at domainLambda = 0.02), likely due to its assumption of Gaussian signal components, whereas ICA’s ability to capture non-Gaussian structures aligns better with the simulated data’s complexity.

The simulation’s design, while controlled, assumes linear mappings and Gaussian noise, which may not fully capture the nonlinearities, scanner artifacts, or subject-specific variability in real neuroimaging data [Avants et al., 2021]. The high reconstruction errors (e.g., 393.123 for acc, PCA, domainLambda = 0) in some configurations suggest sensitivity to parameter choices, particularly for PCA-based methods, which may struggle with the sparse, noisy nature of multimodal data. Future validation on large-scale datasets, such as the UK Biobank or the Adolescent Brain Cognitive Development (ABCD) study, is critical to confirm these findings in the presence of real-world heterogeneity. Additionally, the recovery R² metric, while effective for evaluating latent signal recovery, may require normalization or calibration to avoid numerical amplification, as values like 0.836 (acc, ICA, domainLambda = 0.20) suggest potential scaling issues in the canonical correlation computation.

Practically, SiMLR’s ability to integrate domain priors aligns with the ethos of reproducible, open-science neuroimaging championed by the ANTs ecosystem [Avants et al., 2011]. The feature weights inspection (Section "Feature Weights Inspection") reveals how learned components correspond to cognitive domains, enhancing biological interpretability and supporting applications like biomarker discovery in psychiatric disorders or monitoring longitudinal brain changes post-injury [Stone et al., 2020]. The method’s robustness to cross-modal prediction makes it suitable for clinical scenarios with incomplete datasets, such as predicting functional connectivity from structural MRI in patients with missing fMRI scans.

Limitations include the simulation’s reliance on synthetic data, which simplifies biological variability. Nonlinear relationships between modalities and latents, common in real-world neuroimaging, may require extensions like kernel-based SiMLR or integration with deep learning within the ANTsX framework. The choice of domainLambda remains a critical tuning parameter, as overly strong priors (e.g., domainLambda = 0.50) may overfit to the provided knowledge, potentially missing novel signals. Future work should explore adaptive lambda selection, validate against clinical datasets with ground-truth outcomes (e.g., Alzheimer’s or traumatic brain injury cohorts), and integrate SiMLR with tools like Neuroconductor for scalable analysis [Muschelli et al., 2019]. Incorporating priors from advanced parcellations or connectomics could further enhance performance, particularly for disorders with distributed neural signatures.

In summary, this study demonstrates that SiMLR, augmented with domain knowledge priors, provides a robust, interpretable framework for multimodal neuroimaging analysis. By achieving superior alignment, latent recovery, and cross-modal prediction, it offers a promising tool for uncovering biologically meaningful patterns, advancing precision neuroscience and clinical biomarker discovery.