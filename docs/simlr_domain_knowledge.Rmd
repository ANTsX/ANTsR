---
title: "Similarity-driven Multi-view Linear Reconstruction (SiMLR)"
author: "B. Avants"
date: "September 12, 2025"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    theme: flatly
    code_folding: hide
    df_print: paged
    highlight: tango
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \usepackage{geometry}
  - \geometry{margin=1in}
  - \usepackage{booktabs}
  - \usepackage{graphicx}
  - \usepackage{natbib}
  - \usepackage{hyperref}
  - \usepackage{xcolor}
  - \hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}
  - \usepackage{times}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(ANTsR)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(plotly)
library(subtyper)
library(gt)
library(DT)
library(heatmaply)
library(proxy)
library(clue)
set.seed(123)  # For reproducibility
```



## Data Simulation


```{r simulate-data}
n <- 600
n_train <- 400
domains <- c("processing_speed", "language", "memory", "executive_functioning", "motor", "visuospatial_skills")
d <- length(domains)
p <- c(500, 400, 300)
snr <- 0.5

# Domain latents
domain_latents <- matrix(rnorm(n * d), nrow = n)
domain_latents <- scale(domain_latents, center = TRUE, scale = TRUE)
domain_latents_train <- domain_latents[1:n_train, ]
domain_latents_test <- domain_latents[(n_train+1):n, ]

# Domain knowledge matrices
dlist <- lapply(1:3, function(i) {
  dm <- matrix(0, d, p[i])
  features_per_domain <- p[i] / d
  for (j in 1:d) {
    start <- round((j-1) * features_per_domain + 1)
    end <- round(j * features_per_domain)
    dm[j, start:end] <- runif(end - start + 1, 0.5, 1)
    other_domain <- sample((1:d)[-j], 1)
    overlap_idx <- sample(start:end, size = round(0.1 * (end - start)))
    dm[other_domain, overlap_idx] <- runif(length(overlap_idx), 0.1, 0.3)
  }
  dm <- dm / rowSums(dm, na.rm = TRUE)
  rownames(dm) <- domains
  dm/max(dm)
})
names(dlist) <- c("cortex", "white_matter", "functional_connectivity")

# Modality-specific corruptions
corrupt_cortex <- svd(matrix(rnorm(d * d), d, d))$u
corrupt_wm <- svd(matrix(rnorm(d * d), d, d))$u
corrupt_fc <- svd(matrix(rnorm(d * d), d, d))$u

# Generate data
signal_sd <- 1
noise_sd <- signal_sd / snr
cortex <- domain_latents %*% corrupt_cortex %*% dlist[[1]] + 
          matrix(rnorm(n * p[1], sd = noise_sd), n, p[1]) + 
          matrix(rnorm(n * p[1], sd = noise_sd * 0.2), n, p[1])
white_matter <- domain_latents %*% corrupt_wm %*% dlist[[2]] + 
                matrix(rnorm(n * p[2], sd = noise_sd), n, p[2]) + 
                matrix(rnorm(n * p[2], sd = noise_sd * 0.2), n, p[2])
func_conn <- domain_latents %*% corrupt_fc %*% dlist[[3]] + 
             matrix(rnorm(n * p[3], sd = noise_sd), n, p[3]) + 
             matrix(rnorm(n * p[3], sd = noise_sd * 0.2), n, p[3])

# Split train/test
matlist_train <- list(cortex = cortex[1:n_train, ], white_matter = white_matter[1:n_train, ], functional_connectivity = func_conn[1:n_train, ])
matlist_test <- list(cortex = cortex[(n_train+1):n, ], white_matter = white_matter[(n_train+1):n, ], functional_connectivity = func_conn[(n_train+1):n, ])



smoothingMats <- lapply(matlist_train, function(m) diag(ncol(m)))
myspar <- rep(0.8, length(matlist_train))
mypos <- rep("positive", length(matlist_train))
myorth <- "orthox0.02x1"
initu <- initializeSimlr(matlist_train, round(d*1.2))
myits <- 200
myopt <- "lookahead"
params_grid <- expand.grid(
  energyType = c('normalized_correlation', "acc", "regression"),
  mixAlg = c("pca", "ica", 'svd'),
  domainLambda = c(0, 0.2, 0.5, 1, 2, 5) / 10.0
)

```

Simulate \( n = `r n` \) subjects (\( `r n_train` \) train, \( n - `r n_train` \) test), \( d = `r d` \) domains.

# Methods: SiMLR

Similarity-driven Multi-view Linear Reconstruction (SiMLR), implemented in the ANTsR package, is a framework for integrative dimensionality reduction across multiple views (modalities), designed to uncover shared latent structures in high-dimensional, multimodal data such as neuroimaging datasets \citep{avants2021,stone2020}. Let there be \( Z = `r length(matlist_train)` \) views, representing modalities such as cortical thickness, white matter diffusion metrics, and functional connectivity. Each view is represented as a data matrix

\[
X^{(z)} \in \mathbb{R}^{p_z \times n}, \quad z = 1, \dots, Z,
\]

where \( p_z \) denotes the number of features in view \( z \) (e.g., \( p_1 = `r ncol(matlist_train[[1]])` \) for cortex, \( p_2 = `r ncol(matlist_train[[2]])` \) for white matter, \( p_3 = `r ncol(matlist_train[[3]])` \) for functional connectivity), and \( n = `r n` \) denotes the number of samples (subjects). SiMLR assumes that a **shared latent embedding** \( U \in \mathbb{R}^{k \times n} \), with \( k \ll \min_k p_k \), captures common variation across all views. Each view is linearly associated with this shared representation through a multivariate map

\[
V^{(z)} \in \mathbb{R}^{p_z \times k}.
\]

The columns of \( U \) represent subject-level latent scores, while the columns of \( V^{(z)} \) represent multivariate loadings or spatial maps linking latent dimensions to features in each modality, such as brain regions or connectivity measures.

## SiMLR Framework and Objectives

SiMLR seeks to learn modality-specific loadings \( V^{(z)} \in \mathbb{R}^{p_z \times k} \). The framework optimizes a joint objective that balances data fidelity and regularization, formulated as:

\[
\min_{\{V^{(z)}\}} \;
\sum_{z=1}^Z \mathcal{L}\big(X^{(z)}, U^{(z)}, V^{(z)}\big)
+ \sum_{z=1}^Z R\!\left( V^{(z)} \right).
\]

### Components of the Objective

- **Data Fidelity Term \(\mathcal{L}(X^{(z)}, U^{(z)}, V^{(z)})\)**: Quantifies how well the shared embedding and view maps explain the observed data. SiMLR supports multiple fidelity measures (detailed in the "Energy Functions" subsection), such as reconstruction error (\(\| X^{(z)} - V^{(z)} U^{(z)} \|_F^2\)) or absolute canonical covariance (\(-\| \mathrm{Cov}(U^{(z)}, X^{(z)}) \|_1\)), selected via `energyType` (e.g., `"regression"`, `"acc"`). These allow SiMLR to adapt to tasks like reconstruction, correlation maximization, or cross-modal prediction. The $U^{(z)}$ matrix is updated based on the current estimates of \( V^{(z)} \) and thus is not directly optimized over.

- **Regularization Term \( R(V^{(z)}) \)**: Imposes constraints on the view maps to enhance interpretability and robustness. These include:
  - **Sparsity**: Enforced via \(\ell_1\)-norm penalties, controlled by `sparsenessQuantiles`.
  - **Smoothness**: Enforced via spatial regularizers, specified by `smoothingMatrices`.
  - **Positivity or Orthogonality**: Enforced via constraints like `constraint = "orthox0.02x1"` or `positivities`.
  - **Domain Alignment**: Encourages \( V^{(z)} \) to align with prior domain matrices \( M_z \in \mathbb{R}^{m \times p_z} \), defined as \( R(V^{(z)}) = -\gamma_z \| M_z V^{(z)} \|_F^2 \), where \( \gamma_z \) (controlled by `domainLambdas`) balances prior strength. In this study, \( M_z \) maps features to \( m = `r nrow(dlist[[1]])` \) cognitive domains (e.g., processing speed, memory).

Formally, the smoothing and sparseness component of the regularization on \( V^{(z)} \) is applied to the feature vectors. Denote \( v_{zk} \) as the \( k \)-th column of \( V^{(z)} \), \( G_z^\sigma \) as a sparse regularization matrix with rows summing to one, and \( \gamma_z \) as a scalar weight. The regularization term takes the form:

\[
\sum_{z=1}^Z \sum_{k=1}^k \gamma_z \| G_z^\sigma v_{zk} \|^+_{\ell_p},
\]

where \( \| \cdot \|^+_{\ell_p} \) is a positivity-constrained \(\ell_p\)-norm (typically \( p=0 \) or \( p=1 \)) to enforce sparsity or smoothness, tailored to neuroimaging applications (e.g., spatial adjacency for cortical features).

### Optimization Strategy

The joint optimization problem is nonconvex in \( (U^{(z)}, \{V^{(z)}\}) \). SiMLR employs **alternating minimization**:

1. **Update \( V^{(z)} \)** (fixed \( U^{(z)} \)): Solve a regularized regression or covariance maximization problem for each view, incorporating sparsity, positivity, or domain alignment constraints.
2. **Update \( U^{(z)} \)** (fixed \( \{V^{(z)}\} \)): Solve a dimensionality reduction problem (e.g., PCA, ICA) to obtain a low-dimensional representation based on \( \{X^{(j)} (V^{(j)})^\top\}_{j=1,j\ne z}^Z \) where we "leave-out" the view being updated.
3. Apply constraint projections (e.g., orthogonalization via `constraint = "orthox0.02x1"`, positivity via `positivities`).
4. Repeat until convergence or a fixed number of iterations (`iterations = `r myits``), using gradient-based methods like Adam, NADAM, stochastic gradient descent, or RMSprop (`optimizationStyle = "adam"`). Robust initializations (random, SVD-based, or other low-rank bases) are recommended \citep{kellman2017}.

### Interpretation

- \( V^{(z)} \): Interpretable multivariate maps (e.g., brain spatial maps linking latent dimensions to cortical regions or connectivity measures) for biological insights.

### Energy Functions

The data fidelity term \( \mathcal{L}(X^{(z)}, U^{(z)}, V^{(z)}) \) supports multiple objectives, each emphasizing different properties of the latent space. Below, we detail each energy function, its interpretation, strengths, limitations, and, for domain alignment, its gradient.

#### 1. Regression / Reconstruction Error

**Definition:**

\[
\mathcal{L}_{\text{reg}} = \| X^{(z)} - U^{(z)} (V^{(z)})^\top \|_F^2 \quad \text{or its centered/normalized variants.}
\]

**Interpretation:** Encourages \( U^{(z)} (V^{(z)})^\top \) to approximate \( X^{(z)} \), capturing maximum variance within each modality. Suitable for reconstructing data like cortical thickness maps in neuroimaging.

**Strengths:**
- Convex in \( V^{(z)} \), simplifying optimization.
- Prioritizes data fidelity, ensuring high variance capture.

**Limitations:**
- Focuses on within-modality variance, potentially neglecting cross-modal alignment.
- Sensitive to noise and outliers, which may dominate the Frobenius norm.

#### 2. Procrustes Correlation

**Definition:**

\[
\mathcal{L}_{\text{proc}} = -\frac{\operatorname{tr}((U^{(z)})^\top X^{(z)} V^{(z)})}{\| (U^{(z)})^\top X^{(z)} V^{(z)} \|_F}.
\]

**Interpretation:** Maximizes correlation (up to rotation/scale) between the projected data \( X^{(z)} V^{(z)} \) and \( U^{(z)} \), akin to canonical correlation analysis but normalized by the Frobenius norm. Useful for aligning functional connectivity with structural features in neuroimaging.

**Strengths:**
- Scale-invariant, focusing on geometric alignment of subspaces.
- Enhances cross-modal similarity, supporting multimodal integration.

**Limitations:**
- Non-convex, sensitive to initialization and optimization challenges.
- May prioritize alignment over reconstruction accuracy.

#### 3. Angular Distance Objective

**Definition:**

\[
\mathcal{L}_{\text{ang}} = \left\| \frac{U^{(z)}}{\| U^{(z)} \|_F} - \frac{X^{(z)} V^{(z)}}{\| X^{(z)} V^{(z)} \|_F} \right\|_F^2.
\]

**Interpretation:** Penalizes differences in subspace orientation between \( U^{(z)} \) and \( X^{(z)} V^{(z)} \), emphasizing directionality over magnitude. Ideal for applications where relative orientation matters, such as brain network alignments.

**Strengths:**
- Scale-invariant, robust to magnitude differences across modalities.
- Focuses on geometric properties of the latent space.

**Limitations:**
- May underweight explained variance, reducing reconstruction accuracy.
- Computationally complex due to normalization.

#### 4. Absolute Canonical Covariance

**Definition:**

\[
\mathcal{L}_{\text{acc}} = -\frac{\sum_i | ((U^{(z)})^\top X^{(z)} V^{(z)})_{ii} |}{\| U^{(z)} \|_F \| X^{(z)} V^{(z)} \|_F}.
\]

**Interpretation:** Maximizes absolute diagonal correlations between \( U^{(z)} \) and \( X^{(z)} V^{(z)} \), capturing canonical structure despite sign ambiguities. Relevant for aligning latent factors across modalities with positive or negative connectivity strengths.

**Strengths:**
- Robust to sign flips, ensuring consistent alignment.
- Enhances interpretability of canonical relationships.

**Limitations:**
- Focus on diagonal correlations may miss off-diagonal interactions.
- Normalization can be sensitive to small singular values.

#### 5. ICA (Independent Component Analysis)

**Definition (logcosh example):**

\[
\mathcal{L}_{\text{ICA}} = -\frac{1}{n} \sum_{ij} \log(\cosh(S_{ij})), \quad S = (U^{(z)})^\top X^{(z)} V^{(z)}.
\]

**Interpretation:** Promotes non-Gaussianity to disentangle statistically independent components across modalities. The logcosh function approximates negentropy, encouraging sparse or independent latent factors, useful for separating distinct neural processes (e.g., motor vs. memory signals).

**Strengths:**
- Captures sparse or independent factors, aligning with biological assumptions.
- Robust to Gaussian noise due to non-Gaussianity focus.

**Limitations:**
- Sensitive to non-linearity choice (e.g., logcosh) and data scaling.
- May overfit to non-Gaussian artifacts without careful tuning.

#### 6. Domain Alignment Energy

**Definition:**

\[
\mathcal{L}_{\text{domain}}(V^{(z)}) = -\gamma_z \| M_z V^{(z)} \|_F^2,
\]

where \( M_z \in \mathbb{R}^{m \times p_z} \) encodes prior information (e.g., anatomical or cognitive domain mappings).

**Interpretation:** Encourages \( V^{(z)} \) to align with prior knowledge by maximizing the energy of \( M_z V^{(z)} \). In neuroimaging, \( M_z \) may represent cognitive domains, ensuring learned factors respect biological priors.

**Strengths:**
- Integrates priors (e.g., anatomical atlases) for interpretable solutions.
- Flexible: \( M_z \) can be sparse or derived from prior studies.
- Supports hypothesis-driven research in neuroscience.

**Limitations:**
- Requires accurate priors; restrictive \( M_z \) may bias results.
- Performance depends on \( \gamma_z \) (via `domainLambdas`).

**Gradient:**

\[
\nabla_{V^{(z)}} \mathcal{L}_{\text{domain}} = -2 \gamma_z (M_z)^\top (M_z V^{(z)}).
\]

This quadratic regularizer penalizes deviation from the domain-consistent subspace, guiding optimization toward biologically meaningful solutions.

### Summary of Energy Functions

Each energy function targets a distinct property:
- **Regression**: Prioritizes within-modality variance for data reconstruction.
- **Procrustes Correlation**: Enhances cross-modal subspace alignment.
- **Angular Distance**: Focuses on geometric orientation, robust to scaling.
- **Absolute Canonical Covariance**: Ensures robust canonical relationships.
- **ICA**: Disentangles independent neural signals.
- **Domain Alignment**: Integrates priors for biological interpretability.

SiMLR combines these via `energyType` (e.g., `"acc"`, `"regression"`), enabling flexible multimodal integration for tasks like biomarker discovery \citep{avants2021,stone2020}.

## Simulation Design

We simulate \( V = `r length(matlist_train)` \) modalities with \( n = `r n` \) subjects (\( `r n_train` \) training, \( n - `r n_train` \) test) and \( d = `r d` \) latent cognitive domains. The modalities include:

- **Cortex**: Structural features (e.g., cortical thickness, \( p_1 = `r ncol(matlist_train[[1]])` \) features).
- **White Matter**: Diffusion metrics (e.g., fractional anisotropy, \( p_2 = `r ncol(matlist_train[[2]])` \) features).
- **Functional Connectivity**: Connectivity strengths (\( p_3 = `r ncol(matlist_train[[3]])` \) features).

The simulation embeds \( d = `r d` \) cognitive domains as latent signals, with modality-specific corruptions and noise (signal-to-noise ratio = `r snr`). Domain matrices \( Z_v \in \mathbb{R}^{`r nrow(dlist[[1]])` \times p_z} \) map domains to features, serving as "ground truth" priors. We evaluate SiMLR with and without domain guidance, varying `energyType`, `mixAlg`, and `domainLambdas`, and assess performance using the following metrics:

### Metrics

- **Cross-view Embedding Correlation**: Mean RV coefficient between test embeddings (higher better).
- **Domain Latent Recovery R²**: Mean squared canonical correlations between concatenated test embeddings and true latents (higher better).
- **Domain Alignment Score**: Mean cosine similarity between feature weights (\( V^{(z)} \)) and domain matrices (\( Z_v \)) (higher better).
- **Reconstruction Error**: Mean normalized Frobenius norm of SiMLR reconstruction on test data (lower better).
- **Cross-Modal Reconstruction Error**: Mean normalized Frobenius error predicting each view’s test data from other views’ concatenated embeddings via linear regression (lower better).

## Helper Functions for Metrics

```{r metrics-funcs}
# Cross-view RV coefficient
cross_view_rv <- function(emb_list) {
  tot <- 0
  count <- 0
  for (k in 1:(length(emb_list)-1)) {
    for (j in (k+1):length(emb_list)) {
      myrv <- rvcoef(emb_list[[k]], emb_list[[j]])
      tot <- tot + myrv
      count <- count + 1
    }
  }
  tot / count
}

# Domain recovery R²
domain_recovery_r2 <- function(emb_list, true_latents) {
  if (!is.list(emb_list) || length(emb_list) == 0) {
    stop("emb_list must be a non-empty list of matrices or vectors.")
  }
  if (!is.matrix(true_latents)) {
    stop("true_latents must be a matrix.")
  }
  
  emb_list <- lapply(emb_list, as.matrix)
  n_rows <- unique(sapply(emb_list, nrow))
  if (length(n_rows) != 1 || n_rows != nrow(true_latents)) {
    stop("All elements in emb_list and true_latents must have the same number of rows.")
  }
  
  if (ncol(true_latents) == 0) {
    stop("true_latents has zero columns.")
  }
  
  mean_cc2 <- numeric(length(emb_list))
  for (i in seq_along(emb_list)) {
    emb <- emb_list[[i]]
    if (ncol(emb) == 0) {
      warning(paste("Embedding", i, "has zero columns. Skipping."))
      mean_cc2[i] <- 0
      next
    }
    
    emb_scaled <- scale(emb, center = TRUE, scale = TRUE)
    true_latents_scaled <- scale(true_latents, center = TRUE, scale = TRUE)
    emb_scaled[is.na(emb_scaled)] <- 0
    true_latents_scaled[is.na(true_latents_scaled)] <- 0
    R <- t(emb_scaled) %*% true_latents_scaled / (nrow(emb_scaled) - 1)
    cc <- svd(R, nu = 0, nv = 0)$d
    cc <- pmin(pmax(cc, 0), 1)
    mean_cc2[i] <- mean(cc^2, na.rm = TRUE)
  }
  
  mean(mean_cc2, na.rm = TRUE)
}

# Domain alignment
domain_alignment <- function(basis1, basis2) {
  similarity_matrix <- abs(proxy::simil(basis1, basis2, method = "correlation"))
  match_result <- clue::solve_LSAP(similarity_matrix, maximum = TRUE)
  mean(diag(similarity_matrix[, match_result]))
}

# SiMLR reconstruction error
recon_error <- function(simlr_result, mat_test) {
  pp <- predictSimlr(mat_test, simlr_result)
  errs <- mean(pp$finalErrors)
  mean(errs, na.rm = TRUE)
}

# Cross-modal reconstruction error
cross_modal_recon <- function(emb_train_list, emb_test_list, mat_train_list, mat_test_list) {
  errs <- numeric(length(mat_train_list))
  for (i in 1:length(mat_train_list)) {
    X_train <- mat_train_list[[i]]
    X_test <- mat_test_list[[i]]
    other_inds <- setdiff(1:length(mat_train_list), i)
    concat_train <- do.call(cbind, emb_train_list[other_inds])
    concat_test <- do.call(cbind, emb_test_list[other_inds])
    trdf <- data.frame(concat_train)
    tedf <- data.frame(concat_test)
    mylm <- lm(X_train ~ ., data = trdf)
    pp <- data.matrix(predict(mylm, newdata = tedf))
    errs[i] <- norm(X_test - pp, "F") / norm(X_test, "F")
  }
  mean(errs, na.rm = TRUE)
}

# Permutation test
perm_test <- function(result, matlist_train, dlist, n_perm = 10, domlam) {
  orig_r2 <- domain_recovery_r2(lapply(1:3, function(i) matlist_train[[i]] %*% result$v[[i]]), domain_latents_train)
  perm_r2 <- numeric(n_perm)
  for (p in 1:n_perm) {
    set.seed(p)
    perm_dlist <- lapply(dlist, function(dm) {
      dm_perm <- dm
      for (j in 1:nrow(dm)) dm_perm[j, ] <- dm[j, sample(ncol(dm))]
      dm_perm
    })
    result_perm <- simlr(
      data_matrices = matlist_train,
      smoothingMatrices = smoothingMats,
      iterations = myits,
      sparsenessQuantiles = myspar,
      positivities = mypos,
      initialUMatrix = initu,
      mixAlg = as.character(result$mixAlg),
      energyType = as.character(result$energyType),
      constraint = myorth,
      domainLambdas = rep(domlam, length(matlist_train)),
      domainMatrices = perm_dlist,
      optimizationStyle = myopt,
      randomSeed = p,
      verbose = FALSE
    )
    emb_perm <- lapply(1:3, function(i) matlist_train[[i]] %*% result_perm$v[[i]])
    perm_r2[p] <- domain_recovery_r2(emb_perm, domain_latents_train)
  }
  t_stat <- t.test(orig_r2 - perm_r2, alternative = "greater")$statistic
  list(r2 = orig_r2, t_stat = t_stat)
}
```

## Parameter Grid and Runs

```{r run-grid,eval=!exists("results")}
results <- list()
metrics <- data.frame(params_grid, cross_rv = NA, recovery_r2 = NA, alignment = NA, recon_err = NA, cross_modal = NA, perm_t = NA)

for (row in 1:nrow(params_grid)) {
  par <- params_grid[row, ]
  key <- paste(par$energyType, par$mixAlg, par$domainLambda, sep="_")
  result <- simlr(
    data_matrices = matlist_train,
    smoothingMatrices = smoothingMats,
    iterations = myits,
    sparsenessQuantiles = myspar,
    positivities = mypos,
    initialUMatrix = initu,
    energyType = as.character(par$energyType),
    mixAlg = as.character(par$mixAlg),
    constraint = myorth,
    optimizationStyle = myopt,
    verbose = FALSE,
    domainMatrices = dlist,
    domainLambdas = rep(par$domainLambda, length(matlist_train)),
    randomSeed = 123
  )
  result$mixAlg <- par$mixAlg
  results[[key]] <- result
  emb_train <- lapply(1:3, function(i) matlist_train[[i]] %*% result$v[[i]])
  emb_test <- lapply(1:3, function(i) matlist_test[[i]] %*% result$v[[i]])
  metrics$cross_rv[row] <- cross_view_rv(emb_test)
  metrics$recovery_r2[row] <- domain_recovery_r2(emb_test, domain_latents_test)
  metrics$alignment[row] <- mean(sapply(1:length(dlist), function(i) domain_alignment(dlist[[i]], t(result$v[[i]]))))
  metrics$recon_err[row] <- recon_error(result, matlist_test)
  metrics$cross_modal[row] <- cross_modal_recon(emb_train, emb_test, matlist_train, matlist_test)
  if (par$domainLambda > 0) {
    perm_res <- perm_test(result, matlist_train, dlist, n_perm = 5, domlam = par$domainLambda)
    metrics$perm_t[row] <- perm_res$t_stat
  }
}
```

## Visualizations and Comparisons

### Result Metrics Table

```{r metrics-table}
library(gt)
library(gtExtras)
gt_tbl <- metrics %>% 
  mutate(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal, perm_t), round, 4)) %>%
  arrange(desc(recovery_r2)) 
library(reactable)

reactable(
  gt_tbl,
  bordered = TRUE,
  striped = TRUE,
  highlight = TRUE,
  resizable = TRUE,
  pagination = FALSE,
  defaultSorted = list(recovery_r2 = "desc"),
  defaultColDef = colDef(
    align = "center",
    headerStyle = list(
      background = "#f8f9fa",
      borderBottom = "2px solid #dee2e6",
      fontWeight = "600",
      fontSize = "14px"
    ),
    style = list(
      padding = "6px 8px",
      borderBottom = "1px solid #dee2e6",
      fontSize = "13px"
    )
  ),
  theme = reactableTheme(
    borderColor = "#dee2e6",
    stripedColor = "#f8f9fa",
    highlightColor = "#f1f3f5",
    cellPadding = "8px 12px",
    color = "#212529",
    backgroundColor = "white",
    style = list(
      borderRadius = "8px",
      borderWidth = "1px",
      borderStyle = "solid",
      borderColor = "#dee2e6"
    )
  )
)
```

### Heatmap of Metrics

```{r heatmap, fig.width=10, fig.height=6}
metrics_long <- metrics %>%
  pivot_longer(cols = c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), names_to = "Metric", values_to = "Value") %>%
  mutate(Param = paste(energyType, mixAlg, sep="+"))
heatmaply(
  metrics_long %>% dplyr::select(Param, domainLambda, Metric, Value) %>%
    pivot_wider(names_from = Metric, values_from = Value),
  Rowv = FALSE, Colv = FALSE, scale = "column", colors = viridis::viridis(100),
  main = "Metrics Heatmap by Parameters"
)
```

### Radar Chart of Metrics

```{r radar-chart, fig.width=8, fig.height=6}
metrics_norm <- metrics %>%
  mutate(
    cross_rv = (cross_rv - min(cross_rv)) / (max(cross_rv) - min(cross_rv)),
    recovery_r2 = (recovery_r2 - min(recovery_r2)) / (max(recovery_r2) - min(recovery_r2)),
    alignment = (alignment - min(alignment)) / (max(alignment) - min(alignment)),
    recon_err = 1 - (recon_err - min(recon_err)) / (max(recon_err) - min(recon_err)),
    cross_modal = 1 - (cross_modal - min(cross_modal)) / (max(cross_modal) - min(cross_modal))
  )

top_guided <- metrics_norm %>% filter(domainLambda > 0) %>% slice(which.max(alignment))
best_unguided <- metrics_norm %>% filter(domainLambda == 0) %>% slice(which.max(recovery_r2))
guided_avg <- metrics_norm %>% filter(domainLambda > 0) %>% summarise(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), mean))
unguided_avg <- metrics_norm %>% filter(domainLambda == 0) %>% summarise(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), mean))

plot_ly(type = "scatterpolar") %>%
  add_trace(
    r = as.numeric(top_guided[1, c("cross_rv", "recovery_r2", "alignment", "recon_err", "cross_modal")]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = paste(top_guided$energyType, top_guided$mixAlg, top_guided$domainLambda, sep="_"),
    fill = "toself"
  ) %>%
  add_trace(
    r = as.numeric(best_unguided[1, c("cross_rv", "recovery_r2", "alignment", "recon_err", "cross_modal")]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = paste(best_unguided$energyType, best_unguided$mixAlg, "0", sep="_"),
    fill = "toself"
  ) %>%
  add_trace(
    r = as.numeric(guided_avg[1, ]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = "Guided Avg",
    fill = "toself"
  ) %>%
  add_trace(
    r = as.numeric(unguided_avg[1, ]),
    theta = c("Cross-View Cor", "Recovery R²", "Domain Alignment", "Recon Error (Inv)", "Cross-Modal (Inv)"),
    name = "Unguided Avg",
    fill = "toself"
  ) %>%
  layout(
    polar = list(radialaxis = list(visible = TRUE, range = c(0, 1))),
    showlegend = TRUE,
    title = "Normalized Metrics Comparison"
  )
```

### Grouped Bar Plot with Significance

```{r bar-plot, fig.width=12, fig.height=6}
metrics_long <- metrics %>%
  mutate(Group = ifelse(domainLambda > 0, "Guided", "Unguided")) %>%
  pivot_longer(cols = c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), names_to = "Metric", values_to = "Value")
if (FALSE) {
  t_tests <- metrics_long %>%
    group_by(Metric, energyType) %>%
    summarise(
      p_zalue = t.test(Value[Group == "Guided"], Value[Group == "Unguided"], alternative = "two.sided")$p.value,
      .groups = "drop"
    ) %>%
    mutate(Signif = case_when(p_zalue < 0.01 ~ "***", p_zalue < 0.05 ~ "**", p_zalue < 0.1 ~ "*", TRUE ~ ""))
}
ggplot(metrics_long, aes(x = Group, y = Value, fill = Group)) +
  geom_bar(stat = "summary", fun = "mean", position = position_dodge()) +
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.4, position = position_dodge(0.45)) +
  facet_grid(Metric ~ energyType, scales = "free_y") +
  labs(title = "Guided vs. Unguided Metrics by Energy Type", y = "Value") +
  theme_minimal() +
  scale_fill_manual(values = c("Guided" = "#1b9e77", "Unguided" = "#d95f02"))
```

### Multi-Metric Line Plot

```{r line-plot, fig.width=12, fig.height=8}
ggplot(metrics_long, aes(x = domainLambda, y = Value, color = paste(energyType, mixAlg, sep="+"))) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  facet_wrap(~Metric, scales = "free_y", ncol = 3) +
  labs(title = "Metrics vs. Domain Lambda", x = "Domain Lambda", y = "Value") +
  theme_minimal() +
  scale_color_discrete(name = "Energy + MixAlg")
```

### Scaled Line Plot

```{r scaled-line-plot, fig.width=12, fig.height=8}
library(dplyr)
library(ggplot2)

metrics_long <- metrics_long %>%
  mutate(energy_mix = paste(energyType, mixAlg, sep = "+")) %>%
  group_by(Metric, energy_mix) %>%
  mutate(Value_normalized = (Value - min(Value, na.rm = TRUE)) / 
         (max(Value, na.rm = TRUE) - min(Value, na.rm = TRUE))) %>%
  ungroup()

ggplot(metrics_long, aes(x = domainLambda, y = Value_normalized, 
                        color = energy_mix)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  facet_wrap(~Metric, scales = "free_y", ncol = 3) +
  labs(title = "Normalized Metrics vs. Domain Lambda", 
       x = "Domain Lambda", 
       y = "Normalized Value (0 to 1)") +
  theme_minimal() +
  scale_color_discrete(name = "Energy + MixAlg")
```

### Energy Decomposition for Top Configurations

```{r energy-decomp, fig.width=12, fig.height=8}
top_configs <- metrics %>% filter(domainLambda > 0) %>% arrange(desc(alignment)) %>% slice(1:3)
plots <- lapply(1:nrow(top_configs), function(i) {
  key <- paste(top_configs$energyType[i], top_configs$mixAlg[i], top_configs$domainLambda[i], sep="_")
  p <- plot_energy_decomposition(results[[key]], "cortex")
  p + ggtitle(paste("Energy Decomp:", key))
})
grid.arrange(grobs = plots, ncol = 1)
```

## Example Runs

### Basic (No Domain, acc + pca)

```{r basic-example, fig.width=8, fig.height=6}
result_basic <- results[["acc_pca_0"]]
emb_basic <- lapply(1:3, function(i) matlist_train[[i]] %*% result_basic$v[[i]])
plot_energy_decomposition(result_basic, "cortex")
cat("Recovery R²:", domain_recovery_r2(emb_basic, domain_latents_train), "\n")
cat("Cross-Modal Recon:", cross_modal_recon(emb_basic, lapply(1:3, function(i) matlist_test[[i]] %*% result_basic$v[[i]]), matlist_train, matlist_test), "\n")
```

### With Domain Guidance (Top Config)

```{r domain-example, fig.width=8, fig.height=6}
result_domain <- results[[paste(top_configs$energyType[1], top_configs$mixAlg[1], top_configs$domainLambda[1], sep="_")]]
emb_domain <- lapply(1:3, function(i) matlist_train[[i]] %*% result_domain$v[[i]])
plot_energy_decomposition(result_domain, "cortex")
cat("Recovery R²:", domain_recovery_r2(emb_domain, domain_latents_train), "\n")
cat("Cross-Modal Recon:", cross_modal_recon(emb_domain, lapply(1:3, function(i) matlist_test[[i]] %*% result_domain$v[[i]]), matlist_train, matlist_test), "\n")
```

### Feature Weights Inspection

To gain deeper insight into how domain priors influence the learned representations, we examine the feature weights for the cortical modality in the top guided configuration. We compute the domain contributions by projecting the absolute values of the loading matrix \( V^{(1)} \) onto the prior domain matrix. This reveals the extent to which each learned component captures specific cognitive domains, promoting interpretability in neuroimaging contexts where linking features to biological constructs is essential.

```{r inspect-weights}
v_cortex <- result_domain$v[[1]]
domain_contrib <- dlist[[1]] %*% abs(v_cortex)
colnames(domain_contrib) <- paste0("Comp", 1:ncol(domain_contrib))
rownames(domain_contrib) <- domains

print(domain_contrib)

heatmaply(domain_contrib, 
          colors = viridis::viridis(100), 
          main = "Domain Contributions to Learned Components (Cortex)",
          xlab = "Learned Components", ylab = "Cognitive Domains")
```

## Summary Tables

### Top 3 Guided Runs

```{r top3-guided}
top3 <- metrics %>%
  filter(domainLambda > 0) %>%
  arrange(desc(alignment)) %>%
  slice(1:3) %>%
  mutate(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal, perm_t), round, 3)) 

top3 %>% gt() %>%
  tab_header(title = "Top 3 Guided Configurations")
```

### Best Unguided Baseline

```{r best-unguided}
best_unguided <- metrics %>%
  filter(domainLambda == 0) %>%
  arrange(desc(recovery_r2)) %>%
  slice(1) %>%
  mutate(across(c(cross_rv, recovery_r2, alignment, recon_err, cross_modal), round, 3)) 

best_unguided %>% gt() %>%
  tab_header(title = "Best Unguided Configuration")
```

### Guided vs. Unguided Summary

```{r summary-table}
summary_tbl <- tibble(
  Run = c("Most Significant Guided", "Best Unguided"),
  Perm_t = c(round(top3$perm_t[1], 2), NA),
  Recovery_R2 = c(round(top3$recovery_r2[1], 3), round(best_unguided$recovery_r2[1], 3)),
  Alignment = c(round(top3$alignment[1], 3), round(best_unguided$alignment[1], 3)),
  Recon_Error = c(round(top3$recon_err[1], 3), round(best_unguided$recon_err[1], 3)),
  Cross_Modal = c(round(top3$cross_modal[1], 3), round(best_unguided$cross_modal[1], 3))
) %>%
  mutate(
    Delta_vs_Unguided = c(
      paste0("+", round(top3$recovery_r2[1] - best_unguided$recovery_r2[1], 3), " R²; ",
             "+", round(top3$alignment[1] - best_unguided$alignment[1], 3), " Align; ",
             "-", round(best_unguided$recon_err[1] - top3$recon_err[1], 3), " Recon Err; ",
             "-", round(best_unguided$cross_modal[1] - top3$cross_modal[1], 3), " Cross-Modal"),
      "—"
    )
  ) %>%
  gt() %>%
  tab_header(title = "Guided vs. Unguided Summary")
summary_tbl
```

## Discussion

This study leverages the similarity-driven multi-view linear reconstruction (SiMLR) framework within ANTsR to integrate high-dimensional, multimodal neuroimaging data, incorporating domain knowledge priors to enhance interpretability and statistical power, as inspired by prior work in neuroimaging and multi-omics \citep{avants2021,stone2020}. Using simulated data mimicking cortical thickness, white matter diffusion metrics, and functional connectivity, we demonstrate that guided SiMLR configurations—particularly those employing the "`r top3$energyType[1]`" energy type with `r top3$mixAlg[1]` mixing and moderate domain lambdas—substantially outperform unguided baselines across key metrics, aligning with the need for biologically grounded, reproducible analyses in quantitative neuroscience.

The top-performing guided configuration (`r top3$energyType[1]`, `r top3$mixAlg[1]`, domainLambda = `r top3$domainLambda[1]`) achieves a cross-view RV coefficient of `r round(top3$cross_rv[1], 3)`, a domain latent recovery R² of `r round(top3$recovery_r2[1], 3)`, and a domain alignment score of `r round(top3$alignment[1], 3)`, with a permutation test t-statistic of `r round(top3$perm_t[1], 2)`, indicating robust improvements over random prior permutations. These results highlight SiMLR’s ability to capture inter-modality similarities and align learned embeddings with predefined cognitive domains (e.g., processing speed, memory), as evidenced by the high alignment score. Compared to the best unguided baseline (`r best_unguided$energyType`, `r best_unguided$mixAlg`, domainLambda = 0, recovery R² = `r round(best_unguided$recovery_r2, 3)`, alignment = `r round(best_unguided$alignment, 3)`), the guided configuration yields a `r round(top3$recovery_r2[1] - best_unguided$recovery_r2, 3)` increase in recovery R² and a `r round(top3$alignment[1] - best_unguided$alignment, 3)` increase in alignment, underscoring the value of domain priors in enhancing the recovery of latent signals and their biological interpretability. The reconstruction error (`r round(top3$recon_err[1], 3)`) and cross-modal reconstruction error (`r round(top3$cross_modal[1], 3)`) in this configuration are also lower than the unguided baseline (reconstruction error = `r round(best_unguided$recon_err, 3)`, cross-modal error = `r round(best_unguided$cross_modal, 3)`), suggesting improved generalizability across modalities.

From a neuroimaging perspective, these findings are particularly relevant for integrating multimodal data, such as structural MRI, diffusion tensor imaging (DTI), and resting-state fMRI, where each modality captures complementary aspects of brain structure and function \citep{stone2020}. The high alignment scores in guided configurations (e.g., `r round(top3$alignment[1], 3)` for `r top3$energyType[1]`, `r top3$mixAlg[1]`, domainLambda = `r top3$domainLambda[1]`) indicate that SiMLR effectively maps learned components to biologically meaningful domains, facilitating hypothesis-driven research, such as linking cortical thinning to cognitive deficits or white matter alterations to executive dysfunction. This mirrors applications in studies of traumatic brain injury or Alzheimer’s disease, where priors from anatomical atlases or functional parcellations enhance interpretability \citep{avants2021}. The low cross-modal reconstruction errors (e.g., `r round(metrics$cross_modal[which(metrics$energyType == top3$energyType[1] & metrics$mixAlg == top3$mixAlg[1] & metrics$domainLambda == 0.05)], 3)` for `r top3$energyType[1]`, `r top3$mixAlg[1]`, domainLambda = 0.05) suggest that guided embeddings can predict one modality from others, a critical capability for clinical settings where missing data or modality-specific artifacts are common, such as in longitudinal studies of neurodegenerative disorders.

The permutation tests provide strong statistical evidence of the non-random contribution of domain priors, with t-statistics ranging from `r round(min(metrics$perm_t[metrics$domainLambda > 0], na.rm = TRUE), 3)` to `r round(max(metrics$perm_t[metrics$domainLambda > 0], na.rm = TRUE), 3)` across guided configurations, peaking at `r round(metrics$perm_t[which(metrics$energyType == "regression" & metrics$mixAlg == "ica" & metrics$domainLambda == 0.02)], 3)` for regression, ICA, domainLambda = 0.02. This indicates that the priors significantly enhance performance in a manner robust to random perturbations, reinforcing confidence in SiMLR’s reliability for neuroimaging applications. However, performance trade-offs are notable: configurations with higher domain lambdas (e.g., `r max(metrics$domainLambda[metrics$domainLambda > 0])`, `r top3$energyType[1]`, `r top3$mixAlg[1]`) show a decline in recovery R² (`r round(metrics$recovery_r2[which(metrics$energyType == top3$energyType[1] & metrics$mixAlg == top3$mixAlg[1] & metrics$domainLambda == max(metrics$domainLambda[metrics$domainLambda > 0]))], 3)`) and alignment (`r round(metrics$alignment[which(metrics$energyType == top3$energyType[1] & metrics$mixAlg == top3$mixAlg[1] & metrics$domainLambda == max(metrics$domainLambda[metrics$domainLambda > 0]))], 3)`) compared to moderate lambdas (e.g., `r top3$domainLambda[1]`), suggesting that excessive prior weighting may constrain the model’s flexibility to capture latent signals not fully represented in the domain matrices. This trade-off is evident in the `r top3$energyType[1]`, `r top3$mixAlg[1]` configurations, where cross-view RV (`r round(top3$cross_rv[1], 3)` at domainLambda = `r top3$domainLambda[1]`) and recovery R² (`r round(top3$recovery_r2[1], 3)`) peak at moderate lambdas before declining at higher values.

Interestingly, the regression energy type with ICA mixing shows consistent performance across domain lambdas (e.g., recovery R² ≈ `r round(mean(metrics$recovery_r2[metrics$energyType == "regression" & metrics$mixAlg == "ica"]), 3)`, alignment ≈ `r round(mean(metrics$alignment[metrics$energyType == "regression" & metrics$mixAlg == "ica"]), 3)`), with high permutation t-statistics (e.g., `r round(metrics$perm_t[which(metrics$energyType == "regression" & metrics$mixAlg == "ica" & metrics$domainLambda == 0.1)], 3)` for domainLambda = 0.10), suggesting robustness but limited sensitivity to prior strength. In contrast, the `r top3$energyType[1]` energy type with `r top3$mixAlg[1]` mixing is highly sensitive to domainLambda, with cross-view RV increasing from `r round(metrics$cross_rv[which(metrics$energyType == top3$energyType[1] & metrics$mixAlg == top3$mixAlg[1] & metrics$domainLambda == 0)], 3)` (unguided) to `r round(top3$cross_rv[1], 3)` (domainLambda = `r top3$domainLambda[1]`) and alignment from `r round(metrics$alignment[which(metrics$energyType == top3$energyType[1] & metrics$mixAlg == top3$mixAlg[1] & metrics$domainLambda == 0)], 3)` to `r round(top3$alignment[1], 3)`, indicating that the `r top3$energyType[1]` energy type benefits more from prior guidance. The PCA mixing algorithm, however, underperforms with the `r top3$energyType[1]` energy type (e.g., recovery R² = `r round(metrics$recovery_r2[which(metrics$energyType == top3$energyType[1] & metrics$mixAlg == "pca" & metrics$domainLambda == 0.02)], 3)` at domainLambda = 0.02), likely due to its assumption of Gaussian signal components, whereas ICA’s ability to capture non-Gaussian structures aligns better with the simulated data’s complexity.

The simulation’s design, while controlled, assumes linear mappings and Gaussian noise, which may not fully capture the nonlinearities, scanner artifacts, or subject-specific variability in real neuroimaging data \citep{avants2021}. The high reconstruction errors (e.g., `r round(metrics$recon_err[which(metrics$energyType == "acc" & metrics$mixAlg == "pca" & metrics$domainLambda == 0)], 3)` for acc, PCA, domainLambda = 0) in some configurations suggest sensitivity to parameter choices, particularly for PCA-based methods, which may struggle with the sparse, noisy nature of multimodal data. Future validation on large-scale datasets, such as the UK Biobank or the Adolescent Brain Cognitive Development (ABCD) study, is critical to confirm these findings in the presence of real-world heterogeneity. Additionally, the recovery R² metric, while effective for evaluating latent signal recovery, may require normalization or calibration to avoid numerical amplification, as values like `r round(top3$recovery_r2[1], 3)` (`r top3$energyType[1]`, `r top3$mixAlg[1]`, domainLambda = `r top3$domainLambda[1]`) suggest potential scaling issues in the canonical correlation computation.

Practically, SiMLR’s ability to integrate domain priors aligns with the ethos of reproducible, open-science neuroimaging championed by the ANTs ecosystem \citep{avants2011}. The feature weights inspection (Section "Feature Weights Inspection") reveals how learned components correspond to cognitive domains, enhancing biological interpretability and supporting applications like biomarker discovery in psychiatric disorders or monitoring longitudinal brain changes post-injury \citep{stone2020}. The method’s robustness to cross-modal prediction makes it suitable for clinical scenarios with incomplete datasets, such as predicting functional connectivity from structural MRI in patients with missing fMRI scans.

Limitations include the simulation’s reliance on synthetic data, which simplifies biological variability. Nonlinear relationships between modalities and latents, common in real-world neuroimaging, may require extensions like kernel-based SiMLR or integration with deep learning within the ANTsX framework. The choice of domainLambda remains a critical tuning parameter, as overly strong priors (e.g., domainLambda = `r max(metrics$domainLambda[metrics$domainLambda > 0])`) may overfit to the provided knowledge, potentially missing novel signals. Future work should explore adaptive lambda selection, validate against clinical datasets with ground-truth outcomes (e.g., Alzheimer’s or traumatic brain injury cohorts), and integrate SiMLR with tools like Neuroconductor for scalable analysis \citep{muschelli2019}. Incorporating priors from advanced parcellations or connectomics could further enhance performance, particularly for disorders with distributed neural signatures.

In summary, this study demonstrates that SiMLR, augmented with domain knowledge priors, provides a robust, interpretable framework for multimodal neuroimaging analysis. By achieving superior alignment, latent recovery, and cross-modal prediction, it offers a promising tool for uncovering biologically meaningful patterns, advancing precision neuroscience and clinical biomarker discovery.

\bibliographystyle{apalike}
\bibliography{references}