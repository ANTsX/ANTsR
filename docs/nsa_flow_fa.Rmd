---
title: "Comparison of NSA-Flow Regularized Factor Analysis and Standard Principal Axis Factoring"
author: "Brian B. Avants"
date: "2025-10-25"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: paged
header-includes:
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(psych)
library(tidyverse)
library(gt)
library(corrplot)
library(ggplot2)
library(patchwork)
library(reshape2)
library(ANTsR)
set.seed(1234)
if ( ! exists("mxit") ) mxit=5
if ( ! exists("nn") ) nn=TRUE
if (nn) alpha=1.0 else alpha=5.0
mxit=100
myalpha=1.0
nsa_default <- function(Y0, w=0.5, apply_nonneg=nn ) {
  nsa_flow_autograd(
    Y0 = Y0,
    w = w,
    retraction = "soft_polar",
    lr_strategy = "armijo",
    optimizer='asgd',
    max_iter=1000L,
    warmup_iters=10L,
    apply_nonneg = apply_nonneg, verbose=FALSE
  )
}
```

# Introduction

Factor analysis identifies latent factors explaining correlations among observed variables. This document compares standard Principal Axis Factoring (PAF) from the `psych` package with NSA-Flow Factor Analysis (NSA.FA), which incorporates Non-negative Stiefel Approximating Flow regularization for improved sparsity and interpretability.

PAF iteratively estimates communalities to focus on common variance. NSA.FA adds geometric regularization during iteration, promoting sparser loadings as an alternative to rotation, enhancing stability and fit.

Using the Big Five Inventory (BFI) dataset (25 items, >1000 participants), we compare loadings, explained variance, fit, communalities, interpretability, orthogonality, predictive validity, and reproduced correlations.

```{r expectedcorrs, echo=FALSE}
big5_corr <- tribble(
  ~Trait1, ~Trait2, ~Correlation,
  "Openness (O)", "Conscientiousness (C)", -0.10,
  "Openness (O)", "Extraversion (E)", 0.15,
  "Openness (O)", "Agreeableness (A)", 0.10,
  "Openness (O)", "Neuroticism (N)", 0.05,
  "Conscientiousness (C)", "Extraversion (E)", 0.15,
  "Conscientiousness (C)", "Agreeableness (A)", 0.25,
  "Conscientiousness (C)", "Neuroticism (N)", -0.30,
  "Extraversion (E)", "Agreeableness (A)", 0.25,
  "Extraversion (E)", "Neuroticism (N)", -0.30,
  "Agreeableness (A)", "Neuroticism (N)", -0.30
)
big5_corr %>%
  gt() %>%
  tab_header(
    title = "Expected Correlations Between Big Five Traits",
    subtitle = "Based on empirical meta-analyses"
  ) %>%
  fmt_number(columns = Correlation, decimals = 2) %>%
  cols_label(
    Trait1 = "Trait 1",
    Trait2 = "Trait 2",
    Correlation = "r"
  )
```

**Interpretation**: These typical inter-trait correlations serve as a benchmark for recovery of personality structure.

# Methods

## Overview

NSA.FA extends factor analysis by integrating NSA-Flow regularization to decorrelate the loadings matrix while promoting sparsity. This is achieved by defining a continuous-time flow toward the Stiefel manifold, balancing reconstruction error and soft orthogonality constraints.

## Factor Model

For data \( X \):

\[
X = \Lambda F + E,
\]

implying covariance:

\[
\Sigma \approx \Lambda \Lambda^\top + \Psi.
\]

## NSA-Flow Regularization

Flow dynamics balance reconstruction, sparsity, and orthogonality:

\[
\frac{d\Lambda_t}{dt} = -\nabla_\Lambda \left( \frac{1}{2} \|\Sigma - \Lambda \Lambda^\top\|_F^2 + \lambda \|\Lambda\|_1 - \frac{w}{2} \|\Lambda^\top \Lambda - I_k\|_F^2 \right).
\]

Updates retract via soft-polar for stability.

## Algorithm

Iterative loop: power iteration initialization, scaling, NSA-Flow regularization, damped communality updates, optional rotation (e.g., promax).

Annealing \( w \) aids convergence, monitored by communality changes and reconstruction error.

## Assumptions

Approximate low-rank model, near-Stiefel loadings, local convexity.

## Relation to PAF

Generalizes eigenvalue solving with dynamic regularization, yielding sparse, orthogonal factors.

## Implementation

R with PyTorch backend via `reticulate`.

## Metrics

- Fit: CFA indices (RMSEA, CFI, TLI, SRMR, BIC).

- Interpretability: Hofmann complexity, cross-loadings.

- Orthogonality Defect: Deviation from orthogonality.

- Predictive Validity: Factor-trait correlations in test set.

- Reproduced Correlations: Observed vs. implied.

80/20 train/test split.

```{r methods, echo=FALSE}
paf_standalone <- function(data = NULL, R = NULL, nfactors, rotate = "none", max_iter = 100, tol = 1e-5) {
  if (!is.null(data)) R <- cor(data)
  fit <- psych::fa(R, nfactors = nfactors, rotate = rotate, fm = "pa", max.iter = max_iter)
  list(
    loadings = fit$loadings,
    communalities = fit$communality,
    uniqueness = 1 - fit$communality,
    iterations = fit$iter,
    converged = TRUE
  )
}
same_size_noise_matrix <- function(X) {
  set.seed(42)
  noise_matrix <- matrix(rnorm(nrow(X) * ncol(X)), nrow = nrow(X), ncol = ncol(X))
  colnames(noise_matrix) <- colnames(X)
  rownames(noise_matrix) <- rownames(X)
  return(noise_matrix)
}
```

```{r alg, echo=FALSE,fig.height=10}
library(DiagrammeR)

grViz("
digraph nsa_flow_fa {
  graph [layout = dot, rankdir = TB]
  node [shape = box, style = rounded, fontsize = 11, fontname = Helvetica]

  subgraph cluster_inputs {
    label = 'Input & Initialization'
    style = 'filled'; color = lightgrey; fillcolor = '#F5F5F5'
    X [label = 'X: data matrix (n × p)']
    k [label = 'k: number of factors']
    Rmat [label = 'Rmat = XtX / n (covariance)']
    Y_init [label = 'Y₀: random loadings (p × k)']
  }

  subgraph cluster_energy {
    label = 'FA Energy Function'
    style = 'filled'; color = lightblue; fillcolor = '#E6F3FF'
    recon_cross [label = 'recon_cross = Y Yᵀ']
    diag_err [label = 'diag_err = diag(Rmat - recon_cross)']
    psi [label = 'ψ = pmax(diag_err, ε)']
    recon [label = 'recon = Y Yᵀ + diag(ψ)']
    diff [label = 'diff = Rmat - recon']
    loss [label = 'Loss = ||diff||²_F + λ‖Y‖₁']
  }

  subgraph cluster_grad {
    label = 'Gradient Computation'
    style = 'filled'; color = lightyellow; fillcolor = '#FFFFE0'
    grad [label = '∇E(Y) = -4 (Rmat - recon) Y']
    clip [label = 'Gradient clipping (‖∇E‖ ≤ max_grad_norm)']
  }

  subgraph cluster_opt {
    label = 'Optimization Loop'
    style = 'filled'; color = lightcyan; fillcolor = '#E0FFFF'
    update [label = 'Y ← Y - α ∇E']
    armijo [label = 'Backtracking (Armijo)']
    prox [label = 'Proximal step: basic or nsa_flow_fn']
    rotate [label = 'Optional rotation (varimax/promax/oblimin)']
    check [label = 'Check ΔEnergy, ΔY, convergence']
  }

  subgraph cluster_output {
    label = 'Outputs'
    style = 'filled'; color = lightgreen; fillcolor = '#E8FEE8'
    loadings [label = 'Final loadings (rotated Y)']
    communalities [label = 'Communalities = diag(YYᵀ)']
    energy [label = 'Energy trace']
    summary [label = 'Return list: {loadings, communalities, energy, ...}']
  }

  # Connections
  X -> Rmat
  k -> Y_init
  Y_init -> recon_cross
  Rmat -> diag_err
  recon_cross -> diag_err -> psi -> recon -> diff -> loss
  loss -> grad -> clip -> update -> armijo -> prox -> rotate -> check
  check -> loadings -> communalities -> energy -> summary
}
")
```

# Comparison on BFI Dataset

```{r real-data}
data(bfi)
bfi_data <- bfi %>% select(A1:A5, C1:C5, E1:E5, N1:N5, O1:O5) %>% na.omit()
```

```{r real-data-fit,echo=FALSE}
rot <- "none"
bfi_data_tx <- transform_matrix(data.matrix(bfi_data), 'none')$Xs
real_A <- paf_standalone(bfi_data_tx, nfactors = 5, rotate = rot)
real_B <- nsa_flow_pca_fa(bfi_data_tx, 5, max_iter = mxit, rotate = rot, verbose = FALSE, proximal_type = 'nsa_flow', alpha = myalpha, nsa_flow_fn = nsa_default, objective = "fa")
loadA <- as.data.frame(real_A$loadings[, ])
loadB <- as.data.frame(real_B$loadings[, ])
h2_A <- rowSums(loadA^2)
h2_B <- rowSums(loadB^2)
```

## Model Fit Indices

```{r varx0, echo=FALSE}
library(lavaan)
set.seed(123)
L_A <- as.matrix(real_A$loadings[])
L_B <- as.matrix(real_B$loadings[])
bfi_clean <- na.omit(bfi_data_tx)
S <- cov(bfi_clean)
n <- nrow(bfi_clean)
build_simple_model <- function(L_matrix, prefix = "F") {
  if (is.null(rownames(L_matrix))) rownames(L_matrix) <- paste0("V", seq_len(nrow(L_matrix)))
  assignments <- apply(abs(L_matrix), 1, which.max)
  lines <- sapply(seq_len(ncol(L_matrix)), function(f) {
    items <- rownames(L_matrix)[assignments == f]
    if (length(items) == 0) return(paste0(prefix, f, " =~ 0*dummy"))
    paste0(prefix, f, " =~ ", paste(items, collapse = " + "))
  })
  paste(lines, collapse = "\n")
}
model_A_str <- build_simple_model(L_A)
model_B_str <- build_simple_model(L_B)
fit_A <- try(cfa(model_A_str, sample.cov = S, sample.nobs = n, fixed.x = FALSE, estimator = "ML"), silent = TRUE)
fit_B <- try(cfa(model_B_str, sample.cov = S, sample.nobs = n, fixed.x = FALSE, estimator = "ML"), silent = TRUE)
extract_indices <- function(fit) {
  if (inherits(fit, "try-error")) {
    return(tibble(RMSEA = NA, CFI = NA, TLI = NA, SRMR = NA, BIC = NA, converged = FALSE))
  }
  fm <- fitMeasures(fit, c("rmsea","cfi","tli","srmr","bic"))
  tibble(
    RMSEA = round(fm["rmsea"], 3),
    CFI = round(fm["cfi"], 3),
    TLI = round(fm["tli"], 3),
    SRMR = round(fm["srmr"], 3),
    BIC = round(fm["bic"], 1),
    converged = inspect(fit, "converged")
  )
}
fit_A_idx <- extract_indices(fit_A)
fit_B_idx <- extract_indices(fit_B)
fit_comp <- bind_rows(
  tibble(Method = "PAF") %>% bind_cols(fit_A_idx),
  tibble(Method = "NSA.FA") %>% bind_cols(fit_B_idx)
)
fit_comp %>%
  gt() %>%
  tab_header(
    title = "CFA Fit Indices — PAF vs NSA.FA"
  ) %>%
  tab_spanner(
    label = "Fit Statistics",
    columns = c(RMSEA, CFI, TLI, SRMR, BIC)
  ) %>%
  cols_align(
    align = "center",
    columns = everything()
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(60)
  )
```

**Interpretation**: NSA.FA similar fit metrics with some lossed due to non-negative regularization.


## Train-Test Split Comparisons

```{r compare_loadings_at_last, echo=FALSE, fig.width=10, fig.height=4}
n <- nrow(bfi_data_tx)
train_idx <- sample(seq_len(n), size = floor(0.8 * n))
train_data <- bfi_data_tx[train_idx, ]
test_data <- bfi_data_tx[-train_idx, ]
real_A_train <- paf_standalone(train_data, nfactors = 5, rotate = rot)
L_A <- real_A_train$loadings
real_B_train <- nsa_flow_pca_fa(train_data, 5, max_iter = mxit, rotate = rot, verbose = FALSE, proximal_type = 'nsa_flow', alpha = myalpha, nsa_flow_fn = nsa_default, objective = "fa")
L_B <- real_B_train$loadings
h2_A_train <- rowSums(L_A^2)
h2_B_train <- rowSums(L_B^2)
h2_A_train_norm <- rowSums(L_A^2)/sum(rowSums(L_A^2))
h2_B_train_norm <- rowSums(L_B^2)/sum(rowSums(L_B^2))
iod_A <- ANTsR::invariant_orthogonality_defect(L_A)
iod_B <- ANTsR::invariant_orthogonality_defect(L_B)
comm_df <- tibble(
  Variable = rownames(L_A),
  PAF = h2_A_train_norm,
  `NSA.FA` = h2_B_train_norm
) %>%
  pivot_longer(cols = c(PAF, `NSA.FA`), names_to = "Method", values_to = "Communality")
ggplot(comm_df, aes(x = reorder(Variable, Communality), y = Communality, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  coord_flip() +
  scale_fill_manual(values = c("PAF" = "#0073C2", "NSA.FA" = "#EFC000")) +
  labs(title = "Communalities (Train Set, normalized)", y = "Proportion Explained", x = "Variable") +
  theme_minimal(base_size = 14)
```

## Predictive Validity (Test Set)

```{r predictive_validity, echo=FALSE, fig.width=10, fig.height=5}
scores_test_A <- as.matrix(test_data) %*% as.matrix(L_A)
scores_test_B <- as.matrix(test_data) %*% as.matrix(L_B)
traits <- list(
  Agreeableness = "^A",
  Conscientiousness = "^C",
  Extraversion = "^E",
  Neuroticism = "^N",
  Openness = "^O"
)
outcomes <- lapply(traits, function(pat) rowSums(test_data[, grep(pat, colnames(test_data))]))
cor_matrix_A <- sapply(1:5, function(f) sapply(outcomes, function(out) cor(scores_test_A[,f], out, use = "pairwise.complete.obs")))
cor_matrix_B <- sapply(1:5, function(f) sapply(outcomes, function(out) cor(scores_test_B[,f], out, use = "pairwise.complete.obs")))
cor_df_all <- data.frame(
  Factor = rep(paste0("F", 1:5), length(traits)),
  Trait = rep(names(traits), each = 5),
  PAF = abs(as.vector(cor_matrix_A)),
  `NSA.FA` = abs(as.vector(cor_matrix_B))
) %>%
  pivot_longer(cols = c(PAF, `NSA.FA`), names_to = "Method", values_to = "Correlation")
ggplot(cor_df_all, aes(x = Factor, y = Trait, fill = Correlation)) +
  geom_tile(color = "white", linewidth = 0.1) +
  geom_text(aes(label = round(Correlation, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, name = "Correlation") +
  facet_wrap(~Method) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 0),
        panel.grid = element_blank()) +
  labs(title = "Factor-Trait Correlations (Test Set)", x = "Factor", y = "Trait")
```

**Interpretation**: NSA.FA shows stronger, more specific correlations, indicating better structure recovery.

## Reproduced Correlations (Test Set)

```{r compare_loadings_at_last_cont2, echo=FALSE, fig.width=10, fig.height=4}
uniqueness_A <- 1 - h2_A_train
uniqueness_B <- 1 - h2_B_train
R_hat_A <- L_A %*% t(L_A) + diag(uniqueness_A)
R_hat_B <- L_B %*% t(L_B) + diag(uniqueness_B)
R_test <- cor(test_data, use = "pairwise.complete.obs")
melt_R_test <- melt(R_test); melt_R_test$Method <- "Observed"
melt_R_A <- melt(R_hat_A); melt_R_A$Method <- "PAF"
melt_R_B <- melt(R_hat_B); melt_R_B$Method <- "NSA.FA"
all_corr <- bind_rows(melt_R_test, melt_R_A, melt_R_B)
ggplot(all_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white", linewidth = 0.1) +
  facet_wrap(~Method) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, name = "Correlation") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        axis.text.y = element_text(size = 8),
        panel.grid = element_blank()) +
  labs(title = "Observed vs Implied Correlations", fill = "Correlation")
```

**Interpretation**: NSA.FA provides more unique mappings from the factors to the known traits.

## Interpretability Metrics

```{r interpretability_metrics, echo=FALSE}
hofmann_complexity <- function(loadings) {
  lambda <- abs(as.matrix(loadings))
  sum_l2 <- rowSums(lambda^2)
  sum_l4 <- rowSums(lambda^4)
  sum_l2^2 / sum_l4
}
mean_comp_A <- mean(hofmann_complexity(L_A))
mean_comp_B <- mean(hofmann_complexity(L_B))
cross_load_A <- sum(rowSums(abs(as.matrix(L_A)) > 0.3) > 1)
cross_load_B <- sum(rowSums(abs(as.matrix(L_B)) > 0.3) > 1)
sparsity_A <- mean(abs(L_A) < 0.1) * 100
sparsity_B <- mean(abs(L_B) < 0.1) * 100
interp_tbl <- tibble(
  Method = c("PAF", "NSA.FA"),
  `Mean Complexity` = c(mean_comp_A, mean_comp_B),
  `Cross-Loadings` = c(cross_load_A, cross_load_B),
  `Sparsity (%)` = c(sparsity_A, sparsity_B)
)
interp_tbl %>%
  gt() %>%
  tab_header(
    title = "Interpretability Metrics"
  ) %>%
  fmt_number(
    columns = `Mean Complexity`,
    decimals = 3
  ) %>%
  cols_align(
    align = "center",
    columns = everything()
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(50)
  )
```

**Interpretation**: Lower complexity/cross-loadings and higher sparsity in NSA.FA indicate superior interpretability.

## Summary Tables

```{r compare_loadings_at_last_tables, echo=FALSE}
comm_df_wide <- comm_df %>%
  pivot_wider(names_from = Method, values_from = Communality) %>%
  mutate(
    Group = case_when(
      substr(Variable, 1, 1) == "A" ~ "Agreeableness",
      substr(Variable, 1, 1) == "C" ~ "Conscientiousness",
      substr(Variable, 1, 1) == "E" ~ "Extraversion",
      substr(Variable, 1, 1) == "N" ~ "Neuroticism",
      substr(Variable, 1, 1) == "O" ~ "Openness"
    ),
    Difference = `NSA.FA` - PAF
  ) %>%
  arrange(Group, Variable) %>%
  select(Group, Variable, PAF, `NSA.FA`, Difference)
comm_df_wide %>%
  group_by(Group) %>%
  gt(rowname_col = "Variable") %>%
  tab_header(
    title = "Communalities by Method"
  ) %>%
  tab_spanner(
    label = "Communalities",
    columns = c(PAF, `NSA.FA`)
  ) %>%
  fmt_number(
    columns = c(PAF, `NSA.FA`, Difference),
    decimals = 3
  ) %>%
  data_color(
    columns = Difference,
    fn = scales::col_numeric(
      palette = "RdBu",
      domain = c(-1, 1)
    )
  ) %>%
  cols_align(
    align = "center",
    columns = c(PAF, `NSA.FA`, Difference)
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(60),
    row_group.background.color = "lightgray",
    row_group.font.weight = "bold"
  )
iod_tbl <- tibble(
  Method = c("PAF", "NSA.FA"),
  OrthogonalityDefect = c(iod_A, iod_B)
)
iod_tbl %>%
  gt() %>%
  tab_header(
    title = "Orthogonality Defects"
  ) %>%
  cols_align(
    align = "center",
    columns = OrthogonalityDefect
  ) %>%
  tab_options(
    table.font.size = 13,
    table.width = pct(40)
  )
```

**Interpretation**: NSA.FA often explains more variance (positive differences) and has lower orthogonality defect.

# Conclusion

NSA.FA outperforms PAF in interpretability, fit, and predictive validity by producing sparser, more stable factors through geometric regularization, making it a superior alternative for exploratory factor analysis.