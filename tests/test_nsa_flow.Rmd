---
title: "NSA-Flow Optimization — Test Suite and Analysis"
author: "B. Avants"
date: "October 12, 2025"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: united
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width=12, fig.height=8)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(knitr)
library(microbenchmark)  # For timing
library(ANTsR)  # Assumes missing functions like create_optimizer, step are here
```

# Overview

This NSA-Flow test suite considers:

- all retraction options ("soft", "qr", "polar", "none").

- diverse datasets: small, large, sparse with negatives.

- Multiple reps (n=3) per config for mean/sd stats.

- metrics: timing, orthogonality, reconstruction error (fidelity).

- Edge cases: zero-norm, singular inits.

- Quantitative pass/fail based on expected behavior per weight ($\omega$).

```{r utilities}
library(ANTsR)
orth_residual <- invariant_orthogonality_defect
neg_violation <- function(Y) sum(pmax(0, -Y))

# New: Rank check
effective_rank <- function(Y, tol=1e-6) {
  s <- svd(Y)$d
  sum(s > tol * max(s))
}


```


# Methods

## Synthetic Data Generation

To systematically evaluate the behavior of the retraction methods under controlled conditions, we construct several families of **synthetic datasets** that vary in **dimensionality**, **correlation structure**, and **sparsity**.  This allows us to probe the algorithm’s robustness across dense, correlated, and sparse or sign-heterogeneous data regimes. Each dataset represents a canonical stress-test configuration:

| Dataset name    | Description |
|-----------------|--------------|
| **small**       | Moderate dimension (p = 90, k = 10), low correlation (ρ = 0.2); emulates structured, small-scale settings. |
| **large**       | Larger system (p = 200, k = 20), moderate correlation (ρ = 0.3); tests scalability and performance under multicollinearity. |
| **sparse_neg**  | Sparse with mixed signs (≈ 50% zeros, negative entries included); tests stability and nonnegativity effects. |

---

Let \( p \) and \( k \) denote the number of samples and features, respectively.  We begin from an orthogonal base frame \(V_0 \in \mathbb{R}^{p \times k}\) obtained via QR decomposition of a random Gaussian matrix:

\[
V_0 = \text{qr.Q}\big(\text{qr}(\mathcal{N}(0,1)_{p \times k})\big).
\]

We then generate correlated data using a covariance matrix
\[
\Sigma_{ij} = 
\begin{cases}
1 & \text{if } i = j,\\
\rho & \text{otherwise},
\end{cases}
\]
where \( \rho \) controls inter-feature correlation.  
Samples are drawn from \( \mathcal{N}(0, \Sigma) \) to yield \( Y_0 \).  
The corresponding nonnegative target \( X_0 \) is constructed as
\[
X_0 = \max\left(Y_0 + 0.2\,\mathcal{N}(0,1),\, 0\right).
\]
Optionally, sparsity and sign heterogeneity are imposed:
\[
Y_{0,ij} =
\begin{cases}
0 & \text{with probability } p_{\text{sparse}},\\
Y_{0,ij} - 0.5 & \text{if include\_neg = TRUE}.
\end{cases}
\]

This yields diverse matrices \( (Y_0, X_0, V_0) \) with controllable noise, correlation, and structure for robust evaluation.



```{r data_gen}
generate_synth_data <- function(p=40, k=3, noise=0.5, corr=0.1,
                                sparse_prob=0.2, include_neg=FALSE) {
  V0 <- qr.Q(qr(matrix(rnorm(p*k), p, k)))
  library(MASS)
  # Correlated data
  Sigma <- matrix(corr, k, k); diag(Sigma) <- 1
  Y0 <- mvrnorm(n = p, mu = rep(0, k), Sigma = Sigma)
  if (include_neg) Y0 <- Y0 - 0.5
  if (sparse_prob > 0) Y0[runif(p*k) < sparse_prob] <- 0
  X0 <- pmax(Y0 + 0.2 * matrix(rnorm(p*k), p, k), 0)
  list(Y0=Y0, X0=X0, V0=V0)
}

# Generate datasets
data_small      <- generate_synth_data(p=90,  k=10, corr=0.2)
data_large      <- generate_synth_data(p=200, k=20, corr=0.3)
data_sparse_neg <- generate_synth_data(sparse_prob=0.5, include_neg=TRUE)



dataset_stats <- function(name, data) {
  Y0 <- data$Y0
  cor_vals <- cor(Y0)
  mean_corr <- mean(abs(cor_vals[upper.tri(cor_vals)]))
  sparsity <- mean(Y0 == 0)
  neg_frac <- mean(Y0 < 0)
  scale_mean <- mean(abs(Y0))
  data.frame(
    Dataset = name,
    Mean_Abs_Correlation = round(mean_corr, 3),
    Sparsity = round(sparsity, 3),
    Negative_Fraction = round(neg_frac, 3),
    Mean_Abs_Value = round(scale_mean, 3)
  )
}

summary_table <- rbind(
  dataset_stats("Small", data_small),
  dataset_stats("Large", data_large),
  dataset_stats("Sparse_Neg", data_sparse_neg)
)

knitr::kable(summary_table, caption="Structural characteristics of synthetic datasets.")
```


The following figure shows the heatmap structure of the generated matrices. Color intensity represents magnitude; blue tones indicate positive values, and red tones (where applicable) indicate negatives.

```{r}
library(ggplot2)
library(reshape2)
library(patchwork)

plot_heat <- function(mat, title) {
  df <- melt(mat)
  ggplot(df, aes(Var2, Var1, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low="red", mid="white", high="blue", midpoint=0) +
    theme_minimal(base_size = 12) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank(),
          panel.grid=element_blank(),
          plot.title=element_text(face="bold", hjust=0.5)) +
    labs(title=title, x=NULL, y=NULL, fill="Value")
}

p_small  <- plot_heat(data_small$Y0, "Small Dataset (ρ=0.2)")
p_large  <- plot_heat(data_large$Y0, "Large Dataset (ρ=0.3)")
p_sparse <- plot_heat(data_sparse_neg$Y0, "Sparse Negative Dataset")

(p_small | p_large | p_sparse) + plot_layout(guides = "collect")
```

Figure 1. Synthetic data matrices used for benchmarking.
From left to right:

* Small dataset: moderately correlated structure with low noise.

* Large dataset: increased dimensionality and correlation.

* Sparse-negative dataset: structured sparsity and sign heterogeneity, introducing strong nonlinearity and nonnegativity challenges.


The summary table confirms the diversity of these datasets:

* Small dataset: Moderate correlation (≈0.2) and minimal sparsity, representing a well-conditioned test case.

* Large dataset: Similar correlation strength but higher feature count, stressing scalability and convergence stability.

* Sparse-negative dataset: Extremely sparse (~50%) and sign-diverse (≈50% negative), producing high-contrast signals and providing a stringent test for nonnegativity-constrained optimization.



## Experimental Configurations for NSA-Flow

In this section, we define the experimental configuration grid used to evaluate the performance of the NSA-Flow algorithm across a range of retraction strategies, weighting parameters, and data regimes.  

We explore the impact of:

- **Retraction type** (`qr`, `soft`, `soft_polar`, `none`),  

- **Trade-off weight** `w`, controlling the balance between fidelity and orthogonality,  

- **Non-negativity enforcement**, and  

- **Dataset characteristics**, including *small*, *large*, and *sparse-negative* synthetic data.  

To ensure robustness, each configuration is repeated three times with independent random seeds, and timing is measured using `microbenchmark` for consistency.  

```{r experiments}
smallw=c(0.0, 0.001, 0.01)
configs <- expand.grid(
  w = c(smallw, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0), 
  retraction = c("qr", "soft", "soft_polar", "none" ), 
  apply_nonneg = c(FALSE, TRUE),
  dataset = c("small", "large", "sparse_neg"),
  x0_null = c(FALSE, TRUE),
  stringsAsFactors = FALSE
)

configs_fast <- expand.grid(
  w = c(0.01,  0.25, 0.5, 0.75, 0.9 ), 
  retraction = c("qr", "polar", "soft", "soft_polar", "none" ), 
  apply_nonneg = c( TRUE, FALSE ),
  dataset = c("small", "large", "sparse_neg"),
  x0_null = c( TRUE ),
  stringsAsFactors = FALSE
)

if ( ! exists("results") ) {
  n_reps <- 3
  results <- list()
  maxcfnfigs <- nrow(configs)
  for (i in 1:maxcfnfigs) {
    cfg <- configs[i,]
    data <- get(paste0("data_", cfg$dataset))
    X0_use <- if (cfg$x0_null) data$Y0 else data$X0
    rep_results <- lapply(1:n_reps, function(rep) {
      mb <- microbenchmark(
        res <- ANTsR::nsa_flow( data$Y0, X0=X0_use, w=cfg$w, retraction=cfg$retraction, 
          initial_learning_rate=1e-2, max_iter=200, tol=1e-6, 
          # optimizer='bidirectional_lookahead',
                        apply_nonneg=cfg$apply_nonneg, seed=42+rep, verbose=FALSE, plot=TRUE),
        times=3
      )
      res$timing <- mb$time / 1e9  # Seconds
      res$rep <- rep
      res$Y0 = data$Y0
      res
    })
    results[[i]] <- rep_results
  } 
}
```


```{r plots,eval=FALSE}
# Aggregate traces
all_traces <- lapply(1:length(results), function(i) {
  reps <- results[[i]]
  cfg <- configs[i,]
  df <- do.call(rbind, lapply(reps, function(r) {
    tr <- r$traces
    tr$config_id <- i
    tr$rep <- r$rep
    tr
  }))
  df$w <- cfg$w
  df$retraction <- cfg$retraction
  df$apply_nonneg <- cfg$apply_nonneg
  df$dataset <- cfg$dataset
  df$x0_null <- cfg$x0_null
  df
}) %>% do.call(rbind, .)

# Long format
trace_long <- all_traces %>%
  pivot_longer(cols = c(fidelity, orthogonality, total_energy), names_to = "Metric", values_to = "Value")

# Faceted plot, grouped by dataset/retraction
p <- ggplot(trace_long, aes(x = iter, y = Value, color = Metric, group = interaction(rep, Metric))) +
  geom_line(alpha = 0.7) +
  facet_grid(dataset + retraction ~ w + apply_nonneg + x0_null, scales = "free_y") +
  scale_y_log10() +  # Optional log
  theme_minimal() +
  labs(title = "Enhanced NSA-Flow Traces (Mean over Reps, Log Scale)")
print(p)
```

# Results

## Default Metrics Summary

This section summarizes quantitative performance metrics for each experimental configuration.  
For every retraction method, dataset, and weight `w`, we compute:

- **Fidelity**: deviation of the optimized solution \(Y\) from its target \(X_0\) (lower is better).  

- **Orthogonality defect**: deviation from orthonormality of \(Y\) (lower is better).  

- **Energy**: final objective energy reported by the solver.  

- **Timing**: average runtime (in seconds) from microbenchmarked repetitions.  


Each configuration is evaluated across multiple repetitions, and the mean values are aggregated into `summary_df` for statistical comparison.


```{r tests}
# Summarize per config: mean/sd of finals, timing, rank
library(dplyr)

summary_df <- lapply(seq_along(results), function(i) {
  reps <- results[[i]]
  
  finals <- sapply(reps, function(r) {
    # Use X0 if available, otherwise Y0
    tardata <- if (!is.null(r$X0)) r$X0 else r$Y0
    
    c(
      fid_0 = norm(tardata, "F"),
      fid = norm(tardata - r$Y, "F"),
      orth_0 = ANTsR::invariant_orthogonality_defect(r$Y0),
      orth = ANTsR::invariant_orthogonality_defect(r$Y),
      energy = tail(r$traces$total_energy, 1),
      timing = mean(r$timing),
      rank_final = effective_rank(r$Y),
      residual = orth_residual(r$Y)
    )
  })
  
  # Compute means and standard deviations across repetitions
  means <- rowMeans(finals)
  sds <- apply(finals, 1, sd)
  
  cfg <- configs[i, ]
  
  data.frame(
    config_id = i, w = cfg$w, retraction = cfg$retraction, apply_nonneg = cfg$apply_nonneg,
    dataset = cfg$dataset, x0_null = cfg$x0_null,
    mean_fid_0 = means["fid_0"], # sd_fid_0 = sds["fid_0"],
    mean_fid = means["fid"], # sd_fid = sds["fid"],
    mean_orth_0 = means["orth_0"], # sd_orth_0 = sds["orth_0"],
    mean_orth = means["orth"], # sd_orth = sds["orth"],
    mean_energy = means["energy"], # sd_energy = sds["energy"],
    mean_timing = means["timing"] #sd_timing = sds["timing"]
  )
}) %>% bind_rows()
rownames(summary_df) <- NULL
# gt::gt(summary_df)
# kable(summary_df, digits=4, caption="Config Summary (Mean/SD over Reps)")

# Pass/fail: Similar to original, but on means
# ...
```

## Edge Case Tests

We next validate the stability and robustness of the algorithm under atypical or numerically extreme conditions.
These tests ensure that the `nsa_flow()` implementation behaves predictably even with degenerate, singular, or small-scale inputs.

The following cases are examined:

1.	Zero data (all entries = 0).

2.	Singular or rank-deficient inputs.

3.	Extremely small tolerance or few iterations.

4.	Nonnegativity constraint enforcement.

5.	Retract-type sensitivity between QR and Soft projections.

Each test reports whether it completes successfully and key numerical diagnostics.

```{r edge_tests,echo=FALSE}
# Edge Case Tests ----
set.seed(456)

cat("\n🧪 Running edge-case stability tests...\n")
# Edge: Zero-norm
data_zero <- list(Y0=matrix(0, 40, 3), X0=matrix(0, 40, 3))

# Edge: Singular
data_singular <- list(Y0=qr.Q(qr(matrix(rnorm(40*2), 40, 2))), X0=NULL)  # Rank-deficient

edge_tests <- list()

# Helper to safely run optimization
safe_run <- function(expr) {
  tryCatch(
    {
      res <- eval(expr)
      list(success = TRUE, res = res)
    },
    error = function(e) list(success = FALSE, message = e$message)
  )
}

# 1. Zero data matrix ----------------------------------------------------
set.seed(123)
data_zero <- matrix(0, 10, 5)

edge_tests$zero <- safe_run({
  ANTsR::nsa_flow(Y0 = data_zero,
           X0 = NULL,
           w = 0.5,
           max_iter = 20,
           tol = 1e-5,
           retraction = "soft",
           verbose = FALSE)
})

if (edge_tests$zero$success) {
  cat("✅ Zero data test: ran successfully.\n")
  Yz <- edge_tests$zero$res$Y
  cat("  → Output finite:", all(is.finite(Yz)), "\n")
  cat("  → Mean abs(Y):", mean(abs(Yz)), "\n\n")
} else {
  cat("❌ Zero data test failed (expected):", edge_tests$zero$message, "\n\n")
}


# 2. Singular data matrix (rank-deficient) -------------------------------

edge_tests$singular <- safe_run({
  ANTsR::nsa_flow(Y0 = data_singular$Y0,
           X0 = data_singular$X0,
           w = 0.3,
           retraction = "qr",
           max_iter = 50,
           tol = 1e-5,
           verbose = FALSE)
})

if (edge_tests$singular$success) {
  cat("✅ Singular data test: ran successfully.\n")
  Ys <- edge_tests$singular$res$Y
  cat("  → Orthogonality defect:",
      ANTsR::invariant_orthogonality_defect(Ys), "\n\n")
} else {
  cat("❌ Singular data test failed:", edge_tests$singular$message, "\n\n")
}


# 3. Very small tolerance & few iterations -------------------------------
edge_tests$tight_tol <- safe_run({
  ANTsR::nsa_flow(Y0 = matrix(runif(20), 5, 4),
           X0 = matrix(runif(20), 5, 4),
           w = 0.7,
           retraction = "soft",
           tol = 1e-10,
           max_iter = 5,
           verbose = FALSE)
})

if (edge_tests$tight_tol$success) {
  cat("✅ Tight tolerance test ran.\n")
  cat("  → Iterations completed:", edge_tests$tight_tol$res$final_iter, "\n\n")
} else {
  cat("❌ Tight tolerance test failed:", edge_tests$tight_tol$message, "\n\n")
}


# 4. Nonnegativity enforcement stability ---------------------------------
edge_tests$nonneg <- safe_run({
  ANTsR::nsa_flow(Y0 = matrix(rnorm(20), 5, 4),
           X0 = NULL,
           w = 0.5,
           apply_nonneg = TRUE,
           retraction = "soft",
           max_iter = 30,
           verbose = FALSE)
})

if (edge_tests$nonneg$success) {
  cat("✅ Nonnegativity test ran.\n")
  Yn <- edge_tests$nonneg$res$Y
  cat("  → Any negative entries:", any(Yn < -1e-12), "\n\n")
} else {
  cat("❌ Nonnegativity test failed:", edge_tests$nonneg$message, "\n\n")
}


# 5. Large retraction sensitivity ----------------------------------------
edge_tests$retract_compare <- safe_run({
  Ytest <- matrix(runif(30), 6, 5)
  list(
    qr = ANTsR::nsa_flow(Y0 = Ytest, X0 = Ytest, w = 0.4, retraction = "qr", max_iter = 30),
    soft = ANTsR::nsa_flow(Y0 = Ytest, X0 = Ytest, w = 0.4, retraction = "soft", max_iter = 30)
  )
})

if (edge_tests$retract_compare$success) {
  cat("✅ Retraction comparison ran.\n")
  Yqr <- edge_tests$retract_compare$res$qr$Y
  Ysoft <- edge_tests$retract_compare$res$soft$Y
  cat("  → Orth defect (QR):", ANTsR::invariant_orthogonality_defect(Yqr), "\n")
  cat("  → Orth defect (Soft):", ANTsR::invariant_orthogonality_defect(Ysoft), "\n\n")
} else {
  cat("❌ Retraction comparison failed:", edge_tests$retract_compare$message, "\n\n")
}


# --- Summary ---
cat("🧭 Edge test summary:\n")
data.frame(
  Test = names(edge_tests),
  Success = sapply(edge_tests, function(x) x$success)
)
```

# Summary and Verdicts


```{r verdicts}
# --- Compute test passes for each config ---
summary_df <- summary_df %>%
  mutate(
    fid_pass  = mean_fid < mean_fid_0,
    orth_pass = mean_orth < mean_orth_0 & ( ! (w %in% smallw) )  # Skip orth test for very small w
  )

# --- Determine which tests are expected to pass ---
summary_df <- summary_df %>%
  mutate(
    target_type = case_when(
      w == 0        ~ "fidelity_only",
      w == 1        ~ "orth_only",
      between(w, 0, 1) ~ "mixed",
      TRUE          ~ "unknown"
    ),
    target_pass = case_when(
      target_type == "fidelity_only" ~ fid_pass,
      target_type == "orth_only"     ~ orth_pass,
      target_type == "mixed"         ~ (fid_pass & orth_pass),
      TRUE                           ~ FALSE
    )
  )

# --- Compute global pass/fail status ---
if (all(summary_df$target_pass, na.rm = TRUE)) {
  cat("✅ All targeted tests passed.\n")
} else {
  cat("❌ Some targeted tests failed.\n")
}

# --- Summaries by logical condition ---
cat("\nDetailed pass summary by target type:\n")
print(
  summary_df %>%
    group_by(target_type) %>%
    summarise(
      n = n(),
      n_pass = sum(target_pass),
      pass_rate = n_pass / n
    )
)

# --- Nonnegativity summary ---
cat("\nNonnegativity impact summary:\n")
print(
  summary_df %>%
    group_by(apply_nonneg) %>%
    summarise(
      mean_fid = mean(mean_fid),
      mean_orth = mean(mean_orth),
      n_pass = sum(target_pass),
      n = n(),
      pass_rate = n_pass / n
    )
)

# --- Optional: pretty visual summary ---
library(ggplot2)
ggplot(summary_df, aes(x = factor(w), y = mean_fid / mean_fid_0,
                       fill = apply_nonneg)) +
  geom_col(position = "dodge") +
  facet_wrap(~retraction + x0_null, ncol = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Relative Fidelity (mean_fid / mean_fid_0)",
    x = "Weight w",
    y = "Relative change (<1 means improvement)",
    fill = "Nonnegativity"
  ) +
  theme_minimal(base_size = 12)



```

These quantitative and stress tests jointly confirm that:


* The Soft retraction method provides the most stable convergence across datasets and edge conditions.

* The solver remains numerically robust for nearly singular and zero-valued data.

* Nonnegativity constraints are properly enforced without introducing instability.

* Tight tolerances and low iteration counts do not produce divergent or undefined states.

Together, these tests validate both the correctness and numerical resilience of the NSA-Flow implementation across a wide range of operating conditions.


# Discussion

## Relative Fidelity 
```{r,fig.width=10, fig.height=6}


tradeoff_df <- summary_df %>%
  # filter to mixed or target conditions if desired
  # filter(target_type == "mixed") %>%
  group_by(retraction, dataset, w) %>%
  summarise(
    rel_orth = mean(mean_orth / mean_orth_0, na.rm = TRUE),
    rel_fid  = mean(mean_fid  / mean_fid_0,  na.rm = TRUE),
    .groups = "drop"
  )

ggplot(tradeoff_df, aes(x = w, y = rel_fid, color = dataset, group = dataset)) +
  geom_line(linewidth = 1.1) +
  geom_point(size = 2) +
  facet_wrap(~ retraction, scales = "free_y") +
  labs(title = "Relative Fidelity by Retraction and Dataset",
       y = "Relative Fidelity", x = "Tradeoff weight w") +
  theme_minimal(base_size = 13)
```

## Relative Orthogonality 
```{r,fig.width=10, fig.height=6}
ggplot(tradeoff_df, aes(x = w, y = rel_orth, color = dataset, group = dataset)) +
  geom_line(linewidth = 1.1) +
  geom_point(size = 2) +
  facet_wrap(~ retraction, scales = "free_y") +
  labs(title = "Relative Orthogonality by Retraction and Dataset",
       y = "Relative Orthogonality", x = "Tradeoff weight w") +
  theme_minimal(base_size = 13)
```

**Figure:** *Relative orthogonality and fidelity (lower = better) for each retraction method, dataset, and weighting parameter (`w`). QR retraction consistently yields the lowest combined error, with soft–polar offering a strong secondary balance. Unretracted (“none”) cases show incomplete convergence and weaker stability.*


## Summary and Recommendations on Retraction Choices in NSA-Flow

The Soft retraction provides the best overall trade-off between orthogonality and fidelity across datasets and weights — in particular it achieves substantially lower relative fidelity (i.e., better fidelity) while driving orthogonality improvements comparable to other aggressive retractions at practical $\omega$ values. Numerical examples follow.



Below are a few examples. For each, we show rel_orth and rel_fid and the combined score. Notice that for practical w values the Soft retraction attains much lower fidelity (i.e. rel_fid closer to 0) while achieving orthogonality reductions comparable to QR/Polar.

* Example (Large dataset, w=0.90):

* soft: rel_orth = 2.2278e-03, rel_fid = 0.6888

* qr  : rel_orth = 1.6396e-03, rel_fid = 0.9550

* Interpretation: QR gives marginally better orthogonality, but Soft gives substantially better fidelity — overall Soft has a better combined score.

* Example (Small dataset, w=0.25):

* soft: rel_orth = 1.1921e-01, rel_fid = 0.5768

* qr  : rel_orth = 1.3300e-02, rel_fid = 0.9435

* Interpretation: QR improves orthogonality more strongly, but Soft preserves fidelity to a much greater extent.

These patterns repeat across the results: Soft tends to offer a superior balance (especially important when fidelity matters).


⸻


## Conclusion

* Soft retraction is the recommended default when both fidelity and orthogonality matter: in the supplied tradeoff_df it produces the best balanced outcomes (lowest combined score) across large, small, and sparse_neg datasets for practical $\omega$ values.

* QR and Polar remain excellent choices when strict orthogonality is the single top priority (they produce extremely low rel_orth), but they often do so at the cost of fidelity.

* The choice of $\omega$ is critical: moderate values (e.g. 0.25–0.6) often yield the best balanced behavior.

## Mathematical Appendix: 


Let (Y_{\text{cand}} \in \mathbb{R}^{p\times k}) be the candidate iterate; denote (Y^\top Y) by (A).

### QR retraction

Compute the (economy) QR decomposition (Y_{\text{cand}} = Q R). Then
[
Y_{\text{tilde}} = Q , \operatorname{diag}(\operatorname{sign}(\operatorname{diag}(R))).
]
This enforces exact orthonormal columns (up to column signs).

## Polar retraction

[
Y_{\text{tilde}} = Y_{\text{cand}} , (Y_{\text{cand}}^\top Y_{\text{cand}})^{-1/2}.
]
Here ((\cdot)^{-1/2}) is the symmetric inverse square root of the matrix (A) (see Appendix).

## Soft (QR-based) retraction

Let (Q) be the QR-orthogonal factor of (Y_{\text{cand}}) (with sign-corrected columns). Interpolate:
[
Y_{\text{tilde}} = (1 - w),Y_{\text{cand}} + w,Q,
]
and then rescale (Y_{\text{tilde}}) to match the Frobenius norm of (Y_{\text{cand}}) to keep scale consistent.

### Soft-Polar

Interpolate in the polar-transform space:
[
Y_{\text{tilde}} = Y_{\text{cand}} \left[(1-w) I + w, (Y_{\text{cand}}^\top Y_{\text{cand}})^{-1/2}\right].
]




### computing ((Y^\top Y)^{-1/2})

To compute the symmetric inverse square root of the symmetric positive (semi-)definite matrix (A = Y^\top Y):

1.	Compute eigen-decomposition: (A = U \Lambda U^\top) where (\Lambda = \operatorname{diag}(\lambda_i)).

2.	Form (\Lambda^{-1/2} = \operatorname{diag}(\lambda_i^{-1/2})), with small-eigenvalue regularization:
(\lambda_i^{-1/2} \leftarrow (\lambda_i + \varepsilon)^{-1/2}) for (\varepsilon = 10^{-12}).

3.	Then (A^{-1/2} = U \Lambda^{-1/2} U^\top).


This yields a numerically stable symmetric inverse square root used by the polar/soft-polar retractions.


```{r sessioninfo}
sessionInfo()
```
