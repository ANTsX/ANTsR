---
title: "NSA-Flow Optimization ‚Äî Test Suite and Analysis"
author: "B. Avants"
date: "October 12, 2025"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: united
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width=12, fig.height=8)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(knitr)
library(microbenchmark)  # For timing
library(ANTsR)  # Assumes missing functions like create_optimizer, step are here
```

# Overview

This NSA-Flow test suite considers:

- all retraction options.

- diverse datasets: small, large, sparse with negatives.

- Multiple reps (n=3) per config for mean/sd stats.

- metrics: timing, orthogonality, reconstruction error (fidelity).

- Edge cases: zero-norm, singular inits.

- Quantitative pass/fail based on expected behavior per weight ($\omega$).

```{r utilities}
library(ANTsR)
orth_residual <- invariant_orthogonality_defect
neg_violation <- function(Y) sum(pmax(0, -Y))

# New: Rank check
effective_rank <- function(Y, tol=1e-6) {
  s <- svd(Y)$d
  sum(s > tol * max(s))
}


```


# Methods

## Synthetic Data Generation

To systematically evaluate the behavior of the retraction methods under controlled conditions, we construct several families of **synthetic datasets** that vary in **dimensionality**, **correlation structure**, and **sparsity**.  This allows us to probe the algorithm‚Äôs robustness across dense, correlated, and sparse or sign-heterogeneous data regimes. Each dataset represents a canonical stress-test configuration:

| Dataset name    | Description |
|-----------------|--------------|
| **small**       | Moderate dimension (p = 90, k = 10), low correlation (œÅ = 0.2); emulates structured, small-scale settings. |
| **large**       | Larger system (p = 200, k = 20), moderate correlation (œÅ = 0.3); tests scalability and performance under multicollinearity. |
| **sparse_neg**  | Sparse with mixed signs (‚âà 50% zeros, negative entries included); tests stability and nonnegativity effects. |

---

Let \( p \) and \( k \) denote the number of samples and features, respectively.  We begin from an orthogonal base frame \(V_0 \in \mathbb{R}^{p \times k}\) obtained via QR decomposition of a random Gaussian matrix:

\[
V_0 = \text{qr.Q}\big(\text{qr}(\mathcal{N}(0,1)_{p \times k})\big).
\]

We then generate correlated data using a covariance matrix
\[
\Sigma_{ij} = 
\begin{cases}
1 & \text{if } i = j,\\
\rho & \text{otherwise},
\end{cases}
\]
where \( \rho \) controls inter-feature correlation.  
Samples are drawn from \( \mathcal{N}(0, \Sigma) \) to yield \( Y_0 \).  
The corresponding nonnegative target \( X_0 \) is constructed as
\[
X_0 = \max\left(Y_0 + 0.2\,\mathcal{N}(0,1),\, 0\right).
\]
Optionally, sparsity and sign heterogeneity are imposed:
\[
Y_{0,ij} =
\begin{cases}
0 & \text{with probability } p_{\text{sparse}},\\
Y_{0,ij} - 0.5 & \text{if include\_neg = TRUE}.
\end{cases}
\]

This yields diverse matrices \( (Y_0, X_0, V_0) \) with controllable noise, correlation, and structure for robust evaluation.



```{r data_gen}
generate_synth_data <- function(p=40, k=3, noise=0.5, corr=0.1,
                                sparse_prob=0.2, include_neg=FALSE) {
  V0 <- qr.Q(qr(matrix(rnorm(p*k), p, k)))
  library(MASS)
  # Correlated data
  Sigma <- matrix(corr, k, k); diag(Sigma) <- 1
  Y0 <- mvrnorm(n = p, mu = rep(0, k), Sigma = Sigma)
  if (include_neg) Y0 <- Y0 - 0.5
  if (sparse_prob > 0) Y0[runif(p*k) < sparse_prob] <- 0
  X0 <- pmax(Y0 + 0.2 * matrix(rnorm(p*k), p, k), 0)
  list(Y0=Y0, X0=X0, V0=V0)
}

# Generate datasets
data_small      <- generate_synth_data(p=90,  k=10, corr=0.2)
data_wide = data_small  # Wide version
for ( k in 1:length(data_wide) ) {
  data_wide[[k]] <- t(data_wide[[k]])
}
data_large      <- generate_synth_data(p=200, k=20, corr=0.3)
data_sparse_neg <- generate_synth_data(sparse_prob=0.5, include_neg=TRUE)



dataset_stats <- function(name, data) {
  Y0 <- data$Y0
  cor_vals <- cor(Y0)
  mean_corr <- mean(abs(cor_vals[upper.tri(cor_vals)]))
  sparsity <- mean(Y0 == 0)
  neg_frac <- mean(Y0 < 0)
  scale_mean <- mean(abs(Y0))
  data.frame(
    Dataset = name,
    Mean_Abs_Correlation = round(mean_corr, 3),
    Sparsity = round(sparsity, 3),
    Negative_Fraction = round(neg_frac, 3),
    Mean_Abs_Value = round(scale_mean, 3)
  )
}

summary_table <- rbind(
  dataset_stats("Small", data_small),
  dataset_stats("Wide", data_wide),
  dataset_stats("Large", data_large),
  dataset_stats("Sparse_Neg", data_sparse_neg)
)

knitr::kable(summary_table, caption="Structural characteristics of synthetic datasets.")
```


The following figure shows the heatmap structure of the generated matrices. Color intensity represents magnitude; blue tones indicate positive values, and red tones (where applicable) indicate negatives.

```{r}
library(ggplot2)
library(reshape2)
library(patchwork)

plot_heat <- function(mat, title) {
  df <- melt(mat)
  ggplot(df, aes(Var2, Var1, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low="red", mid="white", high="blue", midpoint=0) +
    theme_minimal(base_size = 12) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank(),
          panel.grid=element_blank(),
          plot.title=element_text(face="bold", hjust=0.5)) +
    labs(title=title, x=NULL, y=NULL, fill="Value")
}

p_small  <- plot_heat(data_small$Y0, "Small Dataset (œÅ=0.2)")
p_large  <- plot_heat(data_large$Y0, "Large Dataset (œÅ=0.3)")
p_sparse <- plot_heat(data_sparse_neg$Y0, "Sparse Negative Dataset")

(p_small | p_large | p_sparse) + plot_layout(guides = "collect")
```

Figure 1. Synthetic data matrices used for benchmarking.
From left to right:

* Small dataset: moderately correlated structure with low noise.

* Wide dataset: transposed small dataset.

* Large dataset: increased dimensionality and correlation.

* Sparse-negative dataset: structured sparsity and sign heterogeneity, introducing strong nonlinearity and nonnegativity challenges.


The summary table confirms the diversity of these datasets:

* Small dataset: Moderate correlation (‚âà0.2) and minimal sparsity, representing a well-conditioned test case.

* Large dataset: Similar correlation strength but higher feature count, stressing scalability and convergence stability.

* Sparse-negative dataset: Extremely sparse (~50%) and sign-diverse (‚âà50% negative), producing high-contrast signals and providing a stringent test for nonnegativity-constrained optimization.



## Experimental Configurations for NSA-Flow

In this section, we define the experimental configuration grid used to evaluate the performance of the NSA-Flow algorithm across a range of retraction strategies, weighting parameters, and data regimes.  

We explore the impact of:

- **Retraction type** (`qr`, `soft`, `soft_polar`, `none`),  

- **Trade-off weight** `w`, controlling the balance between fidelity and orthogonality,  

- **Non-negativity enforcement**, and  

- **Dataset characteristics**, including *small*, *large*, and *sparse-negative* synthetic data.  

To ensure robustness, each configuration is repeated three times with independent random seeds, and timing is measured using `microbenchmark` for consistency.  

```{r experiments,echo=FALSE}

filter_config_space <- function(configs) {
  # Filter out invalid combinations: dataset "wide" with QR-based retractions
  invalid <- configs$dataset == "wide" & configs$retraction %in% c("qr", "soft_qr")
  configs[!invalid, ]
}

smallw=c(0.0, 0.001, 0.01)
configs <- filter_config_space( expand.grid(
  w = c(smallw, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0), 
  retraction = c( "svd", "polar", "qr", "soft_svd", "soft_polar", "soft_qr" ), 
  apply_nonneg = c(FALSE, TRUE),
  dataset = c("small", "wide", "large", "sparse_neg"),
  x0_null = c(FALSE, TRUE),
  stringsAsFactors = FALSE
) )

configs_fast <- filter_config_space( expand.grid(
  w = c(0.01,  0.25, 0.5, 0.75, 0.9 ), 
  retraction = c( "svd", "polar", "qr", "soft_svd", "soft_polar", "soft_qr" ), 
  apply_nonneg = c( TRUE ),
  dataset = c("small", "wide", "large", "sparse_neg"),
  x0_null = c( TRUE ),
  stringsAsFactors = FALSE
) )

if ( ! exists("results") ) {
  n_reps <- 3
  results <- list()
  maxcfnfigs <- nrow(configs)
  for (i in 1:maxcfnfigs) {
    cfg <- configs[i,]
    data <- get(paste0("data_", cfg$dataset))
    X0_use <- if (cfg$x0_null) data$Y0 else data$X0
    rep_results <- lapply(1:n_reps, function(rep) {
      mb <- microbenchmark(
        res <- ANTsR::nsa_flow( data$Y0, X0=X0_use, w=cfg$w, retraction=cfg$retraction, 
          optimizer='armijo_gradient',
          initial_learning_rate=1e-3,
          apply_nonneg=cfg$apply_nonneg, seed=42+rep, verbose=FALSE, plot=TRUE),
        times=3
      )
      res$timing <- mb$time / 1e9  # Seconds
      res$rep <- rep
      res$Y0 = data$Y0
      res
    })
    results[[i]] <- rep_results
  } 
}
```


```{r plots,eval=FALSE}
# Aggregate traces
all_traces <- lapply(1:length(results), function(i) {
  reps <- results[[i]]
  cfg <- configs[i,]
  df <- do.call(rbind, lapply(reps, function(r) {
    tr <- r$traces
    tr$config_id <- i
    tr$rep <- r$rep
    tr
  }))
  df$w <- cfg$w
  df$retraction <- cfg$retraction
  df$apply_nonneg <- cfg$apply_nonneg
  df$dataset <- cfg$dataset
  df$x0_null <- cfg$x0_null
  df
}) %>% do.call(rbind, .)

# Long format
trace_long <- all_traces %>%
  pivot_longer(cols = c(fidelity, orthogonality, total_energy), names_to = "Metric", values_to = "Value")

# Faceted plot, grouped by dataset/retraction
p <- ggplot(trace_long, aes(x = iter, y = Value, color = Metric, group = interaction(rep, Metric))) +
  geom_line(alpha = 0.7) +
  facet_grid(dataset + retraction ~ w + apply_nonneg + x0_null, scales = "free_y") +
  scale_y_log10() +  # Optional log
  theme_minimal() +
  labs(title = "Enhanced NSA-Flow Traces (Mean over Reps, Log Scale)")
print(p)
```

# Results

## Default Metrics Summary

This section summarizes quantitative performance metrics for each experimental configuration.  
For every retraction method, dataset, and weight `w`, we compute:

- **Fidelity**: deviation of the optimized solution \(Y\) from its target \(X_0\) (lower is better).  

- **Orthogonality defect**: deviation from orthonormality of \(Y\) (lower is better).  

- **Energy**: final objective energy reported by the solver.  

- **Timing**: average runtime (in seconds) from microbenchmarked repetitions.  


Each configuration is evaluated across multiple repetitions, and the mean values are aggregated into `summary_df` for statistical comparison.


```{r tests}
# Summarize per config: mean/sd of finals, timing, rank
library(dplyr)

summary_df <- lapply(seq_along(results), function(i) {
  reps <- results[[i]]
  
  finals <- sapply(reps, function(r) {
    # Use X0 if available, otherwise Y0
    tardata <- if (!is.null(r$X0)) r$X0 else r$Y0
    
    c(
      fid_0 = norm(tardata, "F"),
      fid = norm(tardata - r$Y, "F"),
      orth_0 = ANTsR::invariant_orthogonality_defect(r$Y0),
      orth = ANTsR::invariant_orthogonality_defect(r$Y),
      energy = tail(r$traces$total_energy, 1),
      timing = mean(r$timing),
      rank_final = effective_rank(r$Y),
      residual = orth_residual(r$Y)
    )
  })
  
  # Compute means and standard deviations across repetitions
  means <- rowMeans(finals)
  sds <- apply(finals, 1, sd)
  
  cfg <- configs[i, ]
  
  data.frame(
    config_id = i, w = cfg$w, retraction = cfg$retraction, apply_nonneg = cfg$apply_nonneg,
    dataset = cfg$dataset, x0_null = cfg$x0_null,
    mean_fid_0 = means["fid_0"], # sd_fid_0 = sds["fid_0"],
    mean_fid = means["fid"], # sd_fid = sds["fid"],
    mean_orth_0 = means["orth_0"], # sd_orth_0 = sds["orth_0"],
    mean_orth = means["orth"], # sd_orth = sds["orth"],
    mean_energy = means["energy"], # sd_energy = sds["energy"],
    mean_timing = means["timing"] #sd_timing = sds["timing"]
  )
}) %>% bind_rows()
rownames(summary_df) <- NULL
# gt::gt(summary_df)
# kable(summary_df, digits=4, caption="Config Summary (Mean/SD over Reps)")

# Pass/fail: Similar to original, but on means
# ...
```

## Edge Case Tests

We next validate the stability and robustness of the algorithm under atypical or numerically extreme conditions.
These tests ensure that the `nsa_flow()` implementation behaves predictably even with degenerate, singular, or small-scale inputs.

The following cases are examined:

1.	Zero data (all entries = 0).

2.	Singular or rank-deficient inputs.

3.	Extremely small tolerance or few iterations.

4.	Nonnegativity constraint enforcement.

5.	Retract-type sensitivity between QR and Soft projections.

Each test reports whether it completes successfully and key numerical diagnostics.

```{r edge_tests,echo=FALSE}
# Edge Case Tests ----
set.seed(456)

cat("\nüß™ Running edge-case stability tests...\n")
# Edge: Zero-norm
data_zero <- list(Y0=matrix(0, 40, 3), X0=matrix(0, 40, 3))

# Edge: Singular
data_singular <- list(Y0=qr.Q(qr(matrix(rnorm(40*2), 40, 2))), X0=NULL)  # Rank-deficient

edge_tests <- list()

# Helper to safely run optimization
safe_run <- function(expr) {
  tryCatch(
    {
      res <- eval(expr)
      list(success = TRUE, res = res)
    },
    error = function(e) list(success = FALSE, message = e$message)
  )
}

# 1. Zero data matrix ----------------------------------------------------
set.seed(123)
data_zero <- matrix(0, 10, 5)

edge_tests$zero <- safe_run({
  ANTsR::nsa_flow(Y0 = data_zero,
           X0 = NULL,
           w = 0.5,
           max_iter = 20,
           tol = 1e-5,
           retraction = "soft_svd",
           verbose = FALSE)
})

if (edge_tests$zero$success) {
  cat("‚úÖ Zero data test: ran successfully.\n")
  Yz <- edge_tests$zero$res$Y
  cat("  ‚Üí Output finite:", all(is.finite(Yz)), "\n")
  cat("  ‚Üí Mean abs(Y):", mean(abs(Yz)), "\n\n")
} else {
  cat("‚ùå Zero data test failed (expected):", edge_tests$zero$message, "\n\n")
}


# 2. Singular data matrix (rank-deficient) -------------------------------

edge_tests$singular <- safe_run({
  ANTsR::nsa_flow(Y0 = data_singular$Y0,
           X0 = data_singular$X0,
           w = 0.3,
           retraction = "soft_svd",
           max_iter = 50,
           tol = 1e-5,
           verbose = FALSE)
})

if (edge_tests$singular$success) {
  cat("‚úÖ Singular data test: ran successfully.\n")
  Ys <- edge_tests$singular$res$Y
  cat("  ‚Üí Orthogonality defect:",
      ANTsR::invariant_orthogonality_defect(Ys), "\n\n")
} else {
  cat("‚ùå Singular data test failed:", edge_tests$singular$message, "\n\n")
}


# 3. Very small tolerance & few iterations -------------------------------
edge_tests$tight_tol <- safe_run({
  ANTsR::nsa_flow(Y0 = matrix(runif(20), 5, 4),
           X0 = matrix(runif(20), 5, 4),
           w = 0.7,
           retraction = "soft_svd",
           tol = 1e-10,
           max_iter = 5,
           verbose = FALSE)
})

if (edge_tests$tight_tol$success) {
  cat("‚úÖ Tight tolerance test ran.\n")
  cat("  ‚Üí Iterations completed:", edge_tests$tight_tol$res$final_iter, "\n\n")
} else {
  cat("‚ùå Tight tolerance test failed:", edge_tests$tight_tol$message, "\n\n")
}


# 4. Nonnegativity enforcement stability ---------------------------------
edge_tests$nonneg <- safe_run({
  ANTsR::nsa_flow(Y0 = matrix(rnorm(20), 5, 4),
           X0 = NULL,
           w = 0.5,
           apply_nonneg = TRUE,
           retraction = "soft_svd",
           max_iter = 30,
           verbose = FALSE)
})

if (edge_tests$nonneg$success) {
  cat("‚úÖ Nonnegativity test ran.\n")
  Yn <- edge_tests$nonneg$res$Y
  cat("  ‚Üí Any negative entries:", any(Yn < -1e-12), "\n\n")
} else {
  cat("‚ùå Nonnegativity test failed:", edge_tests$nonneg$message, "\n\n")
}


# 5. Large retraction sensitivity ----------------------------------------
edge_tests$retract_compare <- safe_run({
  Ytest <- matrix(runif(30), 6, 5)
  list(
    qr = ANTsR::nsa_flow(Y0 = Ytest, X0 = Ytest, w = 0.4, retraction = "qr", max_iter = 30),
    soft = ANTsR::nsa_flow(Y0 = Ytest, X0 = Ytest, w = 0.4, retraction = "soft_svd", max_iter = 30)
  )
})

if (edge_tests$retract_compare$success) {
  cat("‚úÖ Retraction comparison ran.\n")
  Yqr <- edge_tests$retract_compare$res$qr$Y
  Ysoft <- edge_tests$retract_compare$res$soft$Y
  cat("  ‚Üí Orth defect (QR):", ANTsR::invariant_orthogonality_defect(Yqr), "\n")
  cat("  ‚Üí Orth defect (Soft):", ANTsR::invariant_orthogonality_defect(Ysoft), "\n\n")
} else {
  cat("‚ùå Retraction comparison failed:", edge_tests$retract_compare$message, "\n\n")
}


# --- Summary ---
cat("üß≠ Edge test summary:\n")
data.frame(
  Test = names(edge_tests),
  Success = sapply(edge_tests, function(x) x$success)
)
```

# Summary and Verdicts


```{r verdicts}
# --- Compute test passes for each config ---
summary_df <- summary_df %>%
  mutate(
    fid_pass  = mean_fid < mean_fid_0,
    orth_pass = mean_orth < mean_orth_0 & ( ! (w %in% smallw) )  # Skip orth test for very small w
  )

# --- Determine which tests are expected to pass ---
summary_df <- summary_df %>%
  mutate(
    target_type = case_when(
      w == 0        ~ "fidelity_only",
      w == 1        ~ "orth_only",
      between(w, 0, 1) ~ "mixed",
      TRUE          ~ "unknown"
    ),
    target_pass = case_when(
      target_type == "fidelity_only" ~ fid_pass,
      target_type == "orth_only"     ~ orth_pass,
      target_type == "mixed"         ~ (fid_pass & orth_pass),
      TRUE                           ~ FALSE
    )
  )

# --- Compute global pass/fail status ---
if (all(summary_df$target_pass, na.rm = TRUE)) {
  cat("‚úÖ All targeted tests passed.\n")
} else {
  cat("‚ùå Some targeted tests failed.\n")
}

# --- Summaries by logical condition ---
cat("\nDetailed pass summary by target type:\n")
print(
  summary_df %>%
    group_by(target_type) %>%
    summarise(
      n = n(),
      n_pass = sum(target_pass),
      pass_rate = n_pass / n
    )
)

# --- Nonnegativity summary ---
cat("\nNonnegativity impact summary:\n")
print(
  summary_df %>%
    group_by(apply_nonneg) %>%
    summarise(
      mean_fid = mean(mean_fid),
      mean_orth = mean(mean_orth),
      n_pass = sum(target_pass),
      n = n(),
      pass_rate = n_pass / n
    )
)

# --- Optional: pretty visual summary ---
library(ggplot2)
ggplot(summary_df, aes(x = factor(w), y = mean_fid / mean_fid_0,
                       fill = apply_nonneg)) +
  geom_col(position = "dodge") +
  facet_wrap(~retraction + x0_null, ncol = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Relative Fidelity (mean_fid / mean_fid_0)",
    x = "Weight w",
    y = "Relative change (<1 means improvement)",
    fill = "Nonnegativity"
  ) +
  theme_minimal(base_size = 12)



```

These quantitative and stress tests jointly confirm that:


* The Soft retraction method provides the most stable convergence across datasets and edge conditions.

* The solver remains numerically robust for nearly singular and zero-valued data.

* Nonnegativity constraints are properly enforced without introducing instability.

* Tight tolerances and low iteration counts do not produce divergent or undefined states.

Together, these tests validate both the correctness and numerical resilience of the NSA-Flow implementation across a wide range of operating conditions.


# Discussion

## Relative Fidelity 
```{r,fig.width=10, fig.height=6}


tradeoff_df <- summary_df %>%
  # filter to mixed or target conditions if desired
  # filter(target_type == "mixed") %>%
  group_by(retraction, dataset, w) %>%
  summarise(
    rel_orth = mean(mean_orth / mean_orth_0, na.rm = TRUE),
    rel_fid  = mean(mean_fid  / mean_fid_0,  na.rm = TRUE),
    .groups = "drop"
  )

ggplot(tradeoff_df, aes(x = w, y = rel_fid, color = dataset, group = dataset)) +
  geom_line(linewidth = 1.1) +
  geom_point(size = 2) +
  facet_wrap(~ retraction, scales = "free_y") +
  labs(title = "Relative Fidelity by Retraction and Dataset",
       y = "Relative Fidelity", x = "Tradeoff weight w") +
  theme_minimal(base_size = 13)
```

## Relative Orthogonality 
```{r,fig.width=10, fig.height=6}
ggplot(tradeoff_df, aes(x = w, y = rel_orth, color = dataset, group = dataset)) +
  geom_line(linewidth = 1.1) +
  geom_point(size = 2) +
  facet_wrap(~ retraction, scales = "free_y") +
  labs(title = "Relative Orthogonality by Retraction and Dataset",
       y = "Relative Orthogonality", x = "Tradeoff weight w") +
  theme_minimal(base_size = 13)
```

**Figure:** *Relative orthogonality and fidelity (lower = better) for each retraction method, dataset, and weighting parameter (`w`). QR retraction consistently yields the lowest combined error, with soft‚Äìpolar offering a strong secondary balance. Unretracted (‚Äúnone‚Äù) cases show incomplete convergence and weaker stability.*


## Summary and Recommendations on Retraction Choices in NSA-Flow

The Soft retraction provides the best overall trade-off between orthogonality and fidelity across datasets and weights ‚Äî in particular it achieves substantially lower relative fidelity (i.e., better fidelity) while driving orthogonality improvements comparable to other aggressive retractions at practical $\omega$ values. Numerical examples follow.



Below are a few examples. For each, we show rel_orth and rel_fid and the combined score. Notice that for practical w values the Soft retraction attains much lower fidelity (i.e. rel_fid closer to 0) while achieving orthogonality reductions comparable to QR/Polar.

* Example (Large dataset, w=0.90):

* soft: rel_orth = 2.2278e-03, rel_fid = 0.6888

* qr  : rel_orth = 1.6396e-03, rel_fid = 0.9550

* Interpretation: QR gives marginally better orthogonality, but Soft gives substantially better fidelity ‚Äî overall Soft has a better combined score.

* Example (Small dataset, w=0.25):

* soft: rel_orth = 1.1921e-01, rel_fid = 0.5768

* qr  : rel_orth = 1.3300e-02, rel_fid = 0.9435

* Interpretation: QR improves orthogonality more strongly, but Soft preserves fidelity to a much greater extent.

These patterns repeat across the results: Soft tends to offer a superior balance (especially important when fidelity matters).


‚∏ª


## Conclusion

* Soft retraction is the recommended default when both fidelity and orthogonality matter: in the supplied tradeoff_df it produces the best balanced outcomes (lowest combined score) across large, small, and sparse_neg datasets for practical $\omega$ values.

* QR and Polar remain excellent choices when strict orthogonality is the single top priority (they produce extremely low rel_orth), but they often do so at the cost of fidelity.

* The choice of $\omega$ is critical: moderate values (e.g. 0.25‚Äì0.6) often yield the best balanced behavior.

# Mathematical Appendix: 


## Mathematical Implementations of Retraction Methods

For a candidate matrix $Y \in \mathbb{R}^{m \times n}$ (with $m =$ rows, $n =$ columns; typically $m \geq n$ for well-posedness on Stiefel-like manifolds, but adaptable), the retraction $\Retr_w(Y) = \tilde{Y}$ maps $Y$ toward a nearby point on (or approximating) the Stiefel manifold of semi-orthogonal matrices (columns orthonormal, i.e., $\tilde{Y}^T \tilde{Y} = I_n$). The parameter $w \in [0,1]$ controls the blending strength toward orthogonality ($w=0$: identity; $w=1$: full retraction). All methods assume $Y$ has full column rank unless noted; numerical stabilizations (e.g., pseudoinverses) are implicit in decompositions.

### none: No retraction applied

No retraction applied. This preserves $Y$ exactly, useful for gradient steps without manifold enforcement.

$$
\tilde{Y} = Y
$$

### polar: Polar decomposition-based retraction

Polar decomposition-based retraction. Decompose $Y = U P$ where $P = \sqrt{Y^T Y}$ is the (symmetric positive semi-definite) right polar factor. The retraction projects to the orthogonal left factor $U$.

$$
\tilde{Y} = Y \left( Y^T Y + \epsilon I \right)^{-\frac{1}{2}}
$$

Here, $(\cdot)^{-\frac{1}{2}}$ is the symmetric matrix inverse square root (with regularization $\epsilon > 0$ small for ill-conditioning). Independent of $w$ (full enforcement).

### qr: Signed QR decomposition retraction

Signed QR decomposition retraction. Decompose $Y = Q R$ (thin QR: $Q \in \mathbb{R}^{m \times \min(m,n)}$ orthonormal columns, $R$ upper triangular). Adjust signs to ensure positive diagonal in $R$ (for uniqueness/stability).

$$
\tilde{Y} = Q \, \Diag\left( \sign(\diag(R)) \right)
$$

where $\sign(0) = 1$, and $\Diag(\cdot)$ forms a diagonal matrix (padded if $m < n$). Independent of $w$. Fails dimensionally if $m < n$ without full QR.

### soft (or soft_qr; identical in code): Soft (blended) QR retraction

Soft (blended) QR retraction. Compute the QR orthogonal part $Q$ as above, then convex-combine with $Y$ and rescale to match the Frobenius norm $\|Y\|_F = \sqrt{\tr(Y^T Y)}$.

$$
\tilde{Y} = \frac{ \|Y\|_F \cdot \left[ (1-w) Y + w Q \right] }{ \left\| (1-w) Y + w Q \right\|_F }
$$

The rescaling ensures scale preservation (important for fidelity terms in optimization). Fails if $m < n$.

### soft_polar: Soft polar retraction via right-multiplicative blending

Soft polar retraction via right-multiplicative blending. Compute the polar transformation $T_1 = (Y^T Y)^{-\frac{1}{2}}$ as above, then blend with identity.

$$
\tilde{Y} = Y \left[ (1-w) I_n + w \, T_1 \right]
$$

No norm rescaling (norm may shrink/grow slightly). Preserves the row space of $Y$ multiplicatively.

### svd: SVD-based orthogonal approximation (Procrustes solution)

SVD-based orthogonal approximation (Procrustes solution). Decompose $Y = U \Sigma V^T$ (economy SVD: $U \in \mathbb{R}^{m \times r}$, $V \in \mathbb{R}^{n \times r}$, $r = \rank(Y) \leq \min(m,n)$, $\Sigma$ diagonal non-negative). The retraction discards singular values for the closest orthogonal matrix in Frobenius norm.

$$
\tilde{Y} = U V^T
$$

(Padded with zeros if $r < \min(m,n)$; full rank assumed.) Independent of $w$.

### soft_svd: Soft SVD retraction

Soft SVD retraction. Compute the SVD orthogonal part $Q = U V^T$ as above, then convex-combine with $Y$ and rescale to match $\|Y\|_F$.

$$
\tilde{Y} = \frac{ \|Y\|_F \cdot \left[ (1-w) Y + w (U V^T) \right] }{ \left\| (1-w) Y + w (U V^T) \right\|_F }
$$

Similar to soft QR but uses SVD's balanced approximation. Works for $m < n$.

## Relationship Between Polar/SVD and Their Soft Variations

The **polar** and **SVD** retractions both approximate the "nearest" orthogonal matrix to $Y$, but differ in geometry, optimality, and computational profile‚Äîmaking them complementary on Stiefel-like manifolds (e.g., for orthogonal dictionary learning in NSA-Flow). 

### Core Differences

- **Polar** ($Y (Y^T Y)^{-1/2}$) is a *right-multiplicative* retraction: it preserves the *left singular vectors* (column space of $Y$) exactly, projecting orthogonally in the *row space* via the polar factor. It's the unique solution to minimizing $\| Y - U P \|_F$ over orthogonal $U$ and PSD $P$, and it's a true *retraction* on the Stiefel manifold (first-order consistency with geodesics). Computation: $O(m n^2 + n^3)$ via eigendecomposition of $n \times n$ Gram $Y^T Y$; sensitive to $n > m$ (ill-conditioned Gram).
  
- **SVD** ($U V^T$) is an *additive* (or balanced) approximation: it solves the orthogonal Procrustes problem, minimizing $\| Y - Q \|_F$ directly over orthogonal $Q$ (no PSD constraint). It balances left/right singular vectors, discarding $\Sigma$ symmetrically. Equivalent to polar *only if* $Y$ is square/full-rank *and* $V = I$ (rare); generally, $\| Y - \tilde{Y}_\polar \|_F \neq \| Y - \tilde{Y}_\svd \|_F$, with SVD often closer in absolute Frobenius distance but polar better for column-space fidelity. Computation: $O(m n \min(m,n))$; robust to $m < n$ (economy mode).

In practice, for overcomplete bases ($n > m$, e.g., wide matrices), polar may underperform due to Gram singularity (use pseudoinverse), while SVD naturally handles low rank via truncated modes. Both enforce $\tilde{Y}^T \tilde{Y} \approx I$ (up to regularization), but polar is "asymmetric" (right-action), SVD "symmetric."

### Soft Variations

- **Soft-polar** extends polar multiplicatively: $Y [(1-w) I + w (Y^T Y)^{-1/2}]$, a *geodesic convex combination* along the right-invariant metric (preserves scale directionally, no explicit norm fix). It's smoother for optimization (avoids abrupt jumps), tunable for partial enforcement, and inherits polar's column-space preservation. Ideal for Riemannian flows where tangent steps need gradual manifold pull-back.
  
- **Soft-SVD** (and analogously soft/soft_qr) is *additive*: $\|Y\|_F \cdot \frac{(1-w) Y + w Q}{\|(1-w) Y + w Q\|_F}$, a vector-space blend toward $Q$ (SVD or QR), with rescaling to match original scale (crucial for energy-preserving objectives like fidelity $\|Y - X\|_F^2$). This is more "Euclidean" than geodesic, potentially distorting geometry but computationally cheaper/simpler; rescaling mitigates norm drift in nonneg-constrained settings.

**Relationships**: Soft-polar is the "Riemannian analog" of soft-SVD‚Äîmultiplicative vs. additive blending mirrors polar vs. SVD's actions. For small $w$, both approximate the exponential map $\Retr(\eta) \approx Y + \eta$ (tangent $\eta$); for $w=1$, they reduce to pure polar/SVD. In NSA-Flow (balancing fidelity + orthogonality), soft-polar favors scale-stable convergence on tall matrices ($m \gg n$), while soft-SVD excels on wide/low-rank ($n > m$) by robustness. QR variants (hard/soft) are faster $O(m n^2)$ proxies to SVD (similar $Q \approx U V^T$ for full rank), but fail wide cases without mods. Overall, polar/SVD duality (asymmetric vs. balanced ortho) extends to soft: choose polar/soft-polar for space-preserving (e.g., neural embeddings), SVD/soft-SVD for minimal-distortion approximation (e.g., Procrustes alignment).


## Computing $(Y^T Y)^{-1/2}$

To compute the symmetric inverse square root of the symmetric positive (semi-)definite matrix $A = Y^T Y$:

1. Compute eigen-decomposition: $A = U \Lambda U^T$ where $\Lambda = \operatorname{diag}(\lambda_i)$.

2. Form $\Lambda^{-1/2} = \operatorname{diag}(\lambda_i^{-1/2})$, with small-eigenvalue regularization:  
   $\lambda_i^{-1/2} \leftarrow (\lambda_i + \varepsilon)^{-1/2}$ for $\varepsilon = 10^{-12}$.

3. Then $A^{-1/2} = U \Lambda^{-1/2} U^T$.

This yields a numerically stable symmetric inverse square root used by the polar/soft-polar retractions.

```{r sessioninfo}
sessionInfo()
```
