---
title: "NSA-Flow Optimization ‚Äî Test Suite and Analysis"
author: "B. Avants"
date: "October 12, 2025"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: united
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width=12, fig.height=8)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(knitr)
library(microbenchmark)  # For timing
library(ANTsR)  # Assumes missing functions like create_optimizer, step are here
```

# Overview

This NSA-Flow test suite considers:

- all retraction options ("soft", "qr", "polar", "none").

- diverse datasets: small, large, sparse with negatives.

- Multiple reps (n=3) per config for mean/sd stats.

- metrics: timing, orthogonality, reconstruction error (fidelity).

- Edge cases: zero-norm, singular inits.

- Quantitative pass/fail based on expected behavior per weight ($\omega$).

```{r utilities}
library(ANTsR)
orth_residual <- invariant_orthogonality_defect
neg_violation <- function(Y) sum(pmax(0, -Y))

# New: Rank check
effective_rank <- function(Y, tol=1e-6) {
  s <- svd(Y)$d
  sum(s > tol * max(s))
}


```


# Methods

## Synthetic Data Generation

To systematically evaluate the behavior of the retraction methods under controlled conditions, we construct several families of **synthetic datasets** that vary in **dimensionality**, **correlation structure**, and **sparsity**.  This allows us to probe the algorithm‚Äôs robustness across dense, correlated, and sparse or sign-heterogeneous data regimes. Each dataset represents a canonical stress-test configuration:

| Dataset name    | Description |
|-----------------|--------------|
| **small**       | Moderate dimension (p = 90, k = 10), low correlation (œÅ = 0.2); emulates structured, small-scale settings. |
| **large**       | Larger system (p = 200, k = 20), moderate correlation (œÅ = 0.3); tests scalability and performance under multicollinearity. |
| **sparse_neg**  | Sparse with mixed signs (‚âà 50% zeros, negative entries included); tests stability and nonnegativity effects. |

---

Let \( p \) and \( k \) denote the number of samples and features, respectively.  We begin from an orthogonal base frame \(V_0 \in \mathbb{R}^{p \times k}\) obtained via QR decomposition of a random Gaussian matrix:

\[
V_0 = \text{qr.Q}\big(\text{qr}(\mathcal{N}(0,1)_{p \times k})\big).
\]

We then generate correlated data using a covariance matrix
\[
\Sigma_{ij} = 
\begin{cases}
1 & \text{if } i = j,\\
\rho & \text{otherwise},
\end{cases}
\]
where \( \rho \) controls inter-feature correlation.  
Samples are drawn from \( \mathcal{N}(0, \Sigma) \) to yield \( Y_0 \).  
The corresponding nonnegative target \( X_0 \) is constructed as
\[
X_0 = \max\left(Y_0 + 0.2\,\mathcal{N}(0,1),\, 0\right).
\]
Optionally, sparsity and sign heterogeneity are imposed:
\[
Y_{0,ij} =
\begin{cases}
0 & \text{with probability } p_{\text{sparse}},\\
Y_{0,ij} - 0.5 & \text{if include\_neg = TRUE}.
\end{cases}
\]

This yields diverse matrices \( (Y_0, X_0, V_0) \) with controllable noise, correlation, and structure for robust evaluation.



```{r data_gen}
generate_synth_data <- function(p=40, k=3, noise=0.5, corr=0.1,
                                sparse_prob=0.2, include_neg=FALSE) {
  V0 <- qr.Q(qr(matrix(rnorm(p*k), p, k)))
  library(MASS)
  # Correlated data
  Sigma <- matrix(corr, k, k); diag(Sigma) <- 1
  Y0 <- mvrnorm(n = p, mu = rep(0, k), Sigma = Sigma)
  if (include_neg) Y0 <- Y0 - 0.5
  if (sparse_prob > 0) Y0[runif(p*k) < sparse_prob] <- 0
  X0 <- pmax(Y0 + 0.2 * matrix(rnorm(p*k), p, k), 0)
  list(Y0=Y0, X0=X0, V0=V0)
}

# Generate datasets
data_small      <- generate_synth_data(p=90,  k=10, corr=0.2)
data_large      <- generate_synth_data(p=200, k=20, corr=0.3)
data_sparse_neg <- generate_synth_data(sparse_prob=0.5, include_neg=TRUE)



dataset_stats <- function(name, data) {
  Y0 <- data$Y0
  cor_vals <- cor(Y0)
  mean_corr <- mean(abs(cor_vals[upper.tri(cor_vals)]))
  sparsity <- mean(Y0 == 0)
  neg_frac <- mean(Y0 < 0)
  scale_mean <- mean(abs(Y0))
  data.frame(
    Dataset = name,
    Mean_Abs_Correlation = round(mean_corr, 3),
    Sparsity = round(sparsity, 3),
    Negative_Fraction = round(neg_frac, 3),
    Mean_Abs_Value = round(scale_mean, 3)
  )
}

summary_table <- rbind(
  dataset_stats("Small", data_small),
  dataset_stats("Large", data_large),
  dataset_stats("Sparse_Neg", data_sparse_neg)
)

knitr::kable(summary_table, caption="Structural characteristics of synthetic datasets.")
```


The following figure shows the heatmap structure of the generated matrices. Color intensity represents magnitude; blue tones indicate positive values, and red tones (where applicable) indicate negatives.

```{r}
library(ggplot2)
library(reshape2)
library(patchwork)

plot_heat <- function(mat, title) {
  df <- melt(mat)
  ggplot(df, aes(Var2, Var1, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low="red", mid="white", high="blue", midpoint=0) +
    theme_minimal(base_size = 12) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank(),
          panel.grid=element_blank(),
          plot.title=element_text(face="bold", hjust=0.5)) +
    labs(title=title, x=NULL, y=NULL, fill="Value")
}

p_small  <- plot_heat(data_small$Y0, "Small Dataset (œÅ=0.2)")
p_large  <- plot_heat(data_large$Y0, "Large Dataset (œÅ=0.3)")
p_sparse <- plot_heat(data_sparse_neg$Y0, "Sparse Negative Dataset")

(p_small | p_large | p_sparse) + plot_layout(guides = "collect")
```

Figure 1. Synthetic data matrices used for benchmarking.
From left to right:

* Small dataset: moderately correlated structure with low noise.

* Large dataset: increased dimensionality and correlation.

* Sparse-negative dataset: structured sparsity and sign heterogeneity, introducing strong nonlinearity and nonnegativity challenges.


The summary table confirms the diversity of these datasets:

* Small dataset: Moderate correlation (‚âà0.2) and minimal sparsity, representing a well-conditioned test case.

* Large dataset: Similar correlation strength but higher feature count, stressing scalability and convergence stability.

* Sparse-negative dataset: Extremely sparse (~50%) and sign-diverse (‚âà50% negative), producing high-contrast signals and providing a stringent test for nonnegativity-constrained optimization.



## Experimental Configurations for NSA-Flow

In this section, we define the experimental configuration grid used to evaluate the performance of the NSA-Flow algorithm across a range of retraction strategies, weighting parameters, and data regimes.  

We explore the impact of:

- **Retraction type** (`qr`, `soft`, `soft_polar`, `none`),  

- **Trade-off weight** `w`, controlling the balance between fidelity and orthogonality,  

- **Non-negativity enforcement**, and  

- **Dataset characteristics**, including *small*, *large*, and *sparse-negative* synthetic data.  

To ensure robustness, each configuration is repeated three times with independent random seeds, and timing is measured using `microbenchmark` for consistency.  

```{r experiments}
smallw=c(0.0, 0.001, 0.01)
configs <- expand.grid(
  w = c(smallw, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0), 
  retraction = c("qr", "soft", "soft_polar", "none" ), 
  apply_nonneg = c(FALSE, TRUE),
  dataset = c("small", "large", "sparse_neg"),
  x0_null = c(FALSE, TRUE),
  stringsAsFactors = FALSE
)

configs_fast <- expand.grid(
  w = c(0.01,  0.25, 0.5, 0.75, 0.9 ), 
  retraction = c("qr", "polar", "soft", "soft_polar", "none" ), 
  apply_nonneg = c( TRUE, FALSE ),
  dataset = c("small", "large", "sparse_neg"),
  x0_null = c( TRUE ),
  stringsAsFactors = FALSE
)

if ( ! exists("results") ) {
  n_reps <- 3
  results <- list()
  maxcfnfigs <- nrow(configs)
  for (i in 1:maxcfnfigs) {
    cfg <- configs[i,]
    data <- get(paste0("data_", cfg$dataset))
    X0_use <- if (cfg$x0_null) data$Y0 else data$X0
    rep_results <- lapply(1:n_reps, function(rep) {
      mb <- microbenchmark(
        res <- ANTsR::nsa_flow( data$Y0, X0=X0_use, w=cfg$w, retraction=cfg$retraction, 
          initial_learning_rate=1e-2, max_iter=200, tol=1e-6, 
          # optimizer='bidirectional_lookahead',
                        apply_nonneg=cfg$apply_nonneg, seed=42+rep, verbose=FALSE, plot=TRUE),
        times=3
      )
      res$timing <- mb$time / 1e9  # Seconds
      res$rep <- rep
      res$Y0 = data$Y0
      res
    })
    results[[i]] <- rep_results
  } 
}
```


```{r plots,eval=FALSE}
# Aggregate traces
all_traces <- lapply(1:length(results), function(i) {
  reps <- results[[i]]
  cfg <- configs[i,]
  df <- do.call(rbind, lapply(reps, function(r) {
    tr <- r$traces
    tr$config_id <- i
    tr$rep <- r$rep
    tr
  }))
  df$w <- cfg$w
  df$retraction <- cfg$retraction
  df$apply_nonneg <- cfg$apply_nonneg
  df$dataset <- cfg$dataset
  df$x0_null <- cfg$x0_null
  df
}) %>% do.call(rbind, .)

# Long format
trace_long <- all_traces %>%
  pivot_longer(cols = c(fidelity, orthogonality, total_energy), names_to = "Metric", values_to = "Value")

# Faceted plot, grouped by dataset/retraction
p <- ggplot(trace_long, aes(x = iter, y = Value, color = Metric, group = interaction(rep, Metric))) +
  geom_line(alpha = 0.7) +
  facet_grid(dataset + retraction ~ w + apply_nonneg + x0_null, scales = "free_y") +
  scale_y_log10() +  # Optional log
  theme_minimal() +
  labs(title = "Enhanced NSA-Flow Traces (Mean over Reps, Log Scale)")
print(p)
```

# Results

## Default Metrics Summary

This section summarizes quantitative performance metrics for each experimental configuration.  
For every retraction method, dataset, and weight `w`, we compute:

- **Fidelity**: deviation of the optimized solution \(Y\) from its target \(X_0\) (lower is better).  

- **Orthogonality defect**: deviation from orthonormality of \(Y\) (lower is better).  

- **Energy**: final objective energy reported by the solver.  

- **Timing**: average runtime (in seconds) from microbenchmarked repetitions.  


Each configuration is evaluated across multiple repetitions, and the mean values are aggregated into `summary_df` for statistical comparison.


```{r tests}
# Summarize per config: mean/sd of finals, timing, rank
library(dplyr)

summary_df <- lapply(seq_along(results), function(i) {
  reps <- results[[i]]
  
  finals <- sapply(reps, function(r) {
    # Use X0 if available, otherwise Y0
    tardata <- if (!is.null(r$X0)) r$X0 else r$Y0
    
    c(
      fid_0 = norm(tardata, "F"),
      fid = norm(tardata - r$Y, "F"),
      orth_0 = ANTsR::invariant_orthogonality_defect(r$Y0),
      orth = ANTsR::invariant_orthogonality_defect(r$Y),
      energy = tail(r$traces$total_energy, 1),
      timing = mean(r$timing),
      rank_final = effective_rank(r$Y),
      residual = orth_residual(r$Y)
    )
  })
  
  # Compute means and standard deviations across repetitions
  means <- rowMeans(finals)
  sds <- apply(finals, 1, sd)
  
  cfg <- configs[i, ]
  
  data.frame(
    config_id = i, w = cfg$w, retraction = cfg$retraction, apply_nonneg = cfg$apply_nonneg,
    dataset = cfg$dataset, x0_null = cfg$x0_null,
    mean_fid_0 = means["fid_0"], # sd_fid_0 = sds["fid_0"],
    mean_fid = means["fid"], # sd_fid = sds["fid"],
    mean_orth_0 = means["orth_0"], # sd_orth_0 = sds["orth_0"],
    mean_orth = means["orth"], # sd_orth = sds["orth"],
    mean_energy = means["energy"], # sd_energy = sds["energy"],
    mean_timing = means["timing"] #sd_timing = sds["timing"]
  )
}) %>% bind_rows()
rownames(summary_df) <- NULL
# gt::gt(summary_df)
# kable(summary_df, digits=4, caption="Config Summary (Mean/SD over Reps)")

# Pass/fail: Similar to original, but on means
# ...
```

## Edge Case Tests

We next validate the stability and robustness of the algorithm under atypical or numerically extreme conditions.
These tests ensure that the `nsa_flow()` implementation behaves predictably even with degenerate, singular, or small-scale inputs.

The following cases are examined:

1.	Zero data (all entries = 0).

2.	Singular or rank-deficient inputs.

3.	Extremely small tolerance or few iterations.

4.	Nonnegativity constraint enforcement.

5.	Retract-type sensitivity between QR and Soft projections.

Each test reports whether it completes successfully and key numerical diagnostics.

```{r edge_tests,echo=FALSE}
# Edge Case Tests ----
set.seed(456)

cat("\nüß™ Running edge-case stability tests...\n")
# Edge: Zero-norm
data_zero <- list(Y0=matrix(0, 40, 3), X0=matrix(0, 40, 3))

# Edge: Singular
data_singular <- list(Y0=qr.Q(qr(matrix(rnorm(40*2), 40, 2))), X0=NULL)  # Rank-deficient

edge_tests <- list()

# Helper to safely run optimization
safe_run <- function(expr) {
  tryCatch(
    {
      res <- eval(expr)
      list(success = TRUE, res = res)
    },
    error = function(e) list(success = FALSE, message = e$message)
  )
}

# 1. Zero data matrix ----------------------------------------------------
set.seed(123)
data_zero <- matrix(0, 10, 5)

edge_tests$zero <- safe_run({
  ANTsR::nsa_flow(Y0 = data_zero,
           X0 = NULL,
           w = 0.5,
           max_iter = 20,
           tol = 1e-5,
           retraction = "soft",
           verbose = FALSE)
})

if (edge_tests$zero$success) {
  cat("‚úÖ Zero data test: ran successfully.\n")
  Yz <- edge_tests$zero$res$Y
  cat("  ‚Üí Output finite:", all(is.finite(Yz)), "\n")
  cat("  ‚Üí Mean abs(Y):", mean(abs(Yz)), "\n\n")
} else {
  cat("‚ùå Zero data test failed (expected):", edge_tests$zero$message, "\n\n")
}


# 2. Singular data matrix (rank-deficient) -------------------------------

edge_tests$singular <- safe_run({
  ANTsR::nsa_flow(Y0 = data_singular$Y0,
           X0 = data_singular$X0,
           w = 0.3,
           retraction = "qr",
           max_iter = 50,
           tol = 1e-5,
           verbose = FALSE)
})

if (edge_tests$singular$success) {
  cat("‚úÖ Singular data test: ran successfully.\n")
  Ys <- edge_tests$singular$res$Y
  cat("  ‚Üí Orthogonality defect:",
      ANTsR::invariant_orthogonality_defect(Ys), "\n\n")
} else {
  cat("‚ùå Singular data test failed:", edge_tests$singular$message, "\n\n")
}


# 3. Very small tolerance & few iterations -------------------------------
edge_tests$tight_tol <- safe_run({
  ANTsR::nsa_flow(Y0 = matrix(runif(20), 5, 4),
           X0 = matrix(runif(20), 5, 4),
           w = 0.7,
           retraction = "soft",
           tol = 1e-10,
           max_iter = 5,
           verbose = FALSE)
})

if (edge_tests$tight_tol$success) {
  cat("‚úÖ Tight tolerance test ran.\n")
  cat("  ‚Üí Iterations completed:", edge_tests$tight_tol$res$final_iter, "\n\n")
} else {
  cat("‚ùå Tight tolerance test failed:", edge_tests$tight_tol$message, "\n\n")
}


# 4. Nonnegativity enforcement stability ---------------------------------
edge_tests$nonneg <- safe_run({
  ANTsR::nsa_flow(Y0 = matrix(rnorm(20), 5, 4),
           X0 = NULL,
           w = 0.5,
           apply_nonneg = TRUE,
           retraction = "soft",
           max_iter = 30,
           verbose = FALSE)
})

if (edge_tests$nonneg$success) {
  cat("‚úÖ Nonnegativity test ran.\n")
  Yn <- edge_tests$nonneg$res$Y
  cat("  ‚Üí Any negative entries:", any(Yn < -1e-12), "\n\n")
} else {
  cat("‚ùå Nonnegativity test failed:", edge_tests$nonneg$message, "\n\n")
}


# 5. Large retraction sensitivity ----------------------------------------
edge_tests$retract_compare <- safe_run({
  Ytest <- matrix(runif(30), 6, 5)
  list(
    qr = ANTsR::nsa_flow(Y0 = Ytest, X0 = Ytest, w = 0.4, retraction = "qr", max_iter = 30),
    soft = ANTsR::nsa_flow(Y0 = Ytest, X0 = Ytest, w = 0.4, retraction = "soft", max_iter = 30)
  )
})

if (edge_tests$retract_compare$success) {
  cat("‚úÖ Retraction comparison ran.\n")
  Yqr <- edge_tests$retract_compare$res$qr$Y
  Ysoft <- edge_tests$retract_compare$res$soft$Y
  cat("  ‚Üí Orth defect (QR):", ANTsR::invariant_orthogonality_defect(Yqr), "\n")
  cat("  ‚Üí Orth defect (Soft):", ANTsR::invariant_orthogonality_defect(Ysoft), "\n\n")
} else {
  cat("‚ùå Retraction comparison failed:", edge_tests$retract_compare$message, "\n\n")
}


# --- Summary ---
cat("üß≠ Edge test summary:\n")
data.frame(
  Test = names(edge_tests),
  Success = sapply(edge_tests, function(x) x$success)
)
```

# Summary and Verdicts


```{r verdicts}
# --- Compute test passes for each config ---
summary_df <- summary_df %>%
  mutate(
    fid_pass  = mean_fid < mean_fid_0,
    orth_pass = mean_orth < mean_orth_0 & ( ! (w %in% smallw) )  # Skip orth test for very small w
  )

# --- Determine which tests are expected to pass ---
summary_df <- summary_df %>%
  mutate(
    target_type = case_when(
      w == 0        ~ "fidelity_only",
      w == 1        ~ "orth_only",
      between(w, 0, 1) ~ "mixed",
      TRUE          ~ "unknown"
    ),
    target_pass = case_when(
      target_type == "fidelity_only" ~ fid_pass,
      target_type == "orth_only"     ~ orth_pass,
      target_type == "mixed"         ~ (fid_pass & orth_pass),
      TRUE                           ~ FALSE
    )
  )

# --- Compute global pass/fail status ---
if (all(summary_df$target_pass, na.rm = TRUE)) {
  cat("‚úÖ All targeted tests passed.\n")
} else {
  cat("‚ùå Some targeted tests failed.\n")
}

# --- Summaries by logical condition ---
cat("\nDetailed pass summary by target type:\n")
print(
  summary_df %>%
    group_by(target_type) %>%
    summarise(
      n = n(),
      n_pass = sum(target_pass),
      pass_rate = n_pass / n
    )
)

# --- Nonnegativity summary ---
cat("\nNonnegativity impact summary:\n")
print(
  summary_df %>%
    group_by(apply_nonneg) %>%
    summarise(
      mean_fid = mean(mean_fid),
      mean_orth = mean(mean_orth),
      n_pass = sum(target_pass),
      n = n(),
      pass_rate = n_pass / n
    )
)

# --- Optional: pretty visual summary ---
library(ggplot2)
ggplot(summary_df, aes(x = factor(w), y = mean_fid / mean_fid_0,
                       fill = apply_nonneg)) +
  geom_col(position = "dodge") +
  facet_wrap(~retraction + x0_null, ncol = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(
    title = "Relative Fidelity (mean_fid / mean_fid_0)",
    x = "Weight w",
    y = "Relative change (<1 means improvement)",
    fill = "Nonnegativity"
  ) +
  theme_minimal(base_size = 12)



```

These quantitative and stress tests jointly confirm that:


* The Soft retraction method provides the most stable convergence across datasets and edge conditions.

* The solver remains numerically robust for nearly singular and zero-valued data.

* Nonnegativity constraints are properly enforced without introducing instability.

* Tight tolerances and low iteration counts do not produce divergent or undefined states.

Together, these tests validate both the correctness and numerical resilience of the NSA-Flow implementation across a wide range of operating conditions.


# Discussion

## Relative Fidelity 
```{r,fig.width=10, fig.height=6}


tradeoff_df <- summary_df %>%
  # filter to mixed or target conditions if desired
  # filter(target_type == "mixed") %>%
  group_by(retraction, dataset, w) %>%
  summarise(
    rel_orth = mean(mean_orth / mean_orth_0, na.rm = TRUE),
    rel_fid  = mean(mean_fid  / mean_fid_0,  na.rm = TRUE),
    .groups = "drop"
  )

ggplot(tradeoff_df, aes(x = w, y = rel_fid, color = dataset, group = dataset)) +
  geom_line(linewidth = 1.1) +
  geom_point(size = 2) +
  facet_wrap(~ retraction, scales = "free_y") +
  labs(title = "Relative Fidelity by Retraction and Dataset",
       y = "Relative Fidelity", x = "Tradeoff weight w") +
  theme_minimal(base_size = 13)
```

## Relative Orthogonality 
```{r,fig.width=10, fig.height=6}
ggplot(tradeoff_df, aes(x = w, y = rel_orth, color = dataset, group = dataset)) +
  geom_line(linewidth = 1.1) +
  geom_point(size = 2) +
  facet_wrap(~ retraction, scales = "free_y") +
  labs(title = "Relative Orthogonality by Retraction and Dataset",
       y = "Relative Orthogonality", x = "Tradeoff weight w") +
  theme_minimal(base_size = 13)
```

**Figure:** *Relative orthogonality and fidelity (lower = better) for each retraction method, dataset, and weighting parameter (`w`). QR retraction consistently yields the lowest combined error, with soft‚Äìpolar offering a strong secondary balance. Unretracted (‚Äúnone‚Äù) cases show incomplete convergence and weaker stability.*


## Summary and Recommendations on Retraction Choices in NSA-Flow

The Soft retraction provides the best overall trade-off between orthogonality and fidelity across datasets and weights ‚Äî in particular it achieves substantially lower relative fidelity (i.e., better fidelity) while driving orthogonality improvements comparable to other aggressive retractions at practical $\omega$ values. Numerical examples follow.



Below are a few examples. For each, we show rel_orth and rel_fid and the combined score. Notice that for practical w values the Soft retraction attains much lower fidelity (i.e. rel_fid closer to 0) while achieving orthogonality reductions comparable to QR/Polar.

* Example (Large dataset, w=0.90):

* soft: rel_orth = 2.2278e-03, rel_fid = 0.6888

* qr  : rel_orth = 1.6396e-03, rel_fid = 0.9550

* Interpretation: QR gives marginally better orthogonality, but Soft gives substantially better fidelity ‚Äî overall Soft has a better combined score.

* Example (Small dataset, w=0.25):

* soft: rel_orth = 1.1921e-01, rel_fid = 0.5768

* qr  : rel_orth = 1.3300e-02, rel_fid = 0.9435

* Interpretation: QR improves orthogonality more strongly, but Soft preserves fidelity to a much greater extent.

These patterns repeat across the results: Soft tends to offer a superior balance (especially important when fidelity matters).


‚∏ª


## Conclusion

* Soft retraction is the recommended default when both fidelity and orthogonality matter: in the supplied tradeoff_df it produces the best balanced outcomes (lowest combined score) across large, small, and sparse_neg datasets for practical $\omega$ values.

* QR and Polar remain excellent choices when strict orthogonality is the single top priority (they produce extremely low rel_orth), but they often do so at the cost of fidelity.

* The choice of $\omega$ is critical: moderate values (e.g. 0.25‚Äì0.6) often yield the best balanced behavior.

## Mathematical Appendix: 


Let (Y_{\text{cand}} \in \mathbb{R}^{p\times k}) be the candidate iterate; denote (Y^\top Y) by (A).

### QR retraction

Compute the (economy) QR decomposition (Y_{\text{cand}} = Q R). Then
[
Y_{\text{tilde}} = Q , \operatorname{diag}(\operatorname{sign}(\operatorname{diag}(R))).
]
This enforces exact orthonormal columns (up to column signs).

## Polar retraction

[
Y_{\text{tilde}} = Y_{\text{cand}} , (Y_{\text{cand}}^\top Y_{\text{cand}})^{-1/2}.
]
Here ((\cdot)^{-1/2}) is the symmetric inverse square root of the matrix (A) (see Appendix).

## Soft (QR-based) retraction

Let (Q) be the QR-orthogonal factor of (Y_{\text{cand}}) (with sign-corrected columns). Interpolate:
[
Y_{\text{tilde}} = (1 - w),Y_{\text{cand}} + w,Q,
]
and then rescale (Y_{\text{tilde}}) to match the Frobenius norm of (Y_{\text{cand}}) to keep scale consistent.

### Soft-Polar

Interpolate in the polar-transform space:
[
Y_{\text{tilde}} = Y_{\text{cand}} \left[(1-w) I + w, (Y_{\text{cand}}^\top Y_{\text{cand}})^{-1/2}\right].
]




### computing ((Y^\top Y)^{-1/2})

To compute the symmetric inverse square root of the symmetric positive (semi-)definite matrix (A = Y^\top Y):

1.	Compute eigen-decomposition: (A = U \Lambda U^\top) where (\Lambda = \operatorname{diag}(\lambda_i)).

2.	Form (\Lambda^{-1/2} = \operatorname{diag}(\lambda_i^{-1/2})), with small-eigenvalue regularization:
(\lambda_i^{-1/2} \leftarrow (\lambda_i + \varepsilon)^{-1/2}) for (\varepsilon = 10^{-12}).

3.	Then (A^{-1/2} = U \Lambda^{-1/2} U^\top).


This yields a numerically stable symmetric inverse square root used by the polar/soft-polar retractions.


```{r sessioninfo}
sessionInfo()
```
