% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nsa_flow_torch.R
\name{nsa_flow_autograd}
\alias{nsa_flow_autograd}
\title{NSA-Flow Optimization via PyTorch AutoGrad (R wrapper)}
\usage{
nsa_flow_autograd(
  Y0,
  X0 = NULL,
  w = 0.5,
  retraction = c("soft_polar", "polar", "none"),
  max_iter = 500L,
  tol = 1e-06,
  verbose = FALSE,
  seed = 42L,
  apply_nonneg = TRUE,
  optimizer = "Adam",
  initial_learning_rate = NULL,
  lr_strategy = "auto",
  fidelity_type = "scale_invariant",
  orth_type = "scale_invariant",
  record_every = 1L,
  window_size = 5L,
  plot = FALSE,
  precision = "float64"
)
}
\arguments{
\item{Y0}{numeric matrix p x k initial guess}

\item{X0}{numeric matrix p x k target (or NULL to initialize from Y0)}

\item{w}{numeric in [0,1] weighting fidelity vs orthogonality}

\item{retraction}{character retraction method}

\item{max_iter}{integer max iterations}

\item{tol}{numeric convergence tolerance}

\item{verbose}{logical}

\item{seed}{integer random seed}

\item{apply_nonneg}{logical or 'softplus'/'none' etc.}

\item{optimizer}{character optimizer name (e.g. 'Adam','lars','sgdp')}

\item{initial_learning_rate}{NULL (auto), numeric, or character strategy string}

\item{lr_strategy}{character passed to Python if initial_learning_rate is NULL/'auto'}

\item{fidelity_type}{character ('basic','scale_invariant','symmetric','normalized')}

\item{orth_type}{character ('basic','scale_invariant')}

\item{record_every}{integer frequency of recording traces}

\item{window_size}{integer window for energy stability}

\item{plot}{logical produce ggplot (default FALSE)}

\item{precision}{'float32' or 'float64'}
}
\value{
list: Y (matrix), traces (data.frame), final_iter, best_total_energy, best_Y_iteration, plot (ggplot or NULL), settings
}
\description{
Mirror of Python `nsa_flow_autograd()` but callable from R via reticulate.
Uses the Python implementation (nsa_flow.nsa_flow_autograd) and returns
R-friendly results (matrix, data.frame, ggplot).
}
