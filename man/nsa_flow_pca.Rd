% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multiscaleSVDxpts.R
\name{nsa_flow_pca}
\alias{nsa_flow_pca}
\title{NSA-Flow Sparse PCA (stable + efficient)}
\usage{
nsa_flow_pca(
  X,
  k,
  lambda = 0.1,
  alpha = 1e-04,
  max_iter = 100,
  proximal_type = c("basic", "nsa_flow"),
  w_pca = 1,
  nsa_w = 0.5,
  apply_soft_thresh_in_nns = FALSE,
  tol = 1e-06,
  retraction = def_ret,
  grad_tol = 1e-04,
  nsa_flow_fn = nsa_flow_autograd,
  verbose = FALSE,
  orth_every = 5
)
}
\arguments{
\item{X}{numeric matrix (n x p), rows = observations, cols = variables}

\item{k}{integer > 0 and <= min(n,p), number of components}

\item{lambda}{non-negative numeric, L1 proximal weight}

\item{alpha}{positive numeric, initial step size}

\item{max_iter}{integer, maximum iterations}

\item{proximal_type}{character, "basic" or "nsa_flow"}

\item{w_pca}{positive numeric, weight for PCA fidelity}

\item{nsa_w}{numeric in [0,1], weight parameter passed to nsa_flow}

\item{apply_soft_thresh_in_nns}{logical (passed through if used)}

\item{tol}{numeric, tolerance for relative parameter change convergence}

\item{retraction}{retraction function or identifier (passed to nsa_flow)}

\item{grad_tol}{numeric, gradient-norm tolerance for convergence}

\item{nsa_flow_fn}{optional, nsa_flow function to use (default: nsa_flow)}

\item{verbose}{logical, print iteration diagnostics}

\item{orth_every}{integer >=1, perform orthogonalization every this many iterations (default 5)}
}
\value{
list with components:
  \item{Y}{best p x k loading matrix found}
  \item{energy_trace}{vector of per-iteration energy/explained-variance records}
  \item{final_iter}{last iteration index}
  \item{best_energy}{best energy attained}
  \item{converged}{logical}
  \item{no_improve_count}{internal counter at exit}
  \item{lr_reductions}{number of LR reductions performed}
  \item{expl_var_ratio}{explained variance ratio for best_Y}
}
\description{
Performs sparse PCA using a proximal-gradient scheme with optional NSA-flow
proximal regularization. This implementation preserves the stable behavior
of the original `sparse_pca_imp()` while improving efficiency by:
 - precomputing X'X, using it for gradient/energy,
 - in-place trial updates to reduce allocations,
 - lazy orthogonalization (every `orth_every` iterations),
 - retaining Armijo backtracking and adaptive LR scheduling.
}
