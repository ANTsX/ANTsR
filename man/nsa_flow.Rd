% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multiscaleSVDxpts.R
\name{nsa_flow}
\alias{nsa_flow}
\title{NSA-Flow Optimization for Nonnegative Orthogonal Factor Learning}
\usage{
nsa_flow(
  Y0,
  X0 = NULL,
  w = 0.5,
  lambda = 1 - w,
  retraction = c("soft", "polar", "qr"),
  max_iter = 100,
  tol = 1e-04,
  verbose = FALSE,
  seed = 42,
  apply_nonneg = TRUE,
  optimizer = "bidirectional_lookahead",
  initial_learning_rate = 0.01,
  record_every = 1,
  window_size = 5
)
}
\arguments{
\item{Y0}{Numeric matrix (`p x k`). Initial estimate of the factor matrix.}

\item{X0}{Optional numeric matrix (`p x k`). Target or reference matrix
for fidelity regularization. If `NULL`, only orthogonality is enforced.}

\item{w}{Numeric scalar in `[0, 1]`. Balances fidelity and orthogonality:
small `w` emphasizes orthogonality; large `w` emphasizes fidelity.}

\item{lambda}{Numeric scalar in `[0, 1]`. Complementary weight to `w`
(`lambda = 1 - w`); used internally for retraction interpolation.}

\item{retraction}{Character string. Retraction method used to project
the updated matrix back to a valid manifold:
\itemize{
  \item `"polar"` – polar decomposition-based retraction.
  \item `"qr"` – QR decomposition-based retraction.
  \item `"soft"` – soft interpolation between identity and polar retraction.
}}

\item{max_iter}{Integer. Maximum number of optimization iterations.}

\item{tol}{Numeric. Relative tolerance for convergence; stops early if energy
reduction across the last `window_size` iterations is smaller than `tol`.}

\item{verbose}{Logical. If `TRUE`, prints progress information at each iteration.}

\item{seed}{Optional integer. If provided, fixes the random seed for reproducibility.}

\item{apply_nonneg}{Logical. If `TRUE`, enforces nonnegativity on `Y` after
each retraction via `pmax(Y, 0)`.}

\item{optimizer}{Character string. Name of optimizer used for gradient updates.
Default: `"bidirectional_lookahead"`. Passed to `create_optimizer()`.}

\item{initial_learning_rate}{Numeric. Initial learning rate used in cosine annealing.}

\item{record_every}{Integer. Frequency (in iterations) to record trace information.}

\item{window_size}{Integer. Number of most recent energy values used for convergence check.}
}
\value{
A list with elements:
\describe{
  \item{`Y`}{Final optimized matrix (`p x k`).}
  \item{`traces`}{Data frame containing iteration logs:
       iteration, time, energy, orthogonality residual, nonnegativity violation, and lambda.}
  \item{`final_iter`}{Integer; number of iterations completed before convergence.}
}
}
\description{
The `nsa_flow()` function optimizes a matrix \eqn{Y} to balance two competing
objectives:
(1) **Fidelity** to a target matrix \eqn{X_0}, and  
(2) **Orthogonality** of the columns of \eqn{Y}.

This algorithm is inspired by nonnegative subspace flow dynamics, combining
manifold retractions, adaptive learning rate scheduling, and optional nonnegativity
enforcement. It supports multiple retraction methods (`polar`, `qr`, `soft`)
and gradient-based optimizers. Non-negative Stiefel approximation flow (NSAFlow).
}
\details{
The optimization minimizes an energy function:

\deqn{E(Y) = c_{orth} \cdot \frac{1}{4} \|A^T A - D\|_F^2 + \frac{1}{2} \cdot fid_{\eta} \|Y - X_0\|_F^2}

where:
* \eqn{A = Y / \|Y\|_F}  
* \eqn{D} is the diagonal of \eqn{A^T A}

The first term encourages orthogonality among columns of \eqn{Y}, while the
second encourages fidelity to \eqn{X_0}.  
The coefficients `c_orth` and `fid_eta` are automatically scaled based on
initialization and problem size.

The learning rate is decayed according to a **cosine annealing** schedule:
\deqn{\eta_t = \eta_{final} + 0.5 (\eta_0 - \eta_{final})(1 + \cos(\pi t / T))}

The retraction step ensures that updates remain on (or near) the orthogonal manifold.
}
\examples{
\dontrun{
set.seed(123)
Y0 <- matrix(runif(50), 10, 5)
X0 <- Y0 + matrix(rnorm(50, sd = 0.1), 10, 5)
res <- nsa_flow(Y0, X0, w = 0.3, verbose = TRUE)
plot(res$traces$iter, res$traces$energy, type = "l",
     xlab = "Iteration", ylab = "Energy", main = "Convergence of NNS Flow")
}

}
\seealso{
[create_optimizer()], [orth_residual()], [inv_sqrt_sym()], [symm()]
}
